<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>9.0 现代循环神经网络 | Night's Watch</title><meta name="keywords" content="深度学习"><meta name="author" content="Candle"><meta name="copyright" content="Candle"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="现代循环神经网络门控循环单元GRU并非所有的观测值都有意义，也不是所有的观测值都一样重要  可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。 考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，第一个词元的影响至关重要。 我们希望有某些机制能够在一个记忆元里存储重要的早期信息。 如果没有这样的机制，我们将不得不给这个观测">
<meta property="og:type" content="article">
<meta property="og:title" content="9.0 现代循环神经网络">
<meta property="og:url" content="http://candle1220.github.io/post/6823.html">
<meta property="og:site_name" content="Night&#39;s Watch">
<meta property="og:description" content="现代循环神经网络门控循环单元GRU并非所有的观测值都有意义，也不是所有的观测值都一样重要  可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。 考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，第一个词元的影响至关重要。 我们希望有某些机制能够在一个记忆元里存储重要的早期信息。 如果没有这样的机制，我们将不得不给这个观测">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png">
<meta property="article:published_time" content="2024-02-27T12:41:30.524Z">
<meta property="article:modified_time" content="2024-04-02T12:06:18.656Z">
<meta property="article:author" content="Candle">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png"><link rel="shortcut icon" href="/img/candle.png"><link rel="canonical" href="http://candle1220.github.io/post/6823"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#55d5fc","bgDark":"#121212","position":"top-center"},
  source: {
    jQuery: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/jquery/3.6.0/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.js',
      css: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '9.0 现代循环神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-02 20:06:18'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    document.addEventListener('pjax:complete', detectApple)})(window)</script><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/Swiper/8.0.6/swiper-bundle.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">209</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20240402195814166.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Night's Watch</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">9.0 现代循环神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-27T12:41:30.524Z" title="发表于 2024-02-27 20:41:30">2024-02-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-02T12:06:18.656Z" title="更新于 2024-04-02 20:06:18">2024-04-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="9.0 现代循环神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="现代循环神经网络"><a href="#现代循环神经网络" class="headerlink" title="现代循环神经网络"></a>现代循环神经网络</h1><h2 id="门控循环单元GRU"><a href="#门控循环单元GRU" class="headerlink" title="门控循环单元GRU"></a>门控循环单元GRU</h2><p>并非所有的观测值都有意义，也不是所有的观测值都一样重要</p>
<ul>
<li>可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。 考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，第一个词元的影响至关重要。 我们希望有某些机制能够在一个记忆元里<strong>存储重要的早期信息</strong>。 如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度， 因为它会影响所有后续的观测值。</li>
<li>我们可能会遇到这样的情况：一些词元没有相关的观测值。 例如，在对网页内容进行情感分析时， 可能有一些辅助HTML代码与网页传达的情绪无关。 我们希望有一些机制来<strong><em>跳过</em></strong>隐状态表示中的此类词元。</li>
<li>我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。 例如，书的章节之间可能会有过渡存在， 或者证券的熊市和牛市之间可能会有过渡存在。 在这种情况下，最好有一种方法来<strong><em>重置</em></strong>我们的内部状态表示。</li>
</ul>
<hr>
<h3 id="门控隐状态"><a href="#门控隐状态" class="headerlink" title="门控隐状态"></a>门控隐状态</h3><p>门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态</p>
<p>这些机制是<strong>可学习</strong>的，并且能够解决了上面列出的问题。 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 下面我们将详细讨论各类门控。</p>
<p><strong>重置门和更新门</strong></p>
<p>首先介绍<strong><em>重置门</em></strong>（reset gate）和<strong><em>更新门</em></strong>（update gate）。 我们把它们设计成(0,1)区间中的向量， 这样我们就可以进行凸组合。 重置门允许我们控制“可能还想记住”的过去状态的数量； 更新门将允许我们控制新状态中有多少个是旧状态的副本。</p>
<p>我们从构造这些门控开始。 图描述了门控循环单元中的重置门和更新门的输入， 输入是由当前时间步的输入和前一时间步的隐状态给出。 两个门的输出是由使用sigmoid激活函数的两个全连接层给出。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoragru-1.svg" alt="../_images/gru-1.svg"></p>
<p>对于给定的时间步$t$，假设输入是一个小批量$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本个数$n$，输入个数$d$），上一个时间步的隐状态是$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$（隐藏单元个数$h$）。那么，重置门$\mathbf{R}_t \in \mathbb{R}^{n \times h}$和更新门$\mathbf{Z}_t \in \mathbb{R}^{n \times h}$的计算如下所示：</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\

\mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z),

\end{aligned}</script><p>其中$\mathbf{W}_{xr}, \mathbf{W}_{xz} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hr}, \mathbf{W}_{hz} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1 \times h}$是偏置参数。请注意，在求和过程中会触发广播机制，我们使用sigmoid函数将输入值转换到区间$(0, 1)$。</p>
<hr>
<p><strong>候选隐状态</strong></p>
<p>接下来，让我们将重置门$\mathbf{R}_t$与RNN中的常规隐状态更新机制集成，得到在时间步$t$的<strong>候选隐状态</strong>$\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$</p>
<script type="math/tex; mode=display">
\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h)</script><p>其中$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$是偏置项，符号$\odot$是Hadamard积（按元素乘积）运算符。在这里，我们使用$tanh$非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。</p>
<p>与常规公式相比，候选隐状态中的$\mathbf{R}_t$和$\mathbf{H}_{t-1}$的元素相乘可以减少以往状态的影响。每当重置门$\mathbf{R}_t$中的项接近$1$时，我们恢复一个普通的循环神经网络。当重置门中的项接近0时，意味着忽略之前的隐状态，让模型“忘记”之前的信息。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoragru-2.svg" alt="../_images/gru-2.svg"></p>
<hr>
<p><strong>隐状态</strong></p>
<p>上述的计算结果只是候选隐状态，我们仍然需要结合更新门$\mathbf{Z}_t$的效果。这一步确定新的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$在多大程度上来自旧的状态$\mathbf{H}_{t-1}$和新的候选状态$\tilde{\mathbf{H}}_t$。更新门$\mathbf{Z}_t$仅需要在$\mathbf{H}_{t-1}$和$\tilde{\mathbf{H}}_t$之间进行按元素的凸组合就可以实现这个目标。</p>
<p>这就得出了门控循环单元的最终更新公式：</p>
<script type="math/tex; mode=display">
\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t</script><p>每当更新门$\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。此时，来自$\mathbf{X}_t$的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步$t$。相反，当$\mathbf{Z}_t$接近$0$时，新的隐状态$\mathbf{H}_t$就会接近候选隐状态$\tilde{\mathbf{H}}_t$。</p>
<p>这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。例如，如果整个子序列的所有时间步的更新门都接近于$1$，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoragru-3.svg" alt="../_images/gru-3.svg"></p>
<p>总之，门控循环单元具有以下两个显著特征：</p>
<ul>
<li>重置门有助于捕获序列中的短期依赖关系；</li>
<li>更新门有助于捕获序列中的长期依赖关系。</li>
</ul>
<hr>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = 32, 35</span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>
<p>下一步是初始化模型参数。 我们从标准差为0.01的高斯分布中提取权重， 并将偏置项设为0，超参数<code>num_hiddens</code>定义隐藏单元的数量， 实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">three</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = three()  <span class="comment"># 候选隐状态参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>
<p>现在我们将定义隐状态的初始化函数<code>init_gru_state</code>。 与在循环网络RNN中定义的<code>init_rnn_state</code>函数一样， 此函数返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<p>现在我们准备定义门控循环单元模型， 模型的架构与基本的循环神经网络单元是相同的， 只是权重更新公式更为复杂。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure>
<p>训练和预测的工作方式与循环神经网络RNN完全相同。 训练结束后，我们分别打印输出训练集的困惑度， 以及前缀“time traveler”和“traveler”的预测序列上的困惑度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_params,</span><br><span class="line">                            init_gru_state, gru)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">perplexity 1.1, 19911.5 tokens/sec on cuda:0</span></span><br><span class="line"><span class="string">time traveller firenis i heidfile sook at i jomer and sugard are</span></span><br><span class="line"><span class="string">travelleryou can show black is white by argument said filby</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_gru_b77a34_66_1.svg" alt="../_images/output_gru_b77a34_66_1.svg"></p>
<hr>
<p><strong>简洁实现</strong></p>
<p>高级API包含了前文介绍的所有配置细节， 所以我们可以直接实例化门控循环单元模型。 这段代码的运行速度要快得多， 因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">gru_layer = nn.GRU(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="长短期记忆网络LSTM"><a href="#长短期记忆网络LSTM" class="headerlink" title="长短期记忆网络LSTM"></a>长短期记忆网络LSTM</h2><h3 id="门控记忆元"><a href="#门控记忆元" class="headerlink" title="门控记忆元"></a>门控记忆元</h3><p>可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。 长短期记忆网络引入了<strong><em>记忆元</em></strong>（memory cell），或简称为<em>单元</em>（cell）。 有些文献认为记忆元是隐状态的一种特殊类型， 它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。 为了控制记忆元，我们需要许多门。 其中一个门用来从单元中输出条目，我们将其称为<strong><em>输出门</em></strong>（output gate）。 另外一个门用来决定何时将数据读入单元，我们将其称为<strong><em>输入门</em></strong>（input gate）。 我们还需要一种机制来重置单元的内容，由<strong><em>遗忘门</em></strong>（forget gate）来管理， 这种设计的动机与门控循环单元相同， 能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。</p>
<p><strong>输入门，遗忘门，输出门</strong></p>
<p>就如在门控循环单元中一样， 当前时间步的输入和前一个时间步的隐状态 作为数据送入长短期记忆网络的门中， 如图所示。 它们由三个具有sigmoid激活函数的全连接层处理， 以计算输入门、遗忘门和输出门的值。 因此，这三个门的值都在(0,1)的范围内。<img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoralstm-0.svg" alt="../_images/lstm-0.svg"></p>
<p>我们来细化一下长短期记忆网络的数学表达。假设有$h$个隐藏单元，批量大小为$n$，输入数为$d$。</p>
<p>因此，输入为$\mathbf{X}_t \in \mathbb{R}^{n \times d}$，前一时间步的隐状态为$\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$。</p>
<p>相应地，时间步$t$的门被定义如下：</p>
<p>输入门是$\mathbf{I}_t \in \mathbb{R}^{n \times h}$，</p>
<p>遗忘门是$\mathbf{F}_t \in \mathbb{R}^{n \times h}$，</p>
<p>输出门是$\mathbf{O}_t \in \mathbb{R}^{n \times h}$。</p>
<p>它们的计算方法如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}

\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\

\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\

\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),

\end{aligned}</script><p>其中$\mathbf{W}_{xi}, \mathbf{W}_{xf}, \mathbf{W}_{xo} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hi}, \mathbf{W}_{hf}, \mathbf{W}_{ho} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_i, \mathbf{b}_f, \mathbf{b}_o \in \mathbb{R}^{1 \times h}$是偏置参数。</p>
<hr>
<p><strong>候选记忆元</strong></p>
<p>由于还没有指定各种门的操作，所以先介绍<strong>候选记忆元</strong>（candidate memory cell）$\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$。</p>
<p>它的计算与上面描述的三个门的计算类似，但是使用$\tanh$函数作为激活函数，函数的值范围为$(-1, 1)$。</p>
<p>下面导出在时间步$t$处的方程：</p>
<script type="math/tex; mode=display">
\tilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c)</script><p>其中$\mathbf{W}_{xc} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hc} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_c \in \mathbb{R}^{1 \times h}$是偏置参数。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoralstm-1.svg" alt="../_images/lstm-1.svg"></p>
<hr>
<p><strong>记忆元</strong></p>
<p>在门控循环单元中，有一种机制来控制输入和遗忘（或跳过）。类似地，在长短期记忆网络中，也有两个门用于这样的目的：输入门$\mathbf{I}_t$控制采用多少来自$\tilde{\mathbf{C}}_t$的新数据，而遗忘门$\mathbf{F}_t$控制保留多少过去的记忆元$\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}$的内容。</p>
<p>使用按元素乘法，得出：</p>
<script type="math/tex; mode=display">
\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t</script><p>如果遗忘门始终为$1$且输入门始终为$0$，则过去的记忆元$\mathbf{C}_{t-1}$将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。</p>
<p>这样我们就得到了计算记忆元的流程图</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoralstm-2.svg" alt="../_images/lstm-2.svg"></p>
<hr>
<p><strong>隐状态</strong></p>
<p>最后，我们需要定义如何计算隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times h}$，这就是输出门发挥作用的地方。在长短期记忆网络中，它仅仅是记忆元的$\tanh$的门控版本。这就确保了$\mathbf{H}_t$的值始终在区间$(-1, 1)$内：</p>
<script type="math/tex; mode=display">
\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)</script><p>只要输出门接近$1$，我们就能够有效地将所有记忆信息传递给预测部分，</p>
<p>而对于输出门接近$0$，我们只保留记忆元内的所有信息，而不需要更新隐状态。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoratyporalstm-3.svg" alt="../_images/lstm-3.svg"></p>
<p>尽管记忆元和隐状态在功能上有重叠，它们的存在并不是重复的，而是相互补充的：</p>
<ul>
<li><strong>记忆元</strong>专注于长期信息的存储和传递。它能够在很长的序列中维持重要的状态信息，因为它通过门控机制精细地控制信息的保留和遗忘。</li>
<li><strong>隐状态</strong>则提供了一种机制来输出短期的信息。它反映了当前时间点的网络状态，是基于最近的输入和过去的长期信息（通过记忆元传递）计算得出的。</li>
</ul>
<hr>
<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> np, npx</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> rnn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> mxnet <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">npx.set_np()</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>
<p>接下来，我们需要定义和初始化模型参数。 如前所述，超参数<code>num_hiddens</code>定义隐藏单元的数量。 我们按照标准差0.01的高斯分布初始化权重，并将偏置项设为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lstm_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">three</span>():</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()  <span class="comment"># 候选记忆元参数</span></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,</span><br><span class="line">              b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>
<hr>
<p>在初始化函数中， 长短期记忆网络的隐状态需要返回一个<em>额外</em>的记忆元， 单元的值为0，形状为（批量大小，隐藏单元数）。 因此，我们得到以下的状态初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>
<p>实际模型的定义与我们前面讨论的一样： 提供三个门和一个额外的记忆元。 请注意，只有隐状态才会传递到输出层， 而记忆元$C_t$不直接参与输出计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_lstm_params,</span><br><span class="line">                            init_lstm_state, lstm)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">perplexity 1.3, 17736.0 tokens/sec on cuda:0</span></span><br><span class="line"><span class="string">time traveller for so it will leong go it we melenot ir cove i s</span></span><br><span class="line"><span class="string">traveller care be can so i ngrecpely as along the time dime</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_lstm_86eb9f_66_1.svg" alt="../_images/output_lstm_86eb9f_66_1.svg"></p>
<p>长短期记忆网络是典型的具有重要状态控制的隐变量自回归模型。 多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化。 然而，由于序列的<strong>长距离依赖性</strong>，训练长短期记忆网络 和其他序列模型（例如门控循环单元）的成本是相当高的。 在后面的内容中，我们将讲述更高级的替代模型，如<code>Transformer</code>。</p>
<hr>
<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><p>事实上，我们可以将多层循环神经网络堆叠在一起， 通过对几个简单层的组合，产生了一个灵活的机制。 特别是，数据可能与不同层的堆叠有关。 例如，我们可能希望保持有关金融市场状况 （熊市或牛市）的宏观数据可用， 而微观数据只记录较短期的时间动态。</p>
<p>描述了一个具有$L$个隐藏层的深度循环神经网络， 每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步</p>
<p><img src="https://zh-v2.d2l.ai/_images/deep-rnn.svg" alt="../_images/deep-rnn.svg"></p>
<hr>
<h3 id="函数依赖关系"><a href="#函数依赖关系" class="headerlink" title="函数依赖关系"></a><strong>函数依赖关系</strong></h3><p>我们可以将深度架构中的函数依赖关系形式化，这个架构是由图中描述了$L$个隐藏层构成。后续的讨论主要集中在经典的循环神经网络模型上，但是这些讨论也适应于其他序列模型。</p>
<p>假设在时间步$t$有一个小批量的输入数据$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。同时，将$l^\mathrm{th}$隐藏层（$l=1,\ldots,L$）的隐状态设为$\mathbf{H}_t^{(l)}  \in \mathbb{R}^{n \times h}$（隐藏单元数：$h$），输出层变量设为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。设置$\mathbf{H}_t^{(0)} = \mathbf{X}_t$，第$l$个隐藏层的隐状态使用激活函数$\phi_l$，则：</p>
<script type="math/tex; mode=display">
\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)})</script><p>其中，权重$\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$，$\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$都是第$l$个隐藏层的模型参数。</p>
<p>最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态：</p>
<script type="math/tex; mode=display">
\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q</script><p>其中，权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$都是输出层的模型参数。</p>
<p>与多层感知机一样，隐藏层数目$L$和隐藏单元数目$h$都是超参数。也就是说，它们可以由我们调整的。另外，用门控循环单元或长短期记忆网络的隐状态来代替图中的隐状态进行计算，可以很容易地得到深度门控循环神经网络或深度长短期记忆神经网络。</p>
<hr>
<h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><p>实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的。 简单起见，我们仅示范使用此类内置函数的实现方式。 以长短期记忆网络模型为例， 该代码与之前在上一节中使用的代码非常相似， 实际上唯一的区别是我们指定了层的数量， 而不是使用单一层这个默认值。 像往常一样，我们从加载数据集开始。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>
<p>因为我们有不同的词元，所以输入和输出都选择相同数量，即<code>vocab_size</code>。 隐藏单元的数量仍然是256。 唯一的区别是，我们现在通过<code>num_layers</code>的值来设定隐藏层数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>
<p>由于使用了长短期记忆网络模型来实例化两个层，因此训练速度被大大降低了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">2</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr*<span class="number">1.0</span>, num_epochs, device)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">perplexity 1.0, 186005.7 tokens/sec on cuda:0</span></span><br><span class="line"><span class="string">time traveller for so it will be convenient to speak of himwas e</span></span><br><span class="line"><span class="string">travelleryou can show black is white by argument said filby</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_deep-rnn_d70a11_30_1.svg" alt="../_images/output_deep-rnn_d70a11_30_1.svg"></p>
<hr>
<h2 id="机器翻译与数据集"><a href="#机器翻译与数据集" class="headerlink" title="机器翻译与数据集"></a>机器翻译与数据集</h2><p>语言模型是自然语言处理的关键， 而<em>机器翻译</em>是语言模型最成功的基准测试。 因为机器翻译正是将输入序列转换成输出序列的 <strong><em>序列转换模型</em></strong> 的核心问题。 序列转换模型在各类现代人工智能应用中发挥着至关重要的作用， 因此我们将其做为本章剩余部分和注意力机制的重点。 为此，本节将介绍机器翻译问题及其后文需要使用的数据集</p>
<p><strong><em>机器翻译</em></strong>指的是 将序列从一种语言自动翻译成另一种语言。基于神经网络的方法通常被称为 <strong><em>神经机器翻译</em></strong>， 用于将两种翻译模型区分开来</p>
<p>与之前的语料库是单一语言的语言模型问题存在不同， 机器翻译的数据集是由源语言和目标语言的文本序列对组成的。 因此，我们需要一种完全不同的方法来预处理机器翻译数据集， 而不是复用语言模型的预处理程序。 下面，我们看一下如何将预处理后的数据加载到小批量中用于训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="下载和预处理数据集"><a href="#下载和预处理数据集" class="headerlink" title="下载和预处理数据集"></a>下载和预处理数据集</h3><p>首先，下载一个由<a target="_blank" rel="noopener" href="http://www.manythings.org/anki/">Tatoeba项目的双语句子对</a> 组成的“英－法”数据集，数据集中的每一行都是制表符分隔的文本序列对， 序列对由英文文本序列和翻译后的法语文本序列组成。 请注意，每个文本序列可以是一个句子， 也可以是包含多个句子的一个段落。 在这个将英语翻译成法语的机器翻译问题中， 英语是<strong><em>源语言</em></strong>， 法语是<strong><em>目标语言</em></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;fra-eng.zip&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data_nmt</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;载入“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;fra-eng&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;fra.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">             encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line">raw_text = read_data_nmt()</span><br><span class="line"><span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Go. Va !</span></span><br><span class="line"><span class="string">Hi. Salut !</span></span><br><span class="line"><span class="string">Run!        Cours !</span></span><br><span class="line"><span class="string">Run!        Courez !</span></span><br><span class="line"><span class="string">Who?        Qui ?</span></span><br><span class="line"><span class="string">Wow!        Ça alors !</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>下载数据集后，原始文本数据需要经过几个预处理步骤。 例如，我们用空格代替<em>不间断空格</em>（non-breaking space）， 使用小写字母替换大写字母，并在单词和标点符号之间插入空格。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_nmt</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;预处理“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">no_space</span>(<span class="params">char, prev_char</span>):</span></span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">&#x27;,.!?&#x27;</span>) <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用空格替换不间断空格</span></span><br><span class="line">    <span class="comment"># 使用小写字母替换大写字母</span></span><br><span class="line">    text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>).lower()</span><br><span class="line">    <span class="comment"># 在单词和标点符号之间插入空格</span></span><br><span class="line">    out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span> char</span><br><span class="line">           <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"></span><br><span class="line">text = preprocess_nmt(raw_text)</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">80</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">go .        va !</span></span><br><span class="line"><span class="string">hi .        salut !</span></span><br><span class="line"><span class="string">run !       cours !</span></span><br><span class="line"><span class="string">run !       courez !</span></span><br><span class="line"><span class="string">who ?       qui ?</span></span><br><span class="line"><span class="string">wow !       ça alors !</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="词元化"><a href="#词元化" class="headerlink" title="词元化"></a>词元化</h3><p>在机器翻译中，我们更喜欢单词级词元化 （最先进的模型可能使用更高级的词元化技术）。 下面的<code>tokenize_nmt</code>函数对前<code>num_examples</code>个文本序列对进行词元， 其中每个词元要么是一个词，要么是一个标点符号。 此函数返回两个词元列表：<code>source</code>和<code>target</code>： <code>source[i]</code>是源语言（这里是英语）第$i$个文本序列的词元列表， <code>target[i]</code>是目标语言（这里是法语）第$i$个文本序列的词元列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot;</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br><span class="line"></span><br><span class="line">source, target = tokenize_nmt(text)</span><br><span class="line">source[:<span class="number">6</span>], target[:<span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">([[&#x27;go&#x27;, &#x27;.&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;hi&#x27;, &#x27;.&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;run&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;run&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;who&#x27;, &#x27;?&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;wow&#x27;, &#x27;!&#x27;]],</span></span><br><span class="line"><span class="string"> [[&#x27;va&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;salut&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;cours&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;courez&#x27;, &#x27;!&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;qui&#x27;, &#x27;?&#x27;],</span></span><br><span class="line"><span class="string">  [&#x27;ça&#x27;, &#x27;alors&#x27;, &#x27;!&#x27;]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h3><p>由于机器翻译数据集由语言对组成， 因此我们可以分别为源语言和目标语言构建两个词表。 使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。 为了缓解这一问题，这里我们将出现次数少于2次的低频率词元 视为相同的未知（<code>&lt;unk&gt;</code>）词元。 除此之外，我们还指定了额外的特定词元， 例如在小批量时用于将序列填充到相同长度的填充词元（<code>&lt;pad&gt;</code>）， 以及序列的开始词元（<code>&lt;bos&gt;</code>）和结束词元（<code>&lt;eos&gt;</code>）。 这些特殊词元在自然语言处理任务中比较常用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># source: 包含文本数据的变量，通常是一个列表，其中每个元素是一个句子或单词。</span></span><br><span class="line"><span class="comment"># min_freq=2: 表示只有在数据集中出现至少两次的单词才会被加入到词汇表中。</span></span><br><span class="line"><span class="comment">#             这有助于去除一些罕见词，减少模型的复杂度。</span></span><br><span class="line"><span class="comment"># reserved_tokens=[&#x27;&lt;pad&gt;&#x27;, &#x27;&lt;bos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;]: 指定了一些保留的标记（tokens），</span></span><br><span class="line"><span class="comment">#             分别是填充标记（&lt;pad&gt;）、句子开始标记（&lt;bos&gt;）和句子结束标记（&lt;eos&gt;）。</span></span><br><span class="line"><span class="comment">#             这些标记在训练循环神经网络时非常有用，例如在自然语言处理任务中。</span></span><br><span class="line">src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                      reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取词汇表的长度，即词汇表中不同单词的总数。</span></span><br><span class="line"><span class="comment"># 这个长度包括了因min_freq条件而筛选的单词，以及添加的保留标记。</span></span><br><span class="line"><span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><p>回想一下，语言模型中的序列样本都有一个固定的长度， 无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。 这个固定长度是由语言模型这节中的 <code>num_steps</code>（时间步数或词元数量）参数指定的。 在机器翻译中，每个样本都是由源和目标组成的文本序列对， 其中的每个文本序列可能具有不同的长度。</p>
<p>为了提高计算效率，我们仍然可以通过<strong><em>截断</em></strong>（truncation）和 <strong><em>填充</em></strong>（padding）方式实现一次只处理一个小批量的文本序列。 假设同一个小批量中的每个序列都应该具有相同的长度<code>num_steps</code>， 那么如果文本序列的词元数目少于<code>num_steps</code>时， 我们将继续在其末尾添加特定<code>&lt;pad&gt;</code>词元， 直到其长度达到<code>num_steps</code>； 反之，我们将截断文本序列时，只取其前<code>num_steps</code> 个词元， 并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度， 以便以相同形状的小批量进行加载。</p>
<p>如前所述，下面的<code>truncate_pad</code>函数将截断或填充文本序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line">truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>现在我们定义一个函数，可以将文本序列 转换成小批量数据集用于训练。 我们将特定的<code>&lt;eos&gt;</code>词元添加到所有序列的末尾， 用于表示序列的结束。 当模型通过一个词元接一个词元地生成序列进行预测时， 生成的<code>&lt;eos&gt;</code>词元说明完成了序列输出工作。 此外，我们还记录了每个文本序列的长度， 统计长度时排除了填充词元， 在稍后将要介绍的一些模型会需要这个长度信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将机器翻译的文本序列转换成小批量。</span></span><br><span class="line"><span class="string">    lines: 包含多个句子的列表，其中每个句子是由单词组成的列表。</span></span><br><span class="line"><span class="string">    vocab: 一个Vocab类的实例，用于将文本数据转换为数字索引。</span></span><br><span class="line"><span class="string">    num_steps: 指定每个句子在转换后的最大长度，如果句子长度不足会进行填充，过长则会被截断。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将每个句子中的单词转换为对应的索引。</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在每个索引列表的末尾添加&#x27;&lt;eos&gt;&#x27;标记的索引。</span></span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对每个索引列表进行填充或截断，使它们的长度统一为num_steps。</span></span><br><span class="line">    <span class="comment"># 使用vocab[&#x27;&lt;pad&gt;&#x27;]的索引值作为填充值。</span></span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算每个序列的有效长度（不包括填充部分）。</span></span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回处理后的索引列表和每个列表的有效长度。</span></span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>最后，我们定义<code>load_data_nmt</code>函数来返回数据迭代器， 以及源语言和目标语言的两种词表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回翻译数据集的迭代器和词表。</span></span><br><span class="line"><span class="string">    batch_size: 指定每个小批量的样本数量。</span></span><br><span class="line"><span class="string">    num_steps: 指定每个样本的最大长度（超过这个长度的部分将被截断，不足的部分将被填充）。</span></span><br><span class="line"><span class="string">    num_examples: 从数据集中读取的样本数量，默认为600。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预处理数据集，包括读取数据和初步处理。</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将文本数据分割成源语言数据和目标语言数据，并根据num_examples限制样本数量。</span></span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 为源语言和目标语言各自创建词汇表。</span></span><br><span class="line">    <span class="comment"># min_freq=2表示只包括至少出现两次的单词。</span></span><br><span class="line">    <span class="comment"># reserved_tokens包括特殊标记：&lt;pad&gt;填充标记，&lt;bos&gt;句子开始标记，&lt;eos&gt;句子结束标记。</span></span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将源语言和目标语言的文本数据转换为小批量的数值数据（包括填充或截断处理）。</span></span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将处理后的数据组合成一个元组，准备进行批量读取。</span></span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用d2l.load_array创建数据迭代器，用于批量读取数据。</span></span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回数据迭代器和两个语言的词汇表。</span></span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面我们读出“英语－法语”数据集中的第一个小批量数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X的有效长度:&#x27;</span>, X_valid_len)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y:&#x27;</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y的有效长度:&#x27;</span>, Y_valid_len)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">X: tensor([[ 7, 43,  4,  3,  1,  1,  1,  1],</span></span><br><span class="line"><span class="string">        [44, 23,  4,  3,  1,  1,  1,  1]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">X的有效长度: tensor([4, 4])</span></span><br><span class="line"><span class="string">Y: tensor([[ 6,  7, 40,  4,  3,  1,  1,  1],</span></span><br><span class="line"><span class="string">        [ 0,  5,  3,  1,  1,  1,  1,  1]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">Y的有效长度: tensor([5, 3])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="编码器和解码器"><a href="#编码器和解码器" class="headerlink" title="编码器和解码器"></a>编码器和解码器</h2><p>机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： 第一个组件是一个<strong><em>编码器</em></strong>（encoder）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。 第二个组件是<strong><em>解码器</em></strong>（decoder）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为<strong><em>编码器-解码器</em></strong>（encoder-decoder）架构</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraencoder-decoder.svg" alt="../_images/encoder-decoder.svg"></p>
<p>以英语到法语的机器翻译为例： 给定一个英文的输入序列：“They”“are”“watching”“.”。 首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”， 然后对该状态进行解码， 一个词元接着一个词元地生成翻译后的序列作为输出： “Ils”“regordent”“.”。 由于“编码器－解码器”架构是形成后续章节中不同序列转换模型的基础， 因此本节将把这个架构转换为接口方便后面的代码实现。</p>
<hr>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>在编码器接口中，我们只指定长度可变的序列作为编码器的输入<code>X</code>。 任何继承这个<code>Encoder</code>基类的模型将完成代码实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>在下面的解码器接口中，我们新增一个<code>init_state</code>函数， 用于将编码器的输出（<code>enc_outputs</code>）转换为编码后的状态。 注意，此步骤可能需要额外的输入，例如：输入序列的有效长度， 为了逐个地生成长度可变的词元序列， 解码器在每个时间步都会将输入 （例如：在前一时间步生成的词元）和编码后的状态 映射成当前时间步的输出词元。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h3><p>总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器， 并且还拥有可选的额外的参数。 在前向传播中，编码器的输出用于生成编码状态， 这个状态又被解码器作为其输入的一部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入必要的库</span></span><br><span class="line"><span class="keyword">import</span> nn  <span class="comment"># 假设这是从某个深度学习框架中导入的神经网络库</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Block</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    这个类是编码器-解码器架构的一个通用实现，可以用于多种不同的任务，如机器翻译、</span></span><br><span class="line"><span class="string">    自动摘要等。它由两部分组成：一个编码器和一个解码器。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    encoder: 编码器部分，负责处理输入数据，将其转换成一个内部表示。</span></span><br><span class="line"><span class="string">    decoder: 解码器部分，负责将编码器的输出转换成最终的输出序列。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 初始化编码器和解码器</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播方法。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        enc_X: 编码器的输入数据。</span></span><br><span class="line"><span class="string">        dec_X: 解码器的输入数据。</span></span><br><span class="line"><span class="string">        *args: 可能会传递给编码器和解码器的额外参数。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">        解码器的输出，这通常是一个序列，比如翻译的文本或者生成的文本摘要。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过编码器处理输入</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        <span class="comment"># 使用编码器的输出来初始化解码器的状态</span></span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="comment"># 使用解码器生成最终的输出序列</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="序列到序列学习seq2seq"><a href="#序列到序列学习seq2seq" class="headerlink" title="序列到序列学习seq2seq"></a>序列到序列学习seq2seq</h2><p> 本节，我们将使用两个循环神经网络的编码器和解码器， 并将其应用于<strong><em>序列到序列</em></strong>（sequence to sequence，seq2seq）类的学习任务 </p>
<p>遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态。 换言之，输入序列的信息被<strong><em>编码</em></strong>到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是<strong>基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元</strong>。图中演示了 如何在机器翻译中使用两个循环神经网络进行序列到序列学习</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraseq2seq.svg" alt="../_images/seq2seq.svg"></p>
<p>特定的<code>&lt;eos&gt;</code>表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。 在循环神经网络解码器的初始化时间步，有两个特定的设计决定： 首先，特定的<code>&lt;bos&gt;</code>表示序列开始词元，它是解码器的输入序列的第一个词元。 其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="编码器-1"><a href="#编码器-1" class="headerlink" title="编码器"></a>编码器</h3><p>从技术上讲，编码器将长度可变的输入序列转换成 形状固定的上下文变量$c$， 并且将输入序列的信息在该上下文变量中进行编码</p>
<p>考虑由一个序列组成的样本（批量大小是$1$）。假设输入序列是$x_1, \ldots, x_T$，其中$x_t$是输入文本序列中的第$t$个词元。在时间步$t$，循环神经网络将词元$x_t$的输入特征向量$\mathbf{x}_t$和$\mathbf{h} _{t-1}$（即上一时间步的隐状态）转换为$\mathbf{h}_t$（即当前步的隐状态）。使用一个函数$f$来描述循环神经网络的循环层所做的变换：</p>
<script type="math/tex; mode=display">
\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1})</script><p>总之，编码器通过选定的函数$q$，将所有时间步的隐状态转换为上下文变量：</p>
<script type="math/tex; mode=display">
\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T)</script><p>比如，当选择$q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T$时，上下文变量仅仅是输入序列在最后时间步的隐状态$\mathbf{h}_T$。</p>
<p>现在，让我们实现循环神经网络编码器。 注意，我们使用了<strong><em>嵌入层</em></strong>（embedding layer） 来获得输入序列中每个词元的特征向量。 嵌入层的权重是一个矩阵， 其行数等于输入词表的大小（<code>vocab_size</code>）， 其列数等于特征向量的维度（<code>embed_size</code>）。 对于任意输入词元的索引$i$， 嵌入层获取权重矩阵的第$i$行（从0开始）以返回其特征向量。 另外，本文选择了一个多层门控循环单元来实现编码器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span>(<span class="params">d2l.Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 嵌入层</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span></span><br><span class="line">        <span class="comment"># 输出&#x27;X&#x27;的形状：(batch_size,num_steps,embed_size)</span></span><br><span class="line">        X = self.embedding(X)</span><br><span class="line">        <span class="comment"># 在循环神经网络模型中，第一个轴对应于时间步</span></span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 如果未提及状态，则默认为0</span></span><br><span class="line">        output, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># output的形状:(num_steps,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers,batch_size,num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure>
<p>面，我们实例化上述编码器的实现： 我们使用一个两层门控循环单元编码器，其隐藏单元数为16。 给定一小批量的输入序列<code>X</code>（批量大小为4，时间步为7）。 在完成所有时间步后， 最后一层的隐状态的输出是一个张量（<code>output</code>由编码器的循环层返回）， 其形状为（时间步数，批量大小，隐藏单元数）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">output, state = encoder(X)</span><br><span class="line">output.shape</span><br></pre></td></tr></table></figure>
<p>由于这里使用的是门控循环单元， 所以在最后一个时间步的多层隐状态的形状是 （隐藏层的数量，批量大小，隐藏单元的数量）。 如果使用长短期记忆网络，state中还将包含记忆单元信息。</p>
<hr>
<h3 id="解码器-1"><a href="#解码器-1" class="headerlink" title="解码器"></a>解码器</h3><p>正如上文提到的，编码器输出的上下文变量$\mathbf{c}$对整个输入序列$x_1, \ldots, x_T$进行编码。来自训练数据集的输出序列$y_1, y_2, \ldots, y_{T’}$，对于每个时间步$t’$（与输入序列或编码器的时间步$t$不同），解码器输出$y_{t’}$的概率取决于先前的输出子序列$y_1, \ldots, y_{t’-1}$和上下文变量$\mathbf{c}$，即$P(y_{t’} \mid y_1, \ldots, y_{t’-1}, \mathbf{c})$。</p>
<p>为了在序列上模型化这种条件概率，我们可以使用另一个循环神经网络作为解码器。在输出序列上的任意时间步$t^\prime$，循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$和上下文变量$\mathbf{c}$作为其输入，然后在当前时间步将它们和上一隐状态$\mathbf{s}_{t^\prime-1}$转换为隐状态$\mathbf{s}_{t^\prime}$。因此，可以使用函数$g$来表示解码器的隐藏层的变换：</p>
<script type="math/tex; mode=display">
\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1})</script><p>当实现解码器时， 我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。 这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。 为了进一步包含经过编码的输入序列的信息， 上下文变量在所有的时间步与解码器的输入进行<strong>拼接</strong>。 为了预测输出词元的概率分布， 在循环神经网络解码器的最后一层使用全连接层来变换隐状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> d2l  <span class="comment"># 假设d2l是已经导入的深度学习库</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span>(<span class="params">d2l.Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 词嵌入层：将词索引转换为词向量。</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="comment"># GRU层：循环神经网络核心，处理输入序列和上下文向量。</span></span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        <span class="comment"># 全连接层：将RNN的输出转换为最终的词汇表预测。</span></span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span></span><br><span class="line">        <span class="comment"># 初始化解码器的状态，这里简单地使用编码器的最终隐藏状态。</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># 将输入序列X通过嵌入层转换为词向量。</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># 改变X的形状以匹配RNN的输入要求。</span></span><br><span class="line">        <span class="comment"># 获取上下文向量，并扩展其维度以匹配输入序列的时间步。</span></span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将词向量和上下文向量在特征维度上连接。</span></span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 通过GRU处理输入序列。</span></span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        <span class="comment"># 将RNN的输出通过全连接层转换为词汇预测。</span></span><br><span class="line">        output = self.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 返回输出和新的隐藏状态。</span></span><br><span class="line">        <span class="comment"># output的形状:(batch_size, num_steps, vocab_size)</span></span><br><span class="line">        <span class="comment"># state的形状:(num_layers, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>用与前面提到的编码器中相同的超参数来实例化解码器。 如我们所见，解码器的输出形状变为（批量大小，时间步数，词表大小）， 其中张量的最后一个维度存储预测的词元分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">state = decoder.init_state(encoder(X))</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, state.shape</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraseq2seq-details.svg" alt="../_images/seq2seq-details.svg"></p>
<hr>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在每个时间步，解码器预测了输出词元的概率分布。 类似于语言模型，可以使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化，但是，我们应该将填充词元的预测排除在损失函数的计算之外。</p>
<p>为此，我们可以使用下面的<code>sequence_mask</code>函数 通过零值化屏蔽不相关的项， 以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。 例如，如果两个序列的有效长度（不包括填充词元）分别为1和2， 则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在序列中屏蔽不相关的项。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X: 输入的张量，形状为(batch_size, sequence_length)，包含了需要被屏蔽处理的序列。</span></span><br><span class="line"><span class="string">    valid_len: 一个张量，形状为(batch_size,)，包含了每个序列中有效元素的数量。</span></span><br><span class="line"><span class="string">    value: 屏蔽后的填充值，默认为0。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    经过屏蔽处理的输入张量X。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取输入序列的最大长度。</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 生成一个与序列长度相同的掩码，掩码中每个位置的值表示该位置是否应该被保留。</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment"># 使用生成的掩码对输入X进行屏蔽处理，不需要的元素被设置为指定的value。</span></span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输入序列</span></span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="comment"># 应用序列掩码，指定每个序列的有效长度。</span></span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在，我们可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。 最初，所有预测词元的掩码都设置为1。 一旦给定了有效长度，与填充词元对应的掩码将被设置为0。 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedSoftmaxCELoss</span>(<span class="params">nn.CrossEntropyLoss</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播方法，计算损失。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        pred: 预测值，形状为(batch_size, num_steps, vocab_size)，其中包含了对每个时间步的词汇预测。</span></span><br><span class="line"><span class="string">        label: 真实标签，形状为(batch_size, num_steps)，包含了每个时间步的真实词汇索引。</span></span><br><span class="line"><span class="string">        valid_len: 有效长度，形状为(batch_size,)，指示了每个序列中有效词汇的数量。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">        加权的损失值，其中不应计算损失的部分已被遮蔽。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 创建一个与label形状相同的权重张量，初始全为1。</span></span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        <span class="comment"># 使用sequence_mask函数根据valid_len遮蔽weights中的不相关部分。</span></span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        <span class="comment"># 设置损失计算模式为&#x27;none&#x27;，以获取逐个元素的损失而不是平均损失。</span></span><br><span class="line">        self.reduction = <span class="string">&#x27;none&#x27;</span></span><br><span class="line">        <span class="comment"># 计算未加权的损失，需要调整pred的维度以符合CrossEntropyLoss的输入要求。</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>(MaskedSoftmaxCELoss, self).forward(</span><br><span class="line">            pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        <span class="comment"># 将未加权的损失与权重相乘，然后沿着时间步的维度取平均，得到加权损失。</span></span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 返回加权损失的平均值。</span></span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>在下面的循环训练过程中，如图所示， 特定的序列开始词元（<code>&lt;bos</code>）和 原始的输出序列（不包括序列结束词元<code>&lt;eos&gt;</code>） 拼接在一起作为解码器的输入。 这被称为<strong><em>强制教学</em></strong>（teacher forcing）， 因为原始的输出序列（词元的标签）被送入解码器。 或者，将来自上一个时间步的<strong><em>预测</em></strong> 得到的词元作为解码器的当前输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> d2l  <span class="comment"># 假设d2l是已经导入的深度学习库</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_seq2seq</span>(<span class="params">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xavier_init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用Xavier初始化网络权重&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用权重初始化，并将模型移动到指定的设备上</span></span><br><span class="line">net.apply(xavier_init_weights)</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Adam优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line"><span class="comment"># 使用自定义的MaskedSoftmaxCELoss计算损失</span></span><br><span class="line">loss = MaskedSoftmaxCELoss()</span><br><span class="line">net.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用d2l库的Animator绘制训练过程中的损失变化</span></span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    timer = d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 用于累积损失和词元数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">        <span class="comment"># 准备解码器的输入，包括开始符号</span></span><br><span class="line">        bos = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>], device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)  <span class="comment"># 使用强制教学</span></span><br><span class="line">        Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">        l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">        l.<span class="built_in">sum</span>().backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        d2l.grad_clipping(net, <span class="number">1</span>)  <span class="comment"># 梯度裁剪</span></span><br><span class="line">        num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            metric.add(l.<span class="built_in">sum</span>(), num_tokens)  <span class="comment"># 更新累积的损失和词元数量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每隔一定epoch更新动画</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印训练损失和速度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;tokens/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>现在，在机器翻译数据集上，我们可以 创建和训练一个循环神经网络“编码器－解码器”模型用于序列到序列的学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(<span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                        dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_seq2seq_13725e_171_1.svg" alt="../_images/output_seq2seq_13725e_171_1.svg"></p>
<hr>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraseq2seq-predict.svg" alt="../_images/seq2seq-predict.svg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, tgt_vocab, num_steps,</span></span></span><br><span class="line"><span class="params"><span class="function">                    device, save_attention_weights=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在预测时将net设置为评估模式</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [</span><br><span class="line">        src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    enc_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    enc_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class="line">    <span class="comment"># 添加批量轴</span></span><br><span class="line">    dec_X = torch.unsqueeze(torch.tensor(</span><br><span class="line">        [tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="comment"># 保存注意力权重（稍后讨论）</span></span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="comment"># 一旦序列结束词元被预测，输出序列的生成就完成了</span></span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>我们可以通过与真实的标签序列进行比较来评估预测序列。 BLEU（bilingual evaluation understudy） 最先是用于评估机器翻译的结果， 但现在它已经被广泛用于测量许多应用的输出序列的质量。 原则上说，对于预测序列中的任意$n$元语法（n-grams）， BLEU的评估都是这个$n$元语法是否出现在标签序列中。</p>
<script type="math/tex; mode=display">
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},</script><p>其中$\mathrm{len}_{\text{label}}$表示标签序列中的词元数和$\mathrm{len}_{\text{pred}}$表示预测序列中的词元数，$k$是用于匹配的最长的$n$元语法。</p>
<p>另外，用$p_n$表示$n$元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的$n$元语法的数量，第二个是预测序列中$n$元语法的数量的比率。具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$、$B$、$C$、$D$，我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$。</p>
<p>当预测序列与标签序列完全相同时，BLEU为$1$。此外，由于$n$元语法越长则匹配难度越大，所以BLEU为更长的$n$元语法的精确度分配更大的权重。具体来说，当$p_n$固定时，$p_n^{1/2^n}$会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）。而且，由于预测的序列越短获得的$p_n$值越高，</p>
<p>所以公式中中乘法项之前的系数用于惩罚较短的预测序列。例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，惩罚因子$\exp(1-6/2) \approx 0.14$会降低BLEU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU分数。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    pred_seq: 预测序列，即机器翻译的输出文本。</span></span><br><span class="line"><span class="string">    label_seq: 标签序列，即参考翻译的文本。</span></span><br><span class="line"><span class="string">    k: 最高考虑的n-gram的阶数。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    BLEU分数。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将预测序列和标签序列分割成单词列表。</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算长度惩罚项。</span></span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对于从1到k的每个n，计算n-gram精确度。</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        num_matches = <span class="number">0</span></span><br><span class="line">        label_subs = collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 为标签序列中的所有n-gram建立字典。</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对于预测序列中的每个n-gram，如果在标签序列的字典中，则匹配数加1。</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算n-gram精确度并累乘到分数中。</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回最终的BLEU分数。</span></span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><div class="table-container">
<table>
<thead>
<tr>
<th>特点/模型</th>
<th>GRU</th>
<th>LSTM</th>
<th>深度循环神经网络</th>
<th>Seq2Seq</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>基本结构</strong></td>
<td>2个门（重置门和更新门）</td>
<td>3个门（遗忘门、输入门、输出门）加上一个细胞状态</td>
<td>多层RNN堆叠而成</td>
<td>编码器-解码器结构，可基于RNN、GRU或LSTM</td>
</tr>
<tr>
<td><strong>参数数量</strong></td>
<td>比LSTM少，因为只有两个门</td>
<td>相对较多，因为有三个门和一个细胞状态</td>
<td>随层数增加而大幅增加</td>
<td>取决于编码器和解码器的具体实现</td>
</tr>
<tr>
<td><strong>记忆能力</strong></td>
<td>较好，尤其是在较短的序列中</td>
<td>非常好，能处理长期依赖问题</td>
<td>通过增加层数增强记忆能力</td>
<td>优于单一RNN，因为编码器将所有信息压缩成一个上下文向量</td>
</tr>
<tr>
<td><strong>处理长序列能力</strong></td>
<td>优于基本RNN，但可能不如LSTM</td>
<td>优于GRU和基本RNN</td>
<td>通过深层结构提高处理能力，但容易遇到梯度问题</td>
<td>通过注意力机制（在高级实现中）可以更好地处理长序列</td>
</tr>
<tr>
<td><strong>训练难度</strong></td>
<td>相对容易，参数少</td>
<td>相对困难，参数多</td>
<td>更难，因为层数增多导致的梯度消失或爆炸问题</td>
<td>取决于序列的长度和复杂性，以及是否使用注意力机制</td>
</tr>
<tr>
<td><strong>应用场景</strong></td>
<td>序列数据处理，如语言模型、文本生成</td>
<td>同GRU，但尤其适合需要处理更长依赖的任务</td>
<td>同GRU和LSTM，但适用于更复杂的序列处理任务</td>
<td>机器翻译、文本摘要、语音识别等需要序列转换的任务</td>
</tr>
</tbody>
</table>
</div>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/53892.html"><img class="prev-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">8.0 循环神经网络</div></div></a></div><div class="next-post pull-right"><a href="/post/55896.html"><img class="next-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">10.0 注意力机制</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Candle</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">209</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">现代循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83GRU"><span class="toc-number">1.1.</span> <span class="toc-text">门控循环单元GRU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E9%9A%90%E7%8A%B6%E6%80%81"><span class="toc-number">1.1.1.</span> <span class="toc-text">门控隐状态</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">1.1.2.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9CLSTM"><span class="toc-number">1.2.</span> <span class="toc-text">长短期记忆网络LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E8%AE%B0%E5%BF%86%E5%85%83"><span class="toc-number">1.2.1.</span> <span class="toc-text">门控记忆元</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="toc-number">1.2.2.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.</span> <span class="toc-text">深度循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-number">1.3.1.</span> <span class="toc-text">函数依赖关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="toc-number">1.3.2.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.4.</span> <span class="toc-text">机器翻译与数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.4.1.</span> <span class="toc-text">下载和预处理数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%85%83%E5%8C%96"><span class="toc-number">1.4.2.</span> <span class="toc-text">词元化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8"><span class="toc-number">1.4.3.</span> <span class="toc-text">词表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.4.4.</span> <span class="toc-text">加载数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.4.5.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">1.5.</span> <span class="toc-text">编码器和解码器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.5.1.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">1.5.2.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6"><span class="toc-number">1.5.3.</span> <span class="toc-text">合并</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0seq2seq"><span class="toc-number">1.6.</span> <span class="toc-text">序列到序列学习seq2seq</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-1"><span class="toc-number">1.6.1.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-1"><span class="toc-number">1.6.2.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.6.3.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">1.6.4.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">1.6.5.</span> <span class="toc-text">预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-number">1.6.6.</span> <span class="toc-text">评估</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.7.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recommend-post"><div class="item-headline"><i class="fas fa-dharmachakra"></i><span>相关推荐</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/40793.html" title="1.0 Pytorch入门"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="1.0 Pytorch入门"></a><div class="content"><a class="title" href="/post/40793.html" title="1.0 Pytorch入门">1.0 Pytorch入门</a><time datetime="2023-12-24" title="������ 2023-12-24">2023-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/62658.html" title="2.0 线性回归"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="2.0 线性回归"></a><div class="content"><a class="title" href="/post/62658.html" title="2.0 线性回归">2.0 线性回归</a><time datetime="2023-12-25" title="������ 2023-12-25">2023-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/56727.html" title="3.0 softmax回归"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="3.0 softmax回归"></a><div class="content"><a class="title" href="/post/56727.html" title="3.0 softmax回归">3.0 softmax回归</a><time datetime="2024-02-20" title="������ 2024-02-20">2024-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/58837.html" title="4.0 多层感知机"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="4.0 多层感知机"></a><div class="content"><a class="title" href="/post/58837.html" title="4.0 多层感知机">4.0 多层感知机</a><time datetime="2024-02-21" title="������ 2024-02-21">2024-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/55896.html" title="10.0 注意力机制"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="10.0 注意力机制"></a><div class="content"><a class="title" href="/post/55896.html" title="10.0 注意力机制">10.0 注意力机制</a><time datetime="2024-02-28" title="������ 2024-02-28">2024-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/54714.html" title="7.0 现代卷积神经网络"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="7.0 现代卷积神经网络"></a><div class="content"><a class="title" href="/post/54714.html" title="7.0 现代卷积神经网络">7.0 现代卷积神经网络</a><time datetime="2024-02-23" title="������ 2024-02-23">2024-02-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2024 By Candle</div><div class="framework-info"><span>Powered by </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a></div><div class="footer_custom_text">So Long, and Thanks for All the Fish</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-y/mathjax/3.2.0/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="https://npm.elemecdn.com/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.removeEventListener('scroll', window.tocScrollFn)
  window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>