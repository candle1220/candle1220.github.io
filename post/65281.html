<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>7.0.神经网络：学习 | Night's Watch</title><meta name="keywords" content="机器学习"><meta name="author" content="Candle"><meta name="copyright" content="Candle"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="神经网络：学习代价函数假设神经网络的训练样本有$m$个，每个包含一组输入$x$和一组输出信号$y$，$L$表示神经网络层数，$S_I$表示每层的神经元个数($S_l$表示在$l$层的神经元个数)，$S_L$代表最后一层中处理单元的个数。 将神经网络的分类定义为两种情况：二类分类和多类分类， 二类分类：$S_L&#x3D;1, y&#x3D;0\, or\, 1$表示哪一类； $K$类分类：$S_L&#x3D;k, y_i &#x3D;">
<meta property="og:type" content="article">
<meta property="og:title" content="7.0.神经网络：学习">
<meta property="og:url" content="http://candle1220.github.io/post/65281.html">
<meta property="og:site_name" content="Night&#39;s Watch">
<meta property="og:description" content="神经网络：学习代价函数假设神经网络的训练样本有$m$个，每个包含一组输入$x$和一组输出信号$y$，$L$表示神经网络层数，$S_I$表示每层的神经元个数($S_l$表示在$l$层的神经元个数)，$S_L$代表最后一层中处理单元的个数。 将神经网络的分类定义为两种情况：二类分类和多类分类， 二类分类：$S_L&#x3D;1, y&#x3D;0\, or\, 1$表示哪一类； $K$类分类：$S_L&#x3D;k, y_i &#x3D;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png">
<meta property="article:published_time" content="2023-11-16T12:38:40.558Z">
<meta property="article:modified_time" content="2023-11-23T21:08:16.551Z">
<meta property="article:author" content="Candle">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png"><link rel="shortcut icon" href="/img/candle.png"><link rel="canonical" href="http://candle1220.github.io/post/65281"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#55d5fc","bgDark":"#121212","position":"top-center"},
  source: {
    jQuery: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/jquery/3.6.0/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.js',
      css: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '7.0.神经网络：学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-24 05:08:16'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    document.addEventListener('pjax:complete', detectApple)})(window)</script><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/Swiper/8.0.6/swiper-bundle.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">182</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/background.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Night's Watch</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">7.0.神经网络：学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-16T12:38:40.558Z" title="发表于 2023-11-16 20:38:40">2023-11-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-11-23T21:08:16.551Z" title="更新于 2023-11-24 05:08:16">2023-11-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="7.0.神经网络：学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<h1 id="神经网络：学习"><a href="#神经网络：学习" class="headerlink" title="神经网络：学习"></a>神经网络：学习</h1><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>假设神经网络的训练样本有$m$个，每个包含一组输入$x$和一组输出信号$y$，$L$表示神经网络层数，$S_I$表示每层的<strong>神经元</strong>个数($S_l$表示在$l$层的神经元个数)，$S_L$代表最后一层中处理单元的个数。</p>
<p>将神经网络的分类定义为两种情况：二类分类和多类分类，</p>
<p>二类分类：$S_L=1, y=0\, or\, 1$表示哪一类；</p>
<p>$K$类分类：$S_L=k, y_i = 1$表示分到第$i$类；$(k&gt;2)$</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231116211336769.png" alt="image-20231116211336769"></p>
<p>回顾逻辑回归问题中的代价函数：</p>
<script type="math/tex; mode=display">
J\left(\theta \right)=-\frac{1}{m}\left[\sum_\limits{i=1}^{m}{y}^{(i)}\log{h_\theta({x}^{(i)})}+\left(1-{y}^{(i)}\right)log\left(1-h_\theta\left({x}^{(i)}\right)\right)\right]+\frac{\lambda}{2m}\sum_\limits{j=1}^{n}{\theta_j}^{2}</script><p>在逻辑回归中，我们只有一个输出变量，也只有一个因变量$y$，但是在神经网络中，我们可以有很多输出变量</p>
<p><br/></p>
<p>而在神经网络中，输出层可以有多个输出，假设输出层的神经元数量为<strong>K</strong>，神经网络的代价函数如下：</p>
<script type="math/tex; mode=display">
J(\Theta) = -\frac{1}{m} \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{K} y_k^{(i)} \log \left(h_\Theta(x^{(i)})_k\right) + \left(1 - y_k^{(i)}\right) \log \left(1 - h_\Theta(x^{(i)})_k\right) + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1}} \left(\Theta_{ji}^{(l)}\right)^2</script><ul>
<li>$m$ 是训练样本的数量。</li>
<li>$(y_k^{(i)})$ 是样本 $(i)$ 对于类别 $(k)$ 的实际标签。</li>
<li>$(\hat{y}_k^{(i)})$ 是模型对样本 $(i)$ 对于类别 $(k)$ 的预测。</li>
<li>$(\Theta^{(l)})$ 是第 $(l)$ 层到第 $(l+1)$ 层的权重矩阵。</li>
<li>$L$ 是神经网络的层数。</li>
<li>$s_l$ 是第 $(l)$ 层的神经元数量。</li>
<li>$\lambda$是正则化参数。</li>
</ul>
<p>这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，最后的代价函数是要计算<strong>所有输出层的代价</strong></p>
<p>唯一不同的是，对于每一行特征，我们都会给出$K$个预测，基本上我们可以利用循环，对每一行特征都预测$K$个不同结果，然后在利用循环在$K$个预测中选择可能性最高的一个，将其与$y$中的实际数据进行比较。</p>
<p><br/></p>
<hr>
<h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>之前我们在计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的$h_{\theta}\left(x\right)$</p>
<p>在上一个实验：<code>6.5 ex3：Multi-class Classifification and Neural Networks</code>中，就使用了正向传播方法，很轻松的识别出了手写数字，在ex3中，权重矩阵$\theta$是已经被训练好的，实验的时候只需要调用就行，但是，在实际应用中，权重矩阵的参数我们显然是需要自己进行训练的，这就引出了<strong>反向传播算法</strong></p>
<p><br/></p>
<p>BP（反向传播）的主要目的是通过调整神经网络的权重和偏差，使网络在训练数据上的预测更接近实际标签，从而最小化代价函数。代价函数是衡量网络输出与实际标签之间差异的指标，BP通过<strong>梯度下降</strong>的方式降低这个差异。</p>
<p>既然BP是通过梯度下降来降低这个差异，那么需要先求代价函数$J(\Theta)$的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij}}J\left(\Theta\right)$</p>
<p>举例来说明反向传播算法：假设我们的训练集只有一个样本$\left({x}^{(1)},{y}^{(1)}\right)$，我们的神经网络是一个四层的神经网络，如图：</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231116223513082.png" alt="image-20231116223513082"></p>
<p><br/></p>
<hr>
<h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的。</p>
<p>我们通常假设初始参数为正负$ε$之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">epsilon = <span class="number">0.01</span></span><br><span class="line">theta = np.random.uniform(low=-epsilon, high=epsilon, size=(<span class="number">10</span>, <span class="number">11</span>))</span><br></pre></td></tr></table></figure>
<p><br/></p>
<hr>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>先用前向传播算法算出代价$J(\Theta)$：</p>
<p>神经网络的正向传播计算如下：</p>
<ol>
<li><p>输入层（第1层）：<br>$[ a^{(1)} = x^{(1)} ]$</p>
</li>
<li><p>隐藏层1（第2层）：<br>$[ z^{(2)} = \Theta^{(1)}a^{(1)} ]$<br>$[ a^{(2)} = g(z^{(2)}) ]$</p>
<p>其中，$(g)$ 是激活函数，$(\Theta^{(1)})$ 是连接输入层和隐藏层1的权重矩阵。</p>
</li>
<li><p>隐藏层2（第3层）：<br>$[ z^{(3)} = \Theta^{(2)}a^{(2)} ]$<br>$[ a^{(3)} = g(z^{(3)}) ]$</p>
<p>其中，$(\Theta^{(2)})$ 是连接隐藏层1和隐藏层2的权重矩阵。</p>
</li>
<li><p>输出层（第4层）：<br>$[ z^{(4)} = \Theta^{(3)}a^{(3)} ]$<br>$[ a^{(4)} = h_\Theta(x^{(1)}) = g(z^{(4)}) ]$</p>
<p>其中，$(\Theta^{(3)})$ 是连接隐藏层2和输出层的权重矩阵，$(h_\Theta(x^{(1)}))$ 是神经网络的输出。</p>
</li>
</ol>
<p><br/></p>
<hr>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>接下来，使用反向传播算法，首先，我们定义误差项为</p>
<script type="math/tex; mode=display">
\delta^{(L)} = \frac{\partial J}{\partial z^{(L)}}</script><p>其中$z^{(L)}$是输出层的净输入。通过链式法则，我们可以将$\delta^{(L)}$传播到前一层，形成对每个参数的偏导数。这样，我们可以使用梯度下降等优化算法来调整网络参数，以最小化损失函数</p>
<p>而输出层的误差显然是：$\delta^{(4)}=a^{(4)}-y$</p>
<p>证明如下：</p>
<p>首先，我们有输出层的激活值 $(a^{(4)})$ 和损失函数 $(J)$ 之间的关系。在分类问题中，常用的损失函数是交叉熵损失函数。假设我们的网络是针对二分类问题，输出层使用 $sigmoid$ 激活函数，那么交叉熵损失函数可以表示为：</p>
<script type="math/tex; mode=display">
J(y, a^{(4)}) = - y \log(a^{(4)}) - (1 - y) \log(1 - a^{(4)})</script><p>其中，$(y)$ 是实际标签（0或1），$a^{(4)}$ 是输出层的激活值。</p>
<p>接下来，我们来计算输出层激活值 $(a^{(4)})$ 对净输入值 $(z^{(4)})$ 的偏导数。假设输出层使用 $sigmoid$ 激活函数，有 $(a^{(4)}) = g(z^{(4)}))$，其中 $(g(\cdot))$ 是 $sigmoid$ 函数。$sigmoid$ 函数的导数可以表示为 </p>
<script type="math/tex; mode=display">
(g'(z^{(4)}) = a^{(4)} \cdot (1 - a^{(4)}))。</script><p>应用链式法则：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial z^{(4)}} = \frac{\partial J}{\partial a^{(4)}} \cdot \frac{\partial a^{(4)}}{\partial z^{(4)}}</script><p>将损失函数对激活值的偏导数代入：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial z^{(4)}} = -(\frac{y}{a^{(4)}} - \frac{(1-y)}{(1-a^{(4)})}) \cdot a^{(4)} \cdot (1 - a^{(4)})</script><p>化简上式：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial z^{(4)}} = a^{(4)} - y</script><p>这个结果表示了输出层误差项与实际标签之间的差异，与损失函数对于输出层净输入值的偏导数是一致的。</p>
<p><br/></p>
<hr>
<p>然后计算上一层的误差项 ($\delta^{(3)}$)：</p>
<script type="math/tex; mode=display">
\delta^{(3)} = \frac{\partial J}{\partial z^{(3)}}</script><p>使用链式法则，将 $(\frac{\partial J}{\partial z^{(3)}})$ 分解为两部分：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial z^{(3)}} = \frac{\partial J}{\partial a^{(3)}} \cdot \frac{\partial a^{(3)}}{\partial z^{(3)}}</script><p>考虑激活函数的导数：</p>
<script type="math/tex; mode=display">
\frac{\partial a^{(3)}}{\partial z^{(3)}} = g'\left(z^{(3)}\right)</script><p>将上述结果代入步骤2：</p>
<script type="math/tex; mode=display">
\delta^{(3)} = \frac{\partial J}{\partial a^{(3)}} \cdot g'\left(z^{(3)}\right)</script><p><br/></p>
<hr>
<p>重点是计算$\frac{\partial J}{\partial a^{(3)}}$项：推论如下：</p>
<p>根据链式法则，有：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial a^{(3)}} = \frac{\partial J}{\partial z^{(4)}} \cdot \frac{\partial z^{(4)}}{\partial a^{(3)}}</script><p>其中，$(z^{(4)})$ 是输出层的净输入值，而 $(a^{(3)})$ 是输出层的激活值。</p>
<p>我们知道 $\frac{\partial J}{\partial z^{(4)}} = \delta^{(4)}$，根据之前的讨论。接下来，我们需要计算 ($\frac{\partial z^{(4)}}{\partial a^{(3)}}$)，在一般的神经网络中，输出层的净输入值 ($z^{(4)}$) 与前一层的激活值 ($a^{(3)}$) 之间有以下关系：</p>
<script type="math/tex; mode=display">
 z^{(4)} = \Theta^{(3)} \cdot a^{(3)}</script><p>其中，$(\Theta^{(3)})$ 是连接输出层和前一隐藏层的权重矩阵。对上式关于 $(a^{(3)})$ 求偏导数，得到：</p>
<script type="math/tex; mode=display">
\frac{\partial z^{(4)}}{\partial a^{(3)}} = \Theta^{(3)}</script><p>现在，将这两个结果代入链式法则的表达式：</p>
<script type="math/tex; mode=display">
 \frac{\partial J}{\partial a^{(3)}} = \delta^{(4)} \cdot \left(\Theta^{(3)}\right)^T</script><p>这就得到了</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial a^{(3)}} = \left(\Theta^{(3)}\right)^T \cdot \delta^{(4)}</script><p>这个结果表明，损失函数对于前一层的激活值的偏导数可以通过将输出层误差项与权重矩阵的转置相乘得到。这是误差反向传播算法中计算梯度的一部分</p>
<p> <br/></p>
<p>代入上述结果：</p>
<script type="math/tex; mode=display">
\delta^{(3)} = \left(\Theta^{(3)}\right)^T \cdot \delta^{(4)} \cdot g'\left(z^{(3)}\right)</script><p><br/></p>
<hr>
<h3 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h3><p>通过上述方法可以算出所有误差的表达式：</p>
<script type="math/tex; mode=display">
\delta^{(2)}=(\Theta^{(2)})^{T}\delta^{(3)}\ast g'(z^{(2)})</script><p>$\delta^{(1)}=0$   因为输入层是输入变量，不存在误差，我们有了所有的误差的表达式后，便可以计算权重矩阵的梯度</p>
<p><br/></p>
<p>权重矩阵梯度的具体推导：</p>
<p>假设我们有一个神经网络，其中 $(a^{(l)})$ 是第 $(l)$ 层的激活值（包括输入层），$(\Theta^{(l)})$ 是连接第 $(l)$ 层和第 $(l+1)$ 层的权重矩阵，$(\delta^{(l+1)})$ 是第 $(l+1)$ 层的误差项。</p>
<p>激活值和净输入的关系为：$(a^{(l+1)} = g(z^{(l+1)}))$</p>
<p>其中$，(g(\cdot))$ 是激活函数，$(z^{(l+1)})$ 是第 $(l+1)$ 层的<strong>净输入</strong></p>
<p>我们知道误差项的定义是：</p>
<script type="math/tex; mode=display">
\delta^{(l+1)} = \frac{\partial J}{\partial z^{(l+1)}}</script><p>现在，我们来计算损失函数对于第 $(l)$ 层的权重矩阵 $(\Theta^{(l)})$ 的偏导数，即<strong>梯度</strong>。使用链式法则：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \Theta^{(l)}} = \frac{\partial J}{\partial z^{(l+1)}} \cdot \frac{\partial z^{(l+1)}}{\partial \Theta^{(l)}}</script><p>我们知道 $(\frac{\partial J}{\partial z^{(l+1)}} = \delta^{(l+1)})$，然后考虑 $(z^{(l+1)})$ 与权重矩阵的关系：</p>
<script type="math/tex; mode=display">
z^{(l+1)} = \Theta^{(l)} \cdot a^{(l)}</script><p>对上式对 $(\Theta^{(l)})$ 求偏导数得到：</p>
<script type="math/tex; mode=display">
\frac{\partial z^{(l+1)}}{\partial \Theta^{(l)}} = a^{(l)}</script><p>现在，将上述结果代入梯度计算的表达式：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \Theta^{(l)}} = \delta^{(l+1)} \cdot \frac{\partial z^{(l+1)}}{\partial \Theta^{(l)}} = \delta^{(l+1)} \cdot a^{(l)T}</script><p>最终，我们得到了权重矩阵的梯度计算公式：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \Theta^{(l)}} = \delta^{(l+1)} \cdot a^{(l)T}</script><p>对于偏置的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \Theta^{(l)}} = \delta^{(l+1)}</script><p>这个公式表示了损失函数对于第 $(l)$ 层权重矩阵的偏导数，是反向传播算法中梯度计算的关键步骤之一。</p>
<p><br/></p>
<hr>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>首先将梯度简写为$\Delta_{ij}^{(l)}$，第 $l$  层的第 $i$ 个激活单元受到第 $j$ 个参数影响而导致的误差。</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \Theta^{(l)}} = \delta^{(l+1)}_i \cdot a^{(l)T}_j=\Delta^{(l)}</script><p>换句话说，$(\delta^{(l+1)} \cdot a^{(l)T})$ 是一个矩阵，其中第 $(i)$ 行、第 $(j)$ 列的元素就是 $(\Delta_{ij}^{(l)})$。这个矩阵表示了损失函数对于权重矩阵的梯度，用于更新权重以减小损失。</p>
<p>在求出了$\Delta_{ij}^{(l)}$之后，我们便可以计算代价函数的偏导数了，计算方法如下：</p>
<p>$ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$              ${if}\; j \neq  0$</p>
<p>$ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}$                             ${if}\; j = 0$</p>
<p>在神经网络的训练中，我们通过梯度下降算法来不断调整网络中的权重，使得网络的输出更接近实际值。为了防止过拟合，通常在损失函数中加入<strong>正则化项</strong>。</p>
<p>这里的公式是在计算正则化项的梯度，用于更新权重时的调整。让我们逐步解释：</p>
<ol>
<li><p><strong>$\Delta_{ij}^{(l)}:$</strong> 这是神经网络中权重矩阵 $\Theta^{(l)}$的梯度项。它表示损失函数对于权重的影响，是梯度下降算法的关键。</p>
</li>
<li><p><strong>$\frac{1}{m}$:</strong> </p>
</li>
<li><p><strong>$\lambda$:</strong> 这是正则化参数，它控制正则化项的强度。如果 $(\lambda)$ 较大，那么正则化的影响就会更显著。</p>
</li>
<li><p><strong>$\Theta_{ij}^{(l)}$:</strong> 这是权重矩阵中的具体权重值。</p>
</li>
</ol>
<p>现在，整个公式的含义是，我们通过将权重的梯度项取平均，然后加上正则化项，来计算最终用于更新权重的梯度。这个过程旨在<strong>防止过拟合</strong>，通过对权重的大小进行调整，以使模型更<strong>一般化</strong></p>
<p><br/></p>
<hr>
<h3 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h3><p>我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。</p>
<p>为了避免这样的问题，我们采取一种叫做梯度的数值检验（<strong>Numerical Gradient Checking</strong>）方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。</p>
<p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 $\theta$，我们计算出在 $\theta$-$\varepsilon $ 处和 $\theta$+$\varepsilon $ 的代价值（$\varepsilon $是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在 $\theta$ 处的代价值。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231118041044629.png" alt="image-20231118041044629"></p>
<p>梯度检验是一种验证数值计算中梯度计算是否正确的技术。在机器学习中，特别是在神经网络训练中，梯度是优化算法（如梯度下降）的关键部分。通过梯度检验，可以验证你的梯度计算是否正确，以确保你的模型在学习过程中能够正确地更新参数。</p>
<ol>
<li><p><strong>计算数值梯度：</strong> 对于每个参数 $(\theta_i)$，通过微小的变化 $(\epsilon)$ 来计算数值梯度。数值梯度的计算方式是：</p>
<script type="math/tex; mode=display">
 \text{num\_grad}_i = \frac{J(\theta_1, \theta_2, ..., \theta_i + \epsilon, ..., \theta_n) - J(\theta_1, \theta_2, ..., \theta_i - \epsilon, ..., \theta_n)}{2\epsilon}</script><p>其中，$(J)$ 是损失函数，$(\theta_i)$ 是第 $(i)$ 个参数。</p>
</li>
<li><p><strong>计算反向传播梯度：</strong> 使用反向传播算法计算损失函数对于每个参数的梯度。</p>
</li>
<li><p><strong>比较：</strong> 将数值梯度与反向传播梯度进行比较。如果两者非常接近，那么可以认为梯度计算是正确的。</p>
</li>
</ol>
<p><br/></p>
<p>梯度检验是一种相对慢速但非常强大的调试工具。在实际应用中，梯度检验通常只在开发和调试阶段使用，因为它相<strong>对于反向传播来说计算量较大</strong>，不适合在大规模数据上进行频繁的计算。当然，梯度检验也有一些局限性，例如对于非连续、非凸或计算复杂的损失函数可能不太适用。</p>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>反向传播（Backpropagation，简称BP）是一种用于训练神经网络的优化算法。以下是反向传播算法包含的主要步骤：</p>
<ol>
<li><p><strong>初始化参数：</strong> 随机初始化神经网络的权重和偏置。</p>
</li>
<li><p><strong>正向传播：</strong> 使用当前的参数进行正向传播，计算网络的输出。每一层的激活值  $(a^{(l)} )$ 可以通过应用激活函数到该层的净输入值  $(z^{(l)} )$ 而得到。</p>
<script type="math/tex; mode=display">
z^{(l)} = \Theta^{(l-1)} a^{(l-1)} + b^{(l-1)}</script><script type="math/tex; mode=display">
 a^{(l)} = g(z^{(l)})</script><p>其中， $(\Theta^{(l-1)} )$ 是连接第  $(l-1 )$ 层和第  $(l )$ 层的权重矩阵， $(b^{(l-1)} )$ 是偏置。</p>
</li>
<li><p><strong>计算损失：</strong> 使用网络的输出和实际标签计算损失函数。</p>
<script type="math/tex; mode=display">
 J = \text{cost}(a^{(L)}, y)</script><p>其中， $(L )$ 是输出层的索引， $(a^{(L)} )$ 是输出层的激活值， $(y )$ 是实际标签。</p>
</li>
<li><p><strong>反向传播：</strong> 通过反向传播算法计算每一层的误差项  (\delta^{(l)} )。误差项的计算方式取决于激活函数的选择和损失函数的形式。</p>
<script type="math/tex; mode=display">
\delta^{(L)} = \frac{\partial J}{\partial z^{(L)}}</script><script type="math/tex; mode=display">
\delta^{(l)} = \left(\Theta^{(l)}\right)^T \delta^{(l+1)} \ast g'\left(z^{(l)}\right)</script></li>
</ol>
<ol>
<li><p><strong>计算梯度：</strong> 利用误差项  $(\delta^{(l)} )$ 计算损失函数对于参数的偏导数，即梯度。</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \Theta^{(l)}} = a^{(l-1)} \cdot \delta^{(l)T}</script></li>
<li><p><strong>正则化梯度</strong>： </p>
<script type="math/tex; mode=display">
D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}</script></li>
<li><p><strong>梯度下降更新参数：</strong> 使用梯度下降或其他优化算法，根据计算得到的梯度更新网络的参数。</p>
<script type="math/tex; mode=display">
\Theta^{(l)} = \Theta^{(l)} - \alpha \frac{\partial J}{\partial \Theta^{(l)}}</script><p>其中， $(\alpha )$ 是学习率，用于控制更新步长。</p>
</li>
<li><p><strong>重复：</strong> 重复步骤 2-7，直到损失函数收敛或达到预定的迭代次数。</p>
</li>
</ol>
<p>这些步骤组成了反向传播算法的基本流程，用于训练神经网络。反向传播通过不断地调整网络参数，使得模型逐渐学到数据中的模式，从而提高模型的性能。</p>
<p><br/></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/18106.html"><img class="prev-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">6.5 ex3：Multi-class Classifification and Neural Networks</div></div></a></div><div class="next-post pull-right"><a href="/post/36821.html"><img class="next-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">7.5 ex4：Neural Networks Learning</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Candle</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">182</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">神经网络：学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">反向传播算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.2.1.</span> <span class="toc-text">随机初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.2.2.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.2.3.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.2.4.</span> <span class="toc-text">计算梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.2.5.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="toc-number">1.2.6.</span> <span class="toc-text">梯度检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.2.7.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recommend-post"><div class="item-headline"><i class="fas fa-dharmachakra"></i><span>相关推荐</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/59772.html" title="11.0.无监督学习"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png" alt="11.0.无监督学习"></a><div class="content"><a class="title" href="/post/59772.html" title="11.0.无监督学习">11.0.无监督学习</a><time datetime="2023-11-25" title="������ 2023-11-25">2023-11-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/12550.html" title="Python数据分析"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typora7-200525101140K8.png" alt="Python数据分析"></a><div class="content"><a class="title" href="/post/12550.html" title="Python数据分析">Python数据分析</a><time datetime="2023-10-27" title="������ 2023-10-27">2023-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/3076.html" title="10.0.支持向量机"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png" alt="10.0.支持向量机"></a><div class="content"><a class="title" href="/post/3076.html" title="10.0.支持向量机">10.0.支持向量机</a><time datetime="2023-11-23" title="������ 2023-11-23">2023-11-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/20569.html" title="1.机器学习绪论"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png" alt="1.机器学习绪论"></a><div class="content"><a class="title" href="/post/20569.html" title="1.机器学习绪论">1.机器学习绪论</a><time datetime="2023-10-17" title="������ 2023-10-17">2023-10-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/1111.html" title="10.5 ex6：SVM"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png" alt="10.5 ex6：SVM"></a><div class="content"><a class="title" href="/post/1111.html" title="10.5 ex6：SVM">10.5 ex6：SVM</a><time datetime="2023-11-24" title="������ 2023-11-24">2023-11-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/30433.html" title="11.5 ex7：k-means and PCA"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231020181029461.png" alt="11.5 ex7：k-means and PCA"></a><div class="content"><a class="title" href="/post/30433.html" title="11.5 ex7：k-means and PCA">11.5 ex7：k-means and PCA</a><time datetime="2023-11-28" title="������ 2023-11-28">2023-11-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2023 By Candle</div><div class="framework-info"><span>Powered by </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a></div><div class="footer_custom_text">So Long, and Thanks for All the Fish</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-y/mathjax/3.2.0/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="https://npm.elemecdn.com/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.removeEventListener('scroll', window.tocScrollFn)
  window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>