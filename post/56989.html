<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>11.0 计算机视觉 | Night's Watch</title><meta name="keywords" content="深度学习"><meta name="author" content="Candle"><meta name="copyright" content="Candle"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="计算机视觉图像增广图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。 此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。 例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。 我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。 可以说，图像增广技术对于">
<meta property="og:type" content="article">
<meta property="og:title" content="11.0 计算机视觉">
<meta property="og:url" content="http://candle1220.github.io/post/56989.html">
<meta property="og:site_name" content="Night&#39;s Watch">
<meta property="og:description" content="计算机视觉图像增广图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。 此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。 例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。 我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。 可以说，图像增广技术对于">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png">
<meta property="article:published_time" content="2024-03-01T12:10:44.552Z">
<meta property="article:modified_time" content="2024-04-02T12:06:24.994Z">
<meta property="article:author" content="Candle">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png"><link rel="shortcut icon" href="/img/candle.png"><link rel="canonical" href="http://candle1220.github.io/post/56989"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#55d5fc","bgDark":"#121212","position":"top-center"},
  source: {
    jQuery: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/jquery/3.6.0/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.js',
      css: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '11.0 计算机视觉',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-02 20:06:24'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    document.addEventListener('pjax:complete', detectApple)})(window)</script><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/Swiper/8.0.6/swiper-bundle.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">220</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20240402195814166.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Night's Watch</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">11.0 计算机视觉</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-01T12:10:44.552Z" title="发表于 2024-03-01 20:10:44">2024-03-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-02T12:06:24.994Z" title="更新于 2024-04-02 20:06:24">2024-04-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="11.0 计算机视觉"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h1><h2 id="图像增广"><a href="#图像增广" class="headerlink" title="图像增广"></a>图像增广</h2><p>图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。 此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。 例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。 我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。 可以说，图像增广技术对于AlexNet的成功是必不可少的</p>
<h3 id="翻转和裁剪"><a href="#翻转和裁剪" class="headerlink" title="翻转和裁剪"></a>翻转和裁剪</h3><p>使用<code>transforms</code>模块来创建<code>RandomFlipLeftRight</code>实例，这样就各有50%的几率使图像向左或向右翻转。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.RandomHorizontalFlip())</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_image-augmentation_7d0887_42_0.svg" alt="../_images/output_image-augmentation_7d0887_42_0.svg"></p>
<p>创建一个<code>RandomFlipTopBottom</code>实例，使图像各有50%的几率向上或向下翻转</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.RandomVerticalFlip())</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_image-augmentation_7d0887_54_0.svg" alt="../_images/output_image-augmentation_7d0887_54_0.svg"></p>
<p>下面的代码将随机裁剪一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5～2之间随机取值。 然后，区域的宽度和高度都被缩放到200像素。 在本节中（除非另有说明），$a$和$b$之间的随机数指的是在区间$[a,b]$中通过均匀采样获得的连续值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shape_aug = torchvision.transforms.RandomResizedCrop(</span><br><span class="line">    (<span class="number">200</span>, <span class="number">200</span>), scale=(<span class="number">0.1</span>, <span class="number">1</span>), ratio=(<span class="number">0.5</span>, <span class="number">2</span>))</span><br><span class="line">apply(img, shape_aug)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_image-augmentation_7d0887_66_0.svg" alt="../_images/output_image-augmentation_7d0887_66_0.svg"></p>
<hr>
<h3 id="改变颜色"><a href="#改变颜色" class="headerlink" title="改变颜色"></a>改变颜色</h3><p>另一种增广方法是改变颜色。 我们可以改变图像颜色的四个方面：亮度、对比度、饱和度和色调。 在下面的示例中，我们随机更改图像的亮度，随机值为原始图像的50%（1−0.5）到150%（1+0.5）之间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.ColorJitter(</span><br><span class="line">    brightness=<span class="number">0.5</span>, contrast=<span class="number">0</span>, saturation=<span class="number">0</span>, hue=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_image-augmentation_7d0887_78_0.svg" alt="../_images/output_image-augmentation_7d0887_78_0.svg"></p>
<p>同样，我们可以随机更改图像的色调。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.ColorJitter(</span><br><span class="line">    brightness=<span class="number">0</span>, contrast=<span class="number">0</span>, saturation=<span class="number">0</span>, hue=<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_image-augmentation_7d0887_90_0.svg" alt="../_images/output_image-augmentation_7d0887_90_0.svg"></p>
<hr>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>假如我们想识别图片中不同类型的椅子，然后向用户推荐购买链接。 一种可能的方法是首先识别100把普通椅子，为每把椅子拍摄1000张不同角度的图像，然后在收集的图像数据集上训练一个分类模型。 尽管这个椅子数据集可能大于Fashion-MNIST数据集，但实例数量仍然不到ImageNet中的十分之一。 适合ImageNet的复杂模型可能会在这个椅子数据集上过拟合。 此外，由于训练样本数量有限，训练模型的准确性可能无法满足实际要求。</p>
<p>为了解决上述问题，一个显而易见的解决方案是收集更多的数据。 但是，收集和标记数据可能需要大量的时间和金钱。 例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究资金。 尽管目前的数据收集成本已大幅降低，但这一成本仍不能忽视。</p>
<p>另一种解决方案是应用<strong><em>迁移学习</em></strong>（transfer learning）将从<em>源数据集</em>学到的知识迁移到<em>目标数据集</em>。 例如，尽管ImageNet数据集中的大多数图像与椅子无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。 这些类似的特征也可能有效地识别椅子。</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>微调包括以下四个步骤。</p>
<ol>
<li>在源数据集（例如ImageNet数据集）上预训练神经网络模型，即<strong><em>源模型</em></strong>。</li>
<li>创建一个新的神经网络模型，即<strong><em>目标模型</em></strong>。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。</li>
<li>向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。<img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorafinetune.svg" alt="../_images/finetune.svg"></li>
</ol>
<p>当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。</p>
<hr>
<h3 id="热狗识别"><a href="#热狗识别" class="headerlink" title="热狗识别"></a>热狗识别</h3><p>让我们通过具体案例演示微调：热狗识别。 我们将在一个小型数据集上微调ResNet模型。该模型已在ImageNet数据集上进行了预训练。 这个小型数据集包含数千张包含热狗和不包含热狗的图像，我们将使用微调模型来识别图像中是否包含热狗。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<p>我们使用的热狗数据集来源于网络。 该数据集包含1400张热狗的“正类”图像，以及包含尽可能多的其他食物的“负类”图像。 含着两个类别的1000张图片用于训练，其余的则用于测试。</p>
<p>解压下载的数据集，我们获得了两个文件夹<code>hotdog/train</code>和<code>hotdog/test</code>。 这两个文件夹都有<code>hotdog</code>（有热狗）和<code>not-hotdog</code>（无热狗）两个子文件夹， 子文件夹内都包含相应类的图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;hotdog&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;hotdog.zip&#x27;</span>,</span><br><span class="line">                         <span class="string">&#x27;fba480ffa8aa7e0febbb511d181409f899b9baa5&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data_dir = d2l.download_extract(<span class="string">&#x27;hotdog&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>我们创建两个实例来分别读取训练和测试数据集中的所有图像文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>))</span><br><span class="line">test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>下面显示了前8个正类样本图片和最后8张负类样本图片。正如所看到的，图像的大小和纵横比各有不同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hotdogs = [train_imgs[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">not_hotdogs = [train_imgs[-i - <span class="number">1</span>][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">d2l.show_images(hotdogs + not_hotdogs, <span class="number">2</span>, <span class="number">8</span>, scale=<span class="number">1.4</span>);</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_fine-tuning_368659_42_0.png" alt="../_images/output_fine-tuning_368659_42_0.png"></p>
<p>在训练期间，我们首先从图像中裁切随机大小和随机长宽比的区域，然后将该区域缩放为224×224输入图像。 在测试过程中，我们将图像的高度和宽度都缩放到256像素，然后裁剪中央224×224区域作为输入。 此外，对于RGB（红、绿和蓝）颜色通道，我们分别<strong><em>标准化</em></strong>每个通道。 具体而言，该通道的每个值减去该通道的平均值，然后将结果除以该通道的标准差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用RGB通道的均值和标准差，以标准化每个通道</span></span><br><span class="line">normalize = torchvision.transforms.Normalize(</span><br><span class="line">    [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line">train_augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    normalize])</span><br><span class="line"></span><br><span class="line">test_augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize([<span class="number">256</span>, <span class="number">256</span>]),</span><br><span class="line">    torchvision.transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    normalize])</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>定义和初始化模型</strong></p>
<p>我们使用在ImageNet数据集上预训练的ResNet-18作为源模型。 在这里，我们指定<code>pretrained=True</code>以自动下载预训练的模型参数。 如果首次使用此模型，则需要连接互联网才能下载。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>预训练的源模型实例包含许多特征层和一个输出层<code>fc</code>。 此划分的主要目的是促进对除输出层以外所有层的模型参数进行微调。 下面给出了源模型的成员变量<code>fc</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net.fc</span><br><span class="line"><span class="comment">#Linear(in_features=512, out_features=1000, bias=True)</span></span><br></pre></td></tr></table></figure>
<p>在ResNet的全局平均汇聚层后，全连接层转换为ImageNet数据集的1000个类输出。 之后，我们构建一个新的神经网络作为目标模型。 它的定义方式与预训练源模型的定义方式相同，只是最终层中的输出数量被设置为目标数据集中的类数（而不是1000个）。</p>
<p>在下面的代码中，目标模型<code>finetune_net</code>中成员变量<code>features</code>的参数被初始化为源模型相应层的模型参数。 由于模型参数是在ImageNet数据集上预训练的，并且足够好，因此通常只需要较小的学习率即可微调这些参数。</p>
<p>成员变量<code>output</code>的参数是随机初始化的，通常需要更高的学习率才能从头开始训练。 假设<code>Trainer</code>实例中的学习率为$\eta$，我们将成员变量<code>output</code>中参数的学习率设置为10$\eta$。</p>
<hr>
<h3 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h3><p>首先，我们定义了一个训练函数<code>train_fine_tuning</code>，该函数使用微调，因此可以多次调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fine_tuning</span>(<span class="params">net, learning_rate, batch_size=<span class="number">128</span>, num_epochs=<span class="number">5</span>, param_group=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="comment"># 加载训练数据集。ImageFolder是一个通用的数据加载器，能够从文件夹中按目录结构自动标注图片。</span></span><br><span class="line">    <span class="comment"># train_augs是训练数据的预处理和增强操作。</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>), transform=train_augs),</span><br><span class="line">        batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载测试数据集。与训练数据集相似，但是通常不进行数据增强。</span></span><br><span class="line">    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>), transform=test_augs),</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 尝试使用所有可用的GPU进行训练。</span></span><br><span class="line">    devices = d2l.try_all_gpus()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置损失函数。这里使用交叉熵损失函数，适用于多分类问题。</span></span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据param_group标志决定是否对输出层的模型参数使用不同的学习率。</span></span><br><span class="line">    <span class="keyword">if</span> param_group:</span><br><span class="line">        <span class="comment"># 筛选出除了全连接层外的所有参数，这些参数使用默认的学习率。</span></span><br><span class="line">        params_1x = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()</span><br><span class="line">                     <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;fc.weight&quot;</span>, <span class="string">&quot;fc.bias&quot;</span>]]</span><br><span class="line">        <span class="comment"># 为优化器设置两组参数：一组是除了全连接层的参数，另一组是全连接层的参数。</span></span><br><span class="line">        <span class="comment"># 全连接层的参数使用十倍的学习率。</span></span><br><span class="line">        trainer = torch.optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: params_1x&#125;,</span><br><span class="line">                                   &#123;<span class="string">&#x27;params&#x27;</span>: net.fc.parameters(),</span><br><span class="line">                                    <span class="string">&#x27;lr&#x27;</span>: learning_rate * <span class="number">10</span>&#125;],</span><br><span class="line">                                  lr=learning_rate, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不区分参数组，所有参数都使用相同的学习率。</span></span><br><span class="line">        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,</span><br><span class="line">                                  weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用d2l.train_ch13函数进行训练。这个函数封装了训练循环，包括前向传播、计算损失、反向传播和参数更新。</span></span><br><span class="line">    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们使用较小的学习率，通过<strong><em>微调</em></strong> 预训练获得的模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_fine_tuning(finetune_net, <span class="number">5e-5</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_fine-tuning_368659_108_1.svg" alt="../_images/output_fine-tuning_368659_108_1.svg"></p>
<p>为了进行比较，我们定义了一个相同的模型，但是将其所有模型参数初始化为随机值。 由于整个模型需要从头开始训练，因此我们需要使用更大的学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scratch_net = torchvision.models.resnet18()</span><br><span class="line">scratch_net.fc = nn.Linear(scratch_net.fc.in_features, <span class="number">2</span>)</span><br><span class="line">train_fine_tuning(scratch_net, <span class="number">5e-4</span>, param_group=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_fine-tuning_368659_120_1.svg" alt="../_images/output_fine-tuning_368659_120_1.svg"></p>
<p>意料之中，微调模型往往表现更好，因为它的初始参数值更有效。</p>
<hr>
<h2 id="目标检测和边界框"><a href="#目标检测和边界框" class="headerlink" title="目标检测和边界框"></a>目标检测和边界框</h2><p>图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。 然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。 在计算机视觉里，我们将这类任务称为<strong><em>目标检测</em></strong>（object detection）或<strong><em>目标识别</em></strong>（object recognition）。</p>
<p>目标检测在多个领域中被广泛使用。 例如，在无人驾驶里，我们需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍物的位置来规划行进线路。 机器人也常通过该任务来检测感兴趣的目标。安防领域则需要检测异常目标，如歹徒或者炸弹。</p>
<p>接下来的几节将介绍几种用于目标检测的深度学习方法。 我们将首先介绍目标的<strong><em>位置</em></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<p>下面加载本节将使用的示例图像。可以看到图像左边是一只狗，右边是一只猫。 它们是这张图像里的两个主要目标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.set_figsize()</span><br><span class="line">img = d2l.plt.imread(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>)</span><br><span class="line">d2l.plt.imshow(img);</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_bounding-box_d6b70e_21_0.svg" alt="../_images/output_bounding-box_d6b70e_21_0.svg"></p>
<hr>
<p><strong>边界框</strong></p>
<p>在目标检测中，我们通常使用<em>边界框</em>（bounding box）来描述对象的空间位置。 边界框是矩形的，由矩形左上角的以及右下角的$x$和$y$坐标决定。 另一种常用的边界框表示方法是边界框中心的$(x,y)$轴坐标以及框的宽度和高度。</p>
<p>在这里，我们定义在这两种表示法之间进行转换的函数：<code>box_corner_to_center</code>从两角表示法转换为中心宽度表示法，而<code>box_center_to_corner</code>反之亦然。 输入参数<code>boxes</code>可以是长度为4的张量，也可以是形状为（$n$，4）的二维张量，其中$n$是边界框的数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_corner_to_center</span>(<span class="params">boxes</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从（左上，右下）转换到（中间，宽度，高度）&quot;&quot;&quot;</span></span><br><span class="line">    x1, y1, x2, y2 = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    cx = (x1 + x2) / <span class="number">2</span></span><br><span class="line">    cy = (y1 + y2) / <span class="number">2</span></span><br><span class="line">    w = x2 - x1</span><br><span class="line">    h = y2 - y1</span><br><span class="line">    boxes = torch.stack((cx, cy, w, h), axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_center_to_corner</span>(<span class="params">boxes</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从（中间，宽度，高度）转换到（左上，右下）&quot;&quot;&quot;</span></span><br><span class="line">    cx, cy, w, h = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    x1 = cx - <span class="number">0.5</span> * w</span><br><span class="line">    y1 = cy - <span class="number">0.5</span> * h</span><br><span class="line">    x2 = cx + <span class="number">0.5</span> * w</span><br><span class="line">    y2 = cy + <span class="number">0.5</span> * h</span><br><span class="line">    boxes = torch.stack((x1, y1, x2, y2), axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br></pre></td></tr></table></figure>
<p>我们将根据坐标信息定义图像中狗和猫的边界框。 图像中坐标的原点是图像的左上角，向右的方向为$x$轴的正方向，向下的方向为$y$轴的正方向。</p>
<p>我们可以将边界框在图中画出，以检查其是否准确。 画之前，我们定义一个辅助函数<code>bbox_to_rect</code>。 它将边界框表示成<code>matplotlib</code>的边界格式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def bbox_to_rect(bbox, color):</span><br><span class="line">    # 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：</span><br><span class="line">    # ((左上x,左上y),宽,高)</span><br><span class="line">    return d2l.plt.Rectangle(</span><br><span class="line">        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],</span><br><span class="line">        fill=False, edgecolor=color, linewidth=2)</span><br></pre></td></tr></table></figure>
<p>在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(dog_bbox, <span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(cat_bbox, <span class="string">&#x27;red&#x27;</span>));</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_bounding-box_d6b70e_70_0.svg" alt="../_images/output_bounding-box_d6b70e_70_0.svg"></p>
<hr>
<h2 id="锚框"><a href="#锚框" class="headerlink" title="锚框"></a>锚框</h2><p>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的<strong><em>真实边界框</em></strong>。 不同的模型使用的区域采样方法可能不同。 这里我们介绍其中的一种方法：以每个像素为中心，生成多个缩放比和宽高比不同的边界框。 这些边界框被称为<strong><em>锚框</em></strong>（anchor box）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">torch.set_printoptions(<span class="number">2</span>)  <span class="comment"># 精简输出精度</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="生成多个锚框"><a href="#生成多个锚框" class="headerlink" title="生成多个锚框"></a>生成多个锚框</h3><p>假设输入图像的高度为$h$，宽度为$w$。我们以图像的每个像素为中心生成不同形状的锚框：<strong>缩放比</strong>为$s\in (0, 1]$，<em>宽高比</em>为$r &gt; 0$。那么锚框的宽度和高度分别是$hs\sqrt{r}$和$hs/\sqrt{r}$请注意，当中心位置给定时，已知宽和高的锚框是确定的。</p>
<p>要生成多个不同形状的锚框，让我们设置许多缩放比（scale）取值$s_1,\ldots, s_n$和许多宽高比（aspect ratio）取值$r_1,\ldots, r_m$。</p>
<p>当使用这些比例和长宽比的所有组合以每个像素为中心时，输入图像将总共有$whnm$个锚框。尽管这些锚框可能会覆盖所有真实边界框，但计算复杂性很容易过高。在实践中，我们只考虑包含$s_1$或$r_1$的组合</p>
<script type="math/tex; mode=display">
(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1)</script><p>也就是说，</p>
<p>即第一个缩放比与所有宽高比的组合，这意味着对于第一个缩放比$s_1$，我们会生成与每个宽高比 ($r_1, \ldots, r_m$) 的组合，共计 ($m$) 个锚框，对于除了第一个缩放比之外的每个缩放比 ($s_2, \ldots, s_n$)，我们仅考虑与第一个宽高比 (r_1) 的组合，这会产生额外的 ($n-1$) 个锚框。</p>
<p>以同一像素为中心的锚框的数量是$m+n-1$。对于整个输入图像，将共生成$wh(n+m-1)$个锚框。上述生成锚框的方法在下面的<code>multibox_prior</code>函数中实现。我们指定输入图像、尺寸列表和宽高比列表，然后此函数将返回所有的锚框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multibox_prior</span>(<span class="params">data, sizes, ratios</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成以每个像素为中心具有不同形状的锚框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取输入数据的高度和宽度</span></span><br><span class="line">    in_height, in_width = data.shape[-<span class="number">2</span>:]</span><br><span class="line">    <span class="comment"># 获取设备信息（CPU或GPU）、尺寸和宽高比的数量</span></span><br><span class="line">    device, num_sizes, num_ratios = data.device, <span class="built_in">len</span>(sizes), <span class="built_in">len</span>(ratios)</span><br><span class="line">    <span class="comment"># 计算每个像素位置的锚框数量</span></span><br><span class="line">    boxes_per_pixel = (num_sizes + num_ratios - <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 将尺寸和宽高比转换为张量，并确保它们在相同的设备上</span></span><br><span class="line">    size_tensor = torch.tensor(sizes, device=device)</span><br><span class="line">    ratio_tensor = torch.tensor(ratios, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为了将锚点移动到像素的中心，设置偏移量为0.5</span></span><br><span class="line">    offset_h, offset_w = <span class="number">0.5</span>, <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># 计算在高度和宽度方向上的步长</span></span><br><span class="line">    steps_h = <span class="number">1.0</span> / in_height  </span><br><span class="line">    steps_w = <span class="number">1.0</span> / in_width  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成所有中心点的坐标</span></span><br><span class="line">    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h</span><br><span class="line">    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w</span><br><span class="line">    <span class="comment"># 使用meshgrid生成中心点的网格</span></span><br><span class="line">    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing=<span class="string">&#x27;ij&#x27;</span>)</span><br><span class="line">    <span class="comment"># 重塑为一维数组，为每个中心点准备</span></span><br><span class="line">    shift_y, shift_x = shift_y.reshape(-<span class="number">1</span>), shift_x.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成锚框的宽度和高度</span></span><br><span class="line">    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] * torch.sqrt(ratio_tensor[<span class="number">1</span>:]))) * in_height / in_width  </span><br><span class="line">    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] / torch.sqrt(ratio_tensor[<span class="number">1</span>:])))</span><br><span class="line">    <span class="comment"># 除以2来获取半宽和半高，这些用于确定锚框的四个角</span></span><br><span class="line">    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(</span><br><span class="line">                                        in_height * in_width, <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成包含所有锚框中心的网格，每个中心点重复“boxes_per_pixel”次</span></span><br><span class="line">    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],</span><br><span class="line">                dim=<span class="number">1</span>).repeat_interleave(boxes_per_pixel, dim=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 计算最终的锚框坐标（xmin, ymin, xmax, ymax）并返回</span></span><br><span class="line">    output = out_grid + anchor_manipulations</span><br><span class="line">    <span class="keyword">return</span> output.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>锚框变量<code>Y</code>的形状是（批量大小，锚框的数量，4）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">img = d2l.plt.imread(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>)</span><br><span class="line">h, w = img.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(h, w)</span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, h, w))</span><br><span class="line">Y = multibox_prior(X, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">Y.shape</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">561 728</span></span><br><span class="line"><span class="string">torch.Size([1, 2042040, 4])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>将锚框变量<code>Y</code>的形状更改为(图像高度,图像宽度,以同一像素为中心的锚框的数量,4)后，我们可以获得以指定像素的位置为中心的所有锚框。 在接下来的内容中，我们访问以（250,250）为中心的第一个锚框。 它有四个元素：锚框左上角的$(x,y)$轴坐标和右下角的$(x,y)$轴坐标。 输出中两个轴的坐标各分别除以了图像的宽度和高度。</p>
<p>为了显示以图像中以某个像素为中心的所有锚框，定义下面的<code>show_bboxes</code>函数来在图像上绘制多个边界框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_bboxes</span>(<span class="params">axes, bboxes, labels=<span class="literal">None</span>, colors=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示所有边界框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_list</span>(<span class="params">obj, default_values=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 该辅助函数确保labels和colors参数被处理成列表形式。</span></span><br><span class="line">        <span class="comment"># 如果传入的是None，则使用默认值；如果是单个对象而非列表或元组，则将其转换为列表。</span></span><br><span class="line">        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            obj = default_values</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(obj, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            obj = [obj]</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line"></span><br><span class="line">    labels = _make_list(labels)  <span class="comment"># 处理labels参数，确保其为列表形式。</span></span><br><span class="line">    colors = _make_list(colors, [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])  <span class="comment"># 处理colors参数，设置默认颜色为[&#x27;b&#x27;, &#x27;g&#x27;, &#x27;r&#x27;, &#x27;m&#x27;, &#x27;c&#x27;]。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有边界框，每个边界框根据其在列表中的位置选择颜色。</span></span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> <span class="built_in">enumerate</span>(bboxes):</span><br><span class="line">        color = colors[i % <span class="built_in">len</span>(colors)]  <span class="comment"># 循环使用colors列表中的颜色。</span></span><br><span class="line">        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)  </span><br><span class="line">        <span class="comment"># 将边界框坐标转换为matplotlib的Rectangle对象。</span></span><br><span class="line">        axes.add_patch(rect)  <span class="comment"># 在图像上添加边界框矩形。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果提供了标签，为每个边界框添加标签文本。</span></span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">and</span> <span class="built_in">len</span>(labels) &gt; i:</span><br><span class="line">            text_color = <span class="string">&#x27;k&#x27;</span> <span class="keyword">if</span> color == <span class="string">&#x27;w&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;w&#x27;</span>  <span class="comment"># 根据边界框颜色决定文本颜色，以保证文本可见性。</span></span><br><span class="line">            axes.text(rect.xy[<span class="number">0</span>], rect.xy[<span class="number">1</span>], labels[i],</span><br><span class="line">                      va=<span class="string">&#x27;center&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>, fontsize=<span class="number">9</span>, color=text_color,</span><br><span class="line">                      bbox=<span class="built_in">dict</span>(facecolor=color, lw=<span class="number">0</span>))  <span class="comment"># 添加文本标签，位置位于边界框左上角。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>正如从上面代码中所看到的，变量<code>boxes</code>中$x$轴和$y$轴的坐标值已分别除以图像的宽度和高度。 绘制锚框时，我们需要恢复它们原始的坐标值。 因此，在下面定义了变量<code>bbox_scale</code>。 现在可以绘制出图像中所有以(250,250)为中心的锚框了。 如下所示，缩放比为0.75且宽高比为1的蓝色锚框很好地围绕着图像中的狗。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d2l.set_figsize()</span><br><span class="line">bbox_scale = torch.tensor((w, h, w, h))</span><br><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, boxes[<span class="number">250</span>, <span class="number">250</span>, :, :] * bbox_scale,</span><br><span class="line">            [<span class="string">&#x27;s=0.75, r=1&#x27;</span>, <span class="string">&#x27;s=0.5, r=1&#x27;</span>, <span class="string">&#x27;s=0.25, r=1&#x27;</span>, <span class="string">&#x27;s=0.75, r=2&#x27;</span>,</span><br><span class="line">             <span class="string">&#x27;s=0.75, r=0.5&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_anchor_f592d1_66_0.svg" alt="../_images/output_anchor_f592d1_66_0.svg"></p>
<hr>
<h3 id="交并比"><a href="#交并比" class="headerlink" title="交并比"></a>交并比</h3><p>我们刚刚提到某个锚框“较好地”覆盖了图像中的狗。如果已知目标的真实边界框，那么这里的“好”该如何如何量化呢？直观地说，可以衡量锚框和真实边界框之间的相似性。<strong>杰卡德系数</strong>可以衡量两组之间的相似性。给定集合$\mathcal{A}$和$\mathcal{B}$，他们的杰卡德系数是他们交集的大小除以他们并集的大小：</p>
<script type="math/tex; mode=display">
J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}</script><p>事实上，我们可以将任何边界框的像素区域视为一组像素。通过这种方式，我们可以通过其像素集的杰卡德系数来测量两个边界框的相似性。</p>
<p>对于两个边界框，它们的杰卡德系数通常称为<strong>交并比</strong>（IoU），即两个边界框相交面积与相并面积之比，如图所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框完全重合。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraiou.svg" alt="../_images/iou.svg"></p>
<p>接下来部分将使用交并比来衡量锚框和真实边界框之间、以及不同锚框之间的相似度。给定两个锚框或边界框的列表，以下<code>box_iou</code>函数将在这两个列表中计算它们成对的交并比。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_iou</span>(<span class="params">boxes1, boxes2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算两个锚框或边界框列表中成对的交并比&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 定义一个lambda函数来计算一个边界框列表的面积。</span></span><br><span class="line">    <span class="comment"># 面积计算公式为(宽度) x (高度) = (x2 - x1) * (y2 - y1)</span></span><br><span class="line">    box_area = <span class="keyword">lambda</span> boxes: ((boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>]) *</span><br><span class="line">                              (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算两组边界框各自的面积。</span></span><br><span class="line">    areas1 = box_area(boxes1)  <span class="comment"># boxes1的每个边界框的面积。</span></span><br><span class="line">    areas2 = box_area(boxes2)  <span class="comment"># boxes2的每个边界框的面积。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算所有边界框对之间的交集区域的左上角和右下角坐标。</span></span><br><span class="line">    <span class="comment"># 使用广播机制比较boxes1和boxes2中每对边界框。</span></span><br><span class="line">    inter_upperlefts = torch.<span class="built_in">max</span>(boxes1[:, <span class="literal">None</span>, :<span class="number">2</span>], boxes2[:, :<span class="number">2</span>])  <span class="comment"># 交集区域的左上角。</span></span><br><span class="line">    inter_lowerrights = torch.<span class="built_in">min</span>(boxes1[:, <span class="literal">None</span>, <span class="number">2</span>:], boxes2[:, <span class="number">2</span>:])  <span class="comment"># 交集区域的右下角。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算交集区域的尺寸。如果没有交集，尺寸会是负的，使用clamp将其设置为0。</span></span><br><span class="line">    inters = (inter_lowerrights - inter_upperlefts).clamp(<span class="built_in">min</span>=<span class="number">0</span>)  <span class="comment"># 交集区域的宽度和高度。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算交集区域的面积。</span></span><br><span class="line">    inter_areas = inters[:, :, <span class="number">0</span>] * inters[:, :, <span class="number">1</span>]  <span class="comment"># 每对边界框交集区域的面积。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算并集区域的面积。并集面积 = 边界框1的面积 + 边界框2的面积 - 交集面积。</span></span><br><span class="line">    union_areas = areas1[:, <span class="literal">None</span>] + areas2 - inter_areas  <span class="comment"># 每对边界框并集区域的面积。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算交并比。交并比 = 交集面积 / 并集面积。</span></span><br><span class="line">    <span class="keyword">return</span> inter_areas / union_areas</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="标注锚框"><a href="#标注锚框" class="headerlink" title="标注锚框"></a>标注锚框</h3><p><strong>将真实边界框分配给锚框</strong></p>
<p>在训练集中，我们将每个锚框视为一个训练样本。 为了训练目标检测模型，我们需要每个锚框的<strong><em>类别</em></strong>（class）和<strong><em>偏移量</em></strong>（offset）标签，其中前者是与锚框相关的对象的类别，后者是真实边界框相对于锚框的偏移量。 在预测时，我们为每个图像生成多个锚框，预测所有锚框的类别和偏移量，根据预测的偏移量调整它们的位置以获得预测的边界框，最后只输出符合特定条件的预测边界框。</p>
<p>目标检测训练集带有<strong><em>真实边界框</em></strong> 的位置及其包围物体类别的标签。 要标记任何生成的锚框，我们可以参考分配到的最接近此锚框的真实边界框的位置和类别标签。 下文将介绍一个算法，它能够把最接近的真实边界框分配给锚框。</p>
<p>给定图像，假设锚框是$A_1, A_2, \ldots, A_{n_a}$，真实边界框是$B_1, B_2, \ldots, B_{n_b}$，其中$n_a \geq n_b$。让我们定义一个矩阵$\mathbf{X} \in \mathbb{R}^{n_a \times n_b}$，其中第$i$行、第$j$列的元素$x_{ij}$是锚框$A_i$和真实边界框$B_j$的IoU</p>
<p>该算法包含以下步骤。</p>
<ol>
<li>在矩阵$\mathbf{X}$中找到最大的元素，并将它的行索引和列索引分别表示为$i_1$和$j_1$。然后将真实边界框$B_{j_1}$分配给锚框$A_{i_1}$。这很直观，因为$A_{i_1}$和$B_{j_1}$是所有锚框和真实边界框配对中最相近的。在第一个分配完成后，丢弃矩阵中${i_1}^\mathrm{th}$行和${j_1}^\mathrm{th}$列中的所有元素。</li>
<li>在矩阵$\mathbf{X}$中找到剩余元素中最大的元素，并将它的行索引和列索引分别表示为$i_2$和$j_2$。我们将真实边界框$B_{j_2}$分配给锚框$A_{i_2}$，并丢弃矩阵中${i_2}^\mathrm{th}$行和${j_2}^\mathrm{th}$列中的所有元素。</li>
<li>此时，矩阵$\mathbf{X}$中两行和两列中的元素已被丢弃。我们继续，直到丢弃掉矩阵$\mathbf{X}$中$n_b$列中的所有元素。此时已经为这$n_b$个锚框各自分配了一个真实边界框。</li>
<li>只遍历剩下的$n_a - n_b$个锚框。例如，给定任何锚框$A_i$，在矩阵$\mathbf{X}$的第$i^\mathrm{th}$行中找到与$A_i$的IoU最大的真实边界框$B_j$，只有当此IoU大于预定义的阈值时，才将$B_j$分配给$A_i$。</li>
</ol>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraanchor-label.svg" alt="../_images/anchor-label.svg"></p>
<p>下面用一个具体的例子来说明上述算法。</p>
<p>如图左所示，假设矩阵$\mathbf{X}$中的最大值为$x_{23}$，我们将真实边界框$B_3$分配给锚框$A_2$。</p>
<p>然后，我们丢弃矩阵第2行和第3列中的所有元素，在剩余元素（阴影区域）中找到最大的$x_{71}$，然后将真实边界框$B_1$分配给锚框$A_7$。</p>
<p>接下来，如图中所示，丢弃矩阵第7行和第1列中的所有元素，在剩余元素（阴影区域）中找到最大的$x_{54}$，然后将真实边界框$B_4$分配给锚框$A_5$。</p>
<p>最后，如图右所示，丢弃矩阵第5行和第4列中的所有元素，在剩余元素（阴影区域）中找到最大的$x_{92}$，然后将真实边界框$B_2$分配给锚框$A_9$。</p>
<p>之后，我们只需要遍历剩余的锚框$A_1, A_3, A_4, A_6, A_8$，然后根据阈值确定是否为它们分配真实边界框。</p>
<p>此算法在下面的<code>assign_anchor_to_bbox</code>函数中实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_anchor_to_bbox</span>(<span class="params">ground_truth, anchors, device, iou_threshold=<span class="number">0.5</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将最接近的真实边界框分配给锚框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算锚框和真实边界框的数量</span></span><br><span class="line">    num_anchors, num_gt_boxes = anchors.shape[<span class="number">0</span>], ground_truth.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算所有锚框和真实边界框之间的IoU。jaccard变量存储了这些IoU值。</span></span><br><span class="line">    jaccard = box_iou(anchors, ground_truth)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化一个用于存储每个锚框分配到的真实边界框索引的张量。初始值设为-1，表示没有分配。</span></span><br><span class="line">    anchors_bbox_map = torch.full((num_anchors,), -<span class="number">1</span>, dtype=torch.long,</span><br><span class="line">                                  device=device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 首先，为每个锚框找到IoU最大的真实边界框，如果最大IoU大于阈值，则进行分配。</span></span><br><span class="line">    max_ious, indices = torch.<span class="built_in">max</span>(jaccard, dim=<span class="number">1</span>)</span><br><span class="line">    anc_i = torch.nonzero(max_ious &gt;= iou_threshold).reshape(-<span class="number">1</span>)  <span class="comment"># 找到IoU大于阈值的锚框索引</span></span><br><span class="line">    box_j = indices[max_ious &gt;= iou_threshold]  <span class="comment"># 对应的真实边界框索引</span></span><br><span class="line">    anchors_bbox_map[anc_i] = box_j  <span class="comment"># 分配</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 为了避免在后面的循环中修改jaccard矩阵，预先定义两个丢弃值向量</span></span><br><span class="line">    col_discard = torch.full((num_anchors,), -<span class="number">1</span>)</span><br><span class="line">    row_discard = torch.full((num_gt_boxes,), -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 然后，对于每个真实边界框，找到与其IoU最大的锚框，并进行强制分配，</span></span><br><span class="line">    <span class="comment"># 即使这个IoU值小于阈值。这确保了每个真实边界框至少被一个锚框覆盖。</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_gt_boxes):</span><br><span class="line">        max_idx = torch.argmax(jaccard)  <span class="comment"># 找到IoU矩阵中的最大值索引</span></span><br><span class="line">        box_idx = (max_idx % num_gt_boxes).long()  <span class="comment"># 真实边界框的索引</span></span><br><span class="line">        anc_idx = (max_idx / num_gt_boxes).long()  <span class="comment"># 锚框的索引</span></span><br><span class="line">        anchors_bbox_map[anc_idx] = box_idx  <span class="comment"># 强制分配</span></span><br><span class="line">        jaccard[:, box_idx] = col_discard  <span class="comment"># 丢弃已分配的真实边界框</span></span><br><span class="line">        jaccard[anc_idx, :] = row_discard  <span class="comment"># 丢弃已分配的锚框</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回每个锚框分配到的真实边界框索引</span></span><br><span class="line">    <span class="keyword">return</span> anchors_bbox_map</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>标记类别和偏移量</strong></p>
<p>现在我们可以为每个锚框标记类别和偏移量了。假设一个锚框$A$被分配了一个真实边界框$B$。一方面，锚框$A$的类别将被标记为与$B$相同。另一方面，锚框$A$的偏移量将根据$B$和$A$中心坐标的相对位置以及这两个框的相对大小进行标记。鉴于数据集内不同的框的位置和大小不同，我们可以对那些相对位置和大小应用变换，使其获得分布更均匀且易于拟合的偏移量。这里介绍一种常见的变换。给定框$A$和$B$，中心坐标分别为$(x_a, y_a)$和$(x_b, y_b)$，宽度分别为$w_a$和$w_b$，高度分别为$h_a$和$h_b$，可以将$A$的偏移量标记为：</p>
<script type="math/tex; mode=display">
\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},

\frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},

\frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},

\frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right)</script><p>其中常量的默认值为 $\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1$ ， $\sigma_w=\sigma_h=0.2$。</p>
<p>这种转换在下面的 <code>offset_boxes</code> 函数中实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">offset_boxes</span>(<span class="params">anchors, assigned_bb, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;对锚框偏移量的转换&quot;&quot;&quot;</span></span><br><span class="line">    c_anc = d2l.box_corner_to_center(anchors)</span><br><span class="line">    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)</span><br><span class="line">    offset_xy = <span class="number">10</span> * (c_assigned_bb[:, :<span class="number">2</span>] - c_anc[:, :<span class="number">2</span>]) / c_anc[:, <span class="number">2</span>:]</span><br><span class="line">    offset_wh = <span class="number">5</span> * torch.log(eps + c_assigned_bb[:, <span class="number">2</span>:] / c_anc[:, <span class="number">2</span>:])</span><br><span class="line">    offset = torch.cat([offset_xy, offset_wh], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> offset</span><br></pre></td></tr></table></figure>
<p>如果一个锚框没有被分配真实边界框，我们只需将锚框的类别标记为<strong><em>背景</em></strong>（background）。 背景类别的锚框通常被称为<strong><em>负类</em></strong> 锚框，其余的被称为<strong><em>正类</em></strong> 锚框。 我们使用真实边界框（<code>labels</code>参数）实现以下<code>multibox_target</code>函数，来标记锚框的类别和偏移量（<code>anchors</code>参数）。 此函数将背景类别的索引设置为零，然后将新类别的整数索引递增一。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multibox_target</span>(<span class="params">anchors, labels</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用真实边界框标记锚框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取批量大小和调整锚框的形状</span></span><br><span class="line">    batch_size, anchors = labels.shape[<span class="number">0</span>], anchors.squeeze(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 初始化批量偏移量、掩码和类别标签列表</span></span><br><span class="line">    batch_offset, batch_mask, batch_class_labels = [], [], []</span><br><span class="line">    <span class="comment"># 获取设备信息和锚框数量</span></span><br><span class="line">    device, num_anchors = anchors.device, anchors.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对批量中的每个样本进行循环处理</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        label = labels[i, :, :]  <span class="comment"># 获取当前样本的真实边界框和类别标签</span></span><br><span class="line">        <span class="comment"># 将最接近的真实边界框分配给锚框</span></span><br><span class="line">        anchors_bbox_map = assign_anchor_to_bbox(</span><br><span class="line">            label[:, <span class="number">1</span>:], anchors, device)</span><br><span class="line">        <span class="comment"># 根据分配结果生成边界框掩码，未分配的锚框掩码为0，分配了的为1</span></span><br><span class="line">        bbox_mask = ((anchors_bbox_map &gt;= <span class="number">0</span>).<span class="built_in">float</span>().unsqueeze(-<span class="number">1</span>)).repeat(</span><br><span class="line">            <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化类别标签和分配给锚框的真实边界框坐标</span></span><br><span class="line">        class_labels = torch.zeros(num_anchors, dtype=torch.long,</span><br><span class="line">                                   device=device)</span><br><span class="line">        assigned_bb = torch.zeros((num_anchors, <span class="number">4</span>), dtype=torch.float32,</span><br><span class="line">                                  device=device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 根据分配的真实边界框更新锚框的类别标签和坐标</span></span><br><span class="line">        indices_true = torch.nonzero(anchors_bbox_map &gt;= <span class="number">0</span>)</span><br><span class="line">        bb_idx = anchors_bbox_map[indices_true]</span><br><span class="line">        class_labels[indices_true] = label[bb_idx, <span class="number">0</span>].long() + <span class="number">1</span>  <span class="comment"># 类别标签从1开始编号，0表示背景</span></span><br><span class="line">        assigned_bb[indices_true] = label[bb_idx, <span class="number">1</span>:]  <span class="comment"># 更新锚框的真实边界框坐标</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算锚框相对于其分配的真实边界框的偏移量，并应用掩码</span></span><br><span class="line">        offset = offset_boxes(anchors, assigned_bb) * bbox_mask</span><br><span class="line">        <span class="comment"># 将结果添加到批量列表中</span></span><br><span class="line">        batch_offset.append(offset.reshape(-<span class="number">1</span>))</span><br><span class="line">        batch_mask.append(bbox_mask.reshape(-<span class="number">1</span>))</span><br><span class="line">        batch_class_labels.append(class_labels)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将批量数据堆叠成张量</span></span><br><span class="line">    bbox_offset = torch.stack(batch_offset)</span><br><span class="line">    bbox_mask = torch.stack(batch_mask)</span><br><span class="line">    class_labels = torch.stack(batch_class_labels)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回处理后的批量偏移量、掩码和类别标签</span></span><br><span class="line">    <span class="keyword">return</span> (bbox_offset, bbox_mask, class_labels)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面通过一个具体的例子来说明锚框标签。</p>
<p>我们已经为加载图像中的狗和猫定义了真实边界框，其中第一个元素是类别（0代表狗，1代表猫），其余四个元素是左上角和右下角的$(x, y)$轴坐标（范围介于0和1之间）。我们还构建了五个锚框，用左上角和右下角的坐标进行标记：$A_0, \ldots, A_4$（索引从0开始）。然后我们在图像中绘制这些真实边界框和锚框</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ground_truth = torch.tensor([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>],</span><br><span class="line">                         [<span class="number">1</span>, <span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">anchors = torch.tensor([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.15</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.4</span>],</span><br><span class="line">                    [<span class="number">0.63</span>, <span class="number">0.05</span>, <span class="number">0.88</span>, <span class="number">0.98</span>], [<span class="number">0.66</span>, <span class="number">0.45</span>, <span class="number">0.8</span>, <span class="number">0.8</span>],</span><br><span class="line">                    [<span class="number">0.57</span>, <span class="number">0.3</span>, <span class="number">0.92</span>, <span class="number">0.9</span>]])</span><br><span class="line"></span><br><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, ground_truth[:, <span class="number">1</span>:] * bbox_scale, [<span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>], <span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">show_bboxes(fig.axes, anchors * bbox_scale, [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_anchor_f592d1_126_0.svg" alt="../_images/output_anchor_f592d1_126_0.svg"></p>
<p>使用上面定义的<code>multibox_target</code>函数，我们可以根据狗和猫的真实边界框，标注这些锚框的分类和偏移量。 在这个例子中，背景、狗和猫的类索引分别为0、1和2。 下面我们为锚框和真实边界框样本添加一个维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">labels = multibox_target(anchors.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                         ground_truth.unsqueeze(dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>让我们根据图像中的锚框和真实边界框的位置来分析下面返回的类别标签。首先，在所有的锚框和真实边界框配对中，锚框$A_4$与猫的真实边界框的IoU是最大的。因此，$A_4$的类别被标记为猫。</p>
<p>去除包含$A_4$或猫的真实边界框的配对，在剩下的配对中，锚框$A_1$和狗的真实边界框有最大的IoU。因此，$A_1$的类别被标记为狗。</p>
<p>接下来，我们需要遍历剩下的三个未标记的锚框：$A_0$、$A_2$和$A_3$。</p>
<p>对于$A_0$，与其拥有最大IoU的真实边界框的类别是狗，但IoU低于预定义的阈值（0.5），因此该类别被标记为背景；</p>
<p>对于$A_2$，与其拥有最大IoU的真实边界框的类别是猫，IoU超过阈值，所以类别被标记为猫；</p>
<p>对于$A_3$，与其拥有最大IoU的真实边界框的类别是猫，但值低于阈值，因此该类别被标记为背景</p>
<p>返回的结果中有三个元素，都是张量格式。第三个元素包含标记的输入锚框的类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">labels[<span class="number">2</span>]</span><br><span class="line"><span class="comment">#tensor([[0, 1, 2, 0, 2]])</span></span><br></pre></td></tr></table></figure>
<p>返回的第二个元素是掩码（mask）变量，形状为（批量大小，锚框数的四倍）。 掩码变量中的元素与每个锚框的4个偏移量一一对应。 由于我们不关心对背景的检测，负类的偏移量不应影响目标函数。 通过元素乘法，掩码变量中的零将在计算目标函数之前过滤掉负类偏移量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">labels[<span class="number">1</span>]</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,</span></span><br><span class="line"><span class="string">        1., 1.]])</span></span><br></pre></td></tr></table></figure>
<p>返回的第一个元素包含了为每个锚框标记的四个偏移值。 请注意，负类锚框的偏移量被标记为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">labels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[-0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00,  1.40e+00,  1.00e+01,</span></span><br><span class="line"><span class="string">          2.59e+00,  7.18e+00, -1.20e+00,  2.69e-01,  1.68e+00, -1.57e+00,</span></span><br><span class="line"><span class="string">         -0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, -5.71e-01, -1.00e+00,</span></span><br><span class="line"><span class="string">          4.17e-06,  6.26e-01]])</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="非极大值抑制预测边界框"><a href="#非极大值抑制预测边界框" class="headerlink" title="非极大值抑制预测边界框"></a>非极大值抑制预测边界框</h3><p>在预测时，我们先为图像生成多个锚框，再为这些锚框一一预测类别和偏移量。 一个<strong><em>预测好的边界框</em></strong> 则根据其中某个带有预测偏移量的锚框而生成。 下面我们实现了<code>offset_inverse</code>函数，该函数将锚框和偏移量预测作为输入，并应用逆偏移变换来返回预测的边界框坐标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">offset_inverse</span>(<span class="params">anchors, offset_preds</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据带有预测偏移量的锚框来预测边界框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将锚框从角落表示（xmin, ymin, xmax, ymax）转换为中心表示（x_center, y_center, width, height）</span></span><br><span class="line">    anc = d2l.box_corner_to_center(anchors)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算预测的边界框的中心位置。预测偏移量的前两个元素（offset_preds[:, :2]）</span></span><br><span class="line">    <span class="comment"># 代表中心点的偏移，通过与锚框宽高的比例（除以10作为缩放）相乘后加到锚框的中心位置上。</span></span><br><span class="line">    pred_bbox_xy = (offset_preds[:, :<span class="number">2</span>] * anc[:, <span class="number">2</span>:] / <span class="number">10</span>) + anc[:, :<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算预测的边界框的宽度和高度。预测偏移量的后两个元素（offset_preds[:, 2:]）</span></span><br><span class="line">    <span class="comment"># 代表宽度和高度的比例变化，通过对这些值除以5后取指数，然后乘以锚框的宽度和高度。</span></span><br><span class="line">    pred_bbox_wh = torch.exp(offset_preds[:, <span class="number">2</span>:] / <span class="number">5</span>) * anc[:, <span class="number">2</span>:]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将预测的中心位置和宽高合并，形成中心表示的预测边界框。</span></span><br><span class="line">    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将预测的边界框从中心表示转换回角落表示（xmin, ymin, xmax, ymax）。</span></span><br><span class="line">    predicted_bbox = d2l.box_center_to_corner(pred_bbox)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回预测的边界框坐标。</span></span><br><span class="line">    <span class="keyword">return</span> predicted_bbox</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>当有许多锚框时，可能会输出许多相似的具有明显重叠的预测边界框，都围绕着同一目标。为了简化输出，我们可以使用<strong><em>非极大值抑制</em></strong>（non-maximum suppression，NMS）合并属于同一目标的类似的预测边界框。</p>
<p>以下是非极大值抑制的工作原理：<br>对于一个预测边界框$B$，目标检测模型会计算每个类别的预测概率。假设最大的预测概率为$p$，则该概率所对应的类别$B$即为预测的类别。具体来说，我们将$p$称为预测边界框$B$的<strong><em>置信度</em></strong>（confidence）。在同一张图像中，所有预测的非背景边界框都按置信度降序排序，以生成列表$L$。然后我们通过以下步骤操作排序列表$L$。</p>
<ol>
<li>从$L$中选取置信度最高的预测边界框$B_1$作为基准，然后将所有与$B_1$的IoU超过预定阈值$\epsilon$的非基准预测边界框从$L$中移除。这时，$L$保留了置信度最高的预测边界框，去除了与其太过相似的其他预测边界框。简而言之，那些具有<strong><em>非极大值</em></strong>置信度的边界框被<em>抑制</em>了。</li>
<li>从$L$中选取置信度第二高的预测边界框$B_2$作为又一个基准，然后将所有与$B_2$的IoU大于$\epsilon$的非基准预测边界框从$L$中移除。</li>
<li>重复上述过程，直到$L$中的所有预测边界框都曾被用作基准。此时，$L$中任意一对预测边界框的IoU都小于阈值$\epsilon$；因此，没有一对边界框过于相似。</li>
<li>输出列表$L$中的所有预测边界框。</li>
</ol>
<p>以下<code>nms</code>函数按降序对置信度进行排序并返回其索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">boxes, scores, iou_threshold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;对预测边界框的置信度进行排序，并通过非极大值抑制(NMS)筛选边界框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 根据边界框的置信度分数进行降序排序，B 存储了排序后的索引</span></span><br><span class="line">    B = torch.argsort(scores, dim=-<span class="number">1</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    keep = []  <span class="comment"># 用于存储通过NMS筛选后保留下来的边界框的索引</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当还有边界框未处理时，循环继续</span></span><br><span class="line">    <span class="keyword">while</span> B.numel() &gt; <span class="number">0</span>:</span><br><span class="line">        i = B[<span class="number">0</span>]  <span class="comment"># 取得当前最高置信度分数的边界框索引</span></span><br><span class="line">        keep.append(i)  <span class="comment"># 将其添加到保留列表中</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> B.numel() == <span class="number">1</span>: <span class="keyword">break</span>  <span class="comment"># 如果只剩下一个边界框，则直接结束循环</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算选中的边界框与其他所有边界框的IoU值</span></span><br><span class="line">        iou = box_iou(boxes[i, :].reshape(-<span class="number">1</span>, <span class="number">4</span>),</span><br><span class="line">                      boxes[B[<span class="number">1</span>:], :].reshape(-<span class="number">1</span>, <span class="number">4</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 找出IoU值小于阈值的边界框，这些边界框不被认为是与当前选中边界框重叠的</span></span><br><span class="line">        inds = torch.nonzero(iou &lt;= iou_threshold).reshape(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新B，只保留那些未被当前选中的边界框抑制掉的边界框索引</span></span><br><span class="line">        B = B[inds + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回通过NMS筛选后的边界框索引</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor(keep, device=boxes.device)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们定义以下<code>multibox_detection</code>函数来将非极大值抑制应用于预测边界框</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multibox_detection</span>(<span class="params">cls_probs, offset_preds, anchors, nms_threshold=<span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       pos_threshold=<span class="number">0.009999999</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用非极大值抑制来预测边界框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取设备信息和批量大小</span></span><br><span class="line">    device, batch_size = cls_probs.device, cls_probs.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 确保锚框的形状是正确的</span></span><br><span class="line">    anchors = anchors.squeeze(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 获取类别数和锚框数</span></span><br><span class="line">    num_classes, num_anchors = cls_probs.shape[<span class="number">1</span>], cls_probs.shape[<span class="number">2</span>]</span><br><span class="line">    out = []  <span class="comment"># 用于存储最终输出的列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):  <span class="comment"># 遍历每个样本</span></span><br><span class="line">        <span class="comment"># 提取当前样本的类别概率和偏移量预测</span></span><br><span class="line">        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 计算每个锚框的最大置信度和对应的类别ID</span></span><br><span class="line">        conf, class_id = torch.<span class="built_in">max</span>(cls_prob[<span class="number">1</span>:], <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 使用偏移量预测和锚框计算预测的边界框</span></span><br><span class="line">        predicted_bb = offset_inverse(anchors, offset_pred)</span><br><span class="line">        <span class="comment"># 应用NMS处理预测的边界框，去除重叠边界框</span></span><br><span class="line">        keep = nms(predicted_bb, conf, nms_threshold)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 处理NMS之后未保留的边界框，将它们的类别设置为背景</span></span><br><span class="line">        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)</span><br><span class="line">        combined = torch.cat((keep, all_idx))</span><br><span class="line">        uniques, counts = combined.unique(return_counts=<span class="literal">True</span>)</span><br><span class="line">        non_keep = uniques[counts == <span class="number">1</span>]</span><br><span class="line">        all_id_sorted = torch.cat((keep, non_keep))</span><br><span class="line">        class_id[non_keep] = -<span class="number">1</span>  <span class="comment"># 未保留的设置为背景类别</span></span><br><span class="line">        <span class="comment"># 对类别ID、置信度和边界框坐标进行排序和筛选</span></span><br><span class="line">        class_id = class_id[all_id_sorted]</span><br><span class="line">        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据置信度阈值进一步筛选边界框</span></span><br><span class="line">        below_min_idx = (conf &lt; pos_threshold)</span><br><span class="line">        class_id[below_min_idx] = -<span class="number">1</span>  <span class="comment"># 低于阈值的设置为背景类别</span></span><br><span class="line">        conf[below_min_idx] = <span class="number">1</span> - conf[below_min_idx]  <span class="comment"># 调整置信度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并类别ID、置信度和边界框坐标，形成最终输出</span></span><br><span class="line">        pred_info = torch.cat((class_id.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                               conf.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                               predicted_bb), dim=<span class="number">1</span>)</span><br><span class="line">        out.append(pred_info)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有样本的输出堆叠起来，返回最终结果</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack(out)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在让我们将上述算法应用到一个带有四个锚框的具体示例中。 为简单起见，我们假设预测的偏移量都是零，这意味着预测的边界框即是锚框。 对于背景、狗和猫其中的每个类，我们还定义了它的预测概率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">anchors = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>], [<span class="number">0.08</span>, <span class="number">0.2</span>, <span class="number">0.56</span>, <span class="number">0.95</span>],</span><br><span class="line">                      [<span class="number">0.15</span>, <span class="number">0.3</span>, <span class="number">0.62</span>, <span class="number">0.91</span>], [<span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">offset_preds = torch.tensor([<span class="number">0</span>] * anchors.numel())</span><br><span class="line">cls_probs = torch.tensor([[<span class="number">0</span>] * <span class="number">4</span>,  <span class="comment"># 背景的预测概率</span></span><br><span class="line">                      [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],  <span class="comment"># 狗的预测概率</span></span><br><span class="line">                      [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]])  <span class="comment"># 猫的预测概率</span></span><br></pre></td></tr></table></figure>
<p>我们可以在图像上绘制这些预测边界框和置信度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, anchors * bbox_scale,</span><br><span class="line">            [<span class="string">&#x27;dog=0.9&#x27;</span>, <span class="string">&#x27;dog=0.8&#x27;</span>, <span class="string">&#x27;dog=0.7&#x27;</span>, <span class="string">&#x27;cat=0.9&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_anchor_f592d1_234_0.svg" alt="../_images/output_anchor_f592d1_234_0.svg"></p>
<p>现在我们可以调用<code>multibox_detection</code>函数来执行非极大值抑制，其中阈值设置为0.5。 请注意，我们在示例的张量输入中添加了维度。</p>
<p>我们可以看到返回结果的形状是（批量大小，锚框的数量，6）。 最内层维度中的六个元素提供了同一预测边界框的输出信息。 第一个元素是预测的类索引，从0开始（0代表狗，1代表猫），值-1表示背景或在非极大值抑制中被移除了。 第二个元素是预测的边界框的置信度。 其余四个元素分别是预测边界框左上角和右下角的(�,�)轴坐标（范围介于0和1之间）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">output = multibox_detection(cls_probs.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            offset_preds.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            anchors.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            nms_threshold=<span class="number">0.5</span>)</span><br><span class="line">output</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[ 0.00,  0.90,  0.10,  0.08,  0.52,  0.92],</span></span><br><span class="line"><span class="string">         [ 1.00,  0.90,  0.55,  0.20,  0.90,  0.88],</span></span><br><span class="line"><span class="string">         [-1.00,  0.80,  0.08,  0.20,  0.56,  0.95],</span></span><br><span class="line"><span class="string">         [-1.00,  0.70,  0.15,  0.30,  0.62,  0.91]]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>删除-1类别（背景）的预测边界框后，我们可以输出由非极大值抑制保存的最终预测边界框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="number">0</span>].detach().numpy():</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">0</span>] == -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    label = (<span class="string">&#x27;dog=&#x27;</span>, <span class="string">&#x27;cat=&#x27;</span>)[<span class="built_in">int</span>(i[<span class="number">0</span>])] + <span class="built_in">str</span>(i[<span class="number">1</span>])</span><br><span class="line">    show_bboxes(fig.axes, [torch.tensor(i[<span class="number">2</span>:]) * bbox_scale], label)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_anchor_f592d1_258_0.svg" alt="../_images/output_anchor_f592d1_258_0.svg"></p>
<p>实践中，在执行非极大值抑制前，我们甚至可以将置信度较低的预测边界框移除，从而减少此算法中的计算量。 我们也可以对非极大值抑制的输出结果进行后处理。例如，只保留置信度更高的结果作为最终输出。</p>
<hr>
<h2 id="多尺度目标检测"><a href="#多尺度目标检测" class="headerlink" title="多尺度目标检测"></a>多尺度目标检测</h2><p>我们以输入图像的每个像素为中心，生成了多个锚框。 基本而言，这些锚框代表了图像不同区域的样本。 然而，如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。 想象一个$561×728$的输入图像，如果以每个像素为中心生成五个形状不同的锚框，就需要在图像上标记和预测超过200万个锚框（$561×728×5$）。</p>
<h3 id="多尺度锚框"><a href="#多尺度锚框" class="headerlink" title="多尺度锚框"></a>多尺度锚框</h3><p>减少图像上的锚框数量并不困难。 比如，我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。 此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。 直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。 例如，$1×1$、$1×2$和$2×2$的目标可以分别以4、2和1种可能的方式出现在2×2图像上。 因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。</p>
<p>为了演示如何在多个尺度下生成锚框，让我们先读取一张图像。 它的高度和宽度分别为561和728像素。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">img = d2l.plt.imread(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>)</span><br><span class="line">h, w = img.shape[:<span class="number">2</span>]</span><br><span class="line">h, w</span><br></pre></td></tr></table></figure>
<p>回想一下，在卷积神经网络中，我们将卷积图层的二维数组输出称为<strong>特征图</strong>。 通过定义特征图的形状，我们可以确定任何图像上均匀采样锚框的中心。</p>
<p><code>display_anchors</code>函数定义如下。 我们在特征图（<code>fmap</code>）上生成锚框（<code>anchors</code>），每个单位（像素）作为锚框的中心。 由于锚框中的$(x,y)$轴坐标值（<code>anchors</code>）已经被除以特征图（<code>fmap</code>）的宽度和高度，因此这些值介于0和1之间，表示特征图中锚框的相对位置。</p>
<p>由于锚框（<code>anchors</code>）的中心分布于特征图（<code>fmap</code>）上的所有单位，因此这些中心必须根据其相对空间位置在任何输入图像上<strong><em>均匀</em></strong> 分布。 更具体地说，给定特征图的宽度和高度<code>fmap_w</code>和<code>fmap_h</code>，以下函数将<strong><em>均匀地</em></strong> 对任何输入图像中<code>fmap_h</code>行和<code>fmap_w</code>列中的像素进行采样。 以这些均匀采样的像素为中心，将会生成大小为<code>s</code>（假设列表<code>s</code>的长度为1）且宽高比（<code>ratios</code>）不同的锚框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_anchors</span>(<span class="params">fmap_w, fmap_h, s</span>):</span></span><br><span class="line">    <span class="comment"># 设置图像的显示大小</span></span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成一个模拟的特征图，其深度和真实特征图的深度无关，</span></span><br><span class="line">    <span class="comment"># 只是为了使用multibox_prior函数生成锚框</span></span><br><span class="line">    fmap = torch.zeros((<span class="number">1</span>, <span class="number">10</span>, fmap_h, fmap_w))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用multibox_prior函数生成锚框，sizes参数控制锚框的大小，</span></span><br><span class="line">    <span class="comment"># ratios参数控制锚框的宽高比</span></span><br><span class="line">    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># bbox_scale是一个缩放因子，用于将锚框的坐标从特征图的尺度调整回原始图像的尺度</span></span><br><span class="line">    <span class="comment"># 假设变量w和h分别代表原始图像的宽度和高度</span></span><br><span class="line">    bbox_scale = torch.tensor((w, h, w, h))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 显示锚框。首先，使用plt.imshow(img).axes在图像上绘制锚框，</span></span><br><span class="line">    <span class="comment"># 然后利用show_bboxes函数和bbox_scale将锚框的坐标调整并显示在原始图像上</span></span><br><span class="line">    <span class="comment"># 注意：这里的img应该是一个图像的数组表示，用于背景显示</span></span><br><span class="line">    d2l.show_bboxes(d2l.plt.imshow(img).axes, anchors[<span class="number">0</span>] * bbox_scale)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>首先，让我们考虑探测小目标。 为了在显示时更容易分辨，在这里具有不同中心的锚框不会重叠： 锚框的尺度设置为0.15，特征图的高度和宽度设置为4。 我们可以看到，图像上4行和4列的锚框的中心是均匀分布的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=4, fmap_h=4, s=[0.15])</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_multiscale-object-detection_ad7147_30_0.svg" alt="../_images/output_multiscale-object-detection_ad7147_30_0.svg"></p>
<p>我们进一步将特征图的高度和宽度减小到四分之一，然后将锚框的尺度增加到0.8。 此时，锚框的中心即是图像的中心</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">2</span>, fmap_h=<span class="number">2</span>, s=[<span class="number">0.4</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_multiscale-object-detection_ad7147_54_0.svg" alt="../_images/output_multiscale-object-detection_ad7147_54_0.svg"></p>
<hr>
<h2 id="目标检测数据集"><a href="#目标检测数据集" class="headerlink" title="目标检测数据集"></a>目标检测数据集</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;banana-detection&#x27;</span>] = (</span><br><span class="line">    d2l.DATA_URL + <span class="string">&#x27;banana-detection.zip&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;5de26c8fce5ccdea9f91267273464dc968d20d72&#x27;</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h3><p>通过<code>read_data_bananas</code>函数，我们读取香蕉检测数据集。 该数据集包括一个的CSV文件，内含目标类别标签和位于左上角和右下角的真实边界框坐标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data_bananas</span>(<span class="params">is_train=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取香蕉检测数据集中的图像和标签&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;banana-detection&#x27;</span>)</span><br><span class="line">    csv_fname = os.path.join(data_dir, <span class="string">&#x27;bananas_train&#x27;</span> <span class="keyword">if</span> is_train</span><br><span class="line">                             <span class="keyword">else</span> <span class="string">&#x27;bananas_val&#x27;</span>, <span class="string">&#x27;label.csv&#x27;</span>)</span><br><span class="line">    csv_data = pd.read_csv(csv_fname)</span><br><span class="line">    csv_data = csv_data.set_index(<span class="string">&#x27;img_name&#x27;</span>)</span><br><span class="line">    images, targets = [], []</span><br><span class="line">    <span class="keyword">for</span> img_name, target <span class="keyword">in</span> csv_data.iterrows():</span><br><span class="line">        images.append(torchvision.io.read_image(</span><br><span class="line">            os.path.join(data_dir, <span class="string">&#x27;bananas_train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span></span><br><span class="line">                         <span class="string">&#x27;bananas_val&#x27;</span>, <span class="string">&#x27;images&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;img_name&#125;</span>&#x27;</span>)))</span><br><span class="line">        <span class="comment"># 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），</span></span><br><span class="line">        <span class="comment"># 其中所有图像都具有相同的香蕉类（索引为0）</span></span><br><span class="line">        targets.append(<span class="built_in">list</span>(target))</span><br><span class="line">    <span class="keyword">return</span> images, torch.tensor(targets).unsqueeze(<span class="number">1</span>) / <span class="number">256</span></span><br></pre></td></tr></table></figure>
<p>通过使用<code>read_data_bananas</code>函数读取图像和标签，以下<code>BananasDataset</code>类别将允许我们创建一个自定义<code>Dataset</code>实例来加载香蕉检测数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BananasDataset</span>(<span class="params">torch.utils.data.Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载香蕉检测数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, is_train</span>):</span></span><br><span class="line">        self.features, self.labels = read_data_bananas(is_train)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + (<span class="string">f&#x27; training examples&#x27;</span> <span class="keyword">if</span></span><br><span class="line">              is_train <span class="keyword">else</span> <span class="string">f&#x27; validation examples&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (self.features[idx].<span class="built_in">float</span>(), self.labels[idx])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br></pre></td></tr></table></figure>
<p>最后，我们定义<code>load_data_bananas</code>函数，来为训练集和测试集返回两个数据加载器实例。对于测试集，无须按随机顺序读取它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_bananas</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载香蕉检测数据集&quot;&quot;&quot;</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=<span class="literal">True</span>),</span><br><span class="line">                                             batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=<span class="literal">False</span>),</span><br><span class="line">                                           batch_size)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter</span><br></pre></td></tr></table></figure>
<p>让我们读取一个小批量，并打印其中的图像和标签的形状。 图像的小批量的形状为（批量大小、通道数、高度、宽度），看起来很眼熟：它与我们之前图像分类任务中的相同。 标签的小批量的形状为（批量大小，$m$，5），其中$m$是数据集的任何图像中边界框可能出现的最大数量。</p>
<p>小批量计算虽然高效，但它要求每张图像含有相同数量的边界框，以便放在同一个批量中。 通常来说，图像可能拥有不同数量个边界框；因此，在达到$m$之前，边界框少于$m$的图像将被非法边界框填充。 这样，每个边界框的标签将被长度为5的数组表示。 数组中的第一个元素是边界框中对象的类别，其中-1表示用于填充的非法边界框。 数组的其余四个元素是边界框左上角和右下角的（$x$ ,$y$）坐标值（值域在0～1之间）。 对于香蕉数据集而言，由于每张图像上只有一个边界框，因此$m=1$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size, edge_size = <span class="number">32</span>, <span class="number">256</span></span><br><span class="line">train_iter, _ = load_data_bananas(batch_size)</span><br><span class="line">batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_iter))</span><br><span class="line">batch[<span class="number">0</span>].shape, batch[<span class="number">1</span>].shape</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h3><p>让我们展示10幅带有真实边界框的图像。 我们可以看到在所有这些图像中香蕉的旋转角度、大小和位置都有所不同。 当然，这只是一个简单的人工数据集，实践中真实世界的数据集通常要复杂得多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">imgs = (batch[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>].permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)) / <span class="number">255</span></span><br><span class="line">axes = d2l.show_images(imgs, <span class="number">2</span>, <span class="number">5</span>, scale=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> ax, label <span class="keyword">in</span> <span class="built_in">zip</span>(axes, batch[<span class="number">1</span>][<span class="number">0</span>:<span class="number">10</span>]):</span><br><span class="line">    d2l.show_bboxes(ax, [label[<span class="number">0</span>][<span class="number">1</span>:<span class="number">5</span>] * edge_size], colors=[<span class="string">&#x27;w&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_object-detection-dataset_641ef0_66_0.png" alt="../_images/output_object-detection-dataset_641ef0_66_0.png"></p>
<hr>
<h2 id="单发多框检测SSD"><a href="#单发多框检测SSD" class="headerlink" title="单发多框检测SSD"></a>单发多框检测SSD</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>此模型主要由基础网络组成，其后是几个多尺度特征块。 基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。 单发多框检测论文中选用了在分类层之前截断的VGG现在也常用ResNet替代。 我们可以设计基础网络，使它输出的高和宽较大。 这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。 接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。</p>
<p>由于接近顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。 简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorassd.svg" alt="../_images/ssd.svg"></p>
<hr>
<p><strong>类别预测层</strong></p>
<p>设目标类别的数量为$q$。这样一来，锚框有$q+1$个类别，其中0类是背景。在某个尺度下，设特征图的高和宽分别为$h$和$w$。如果以其中每个单元为中心生成$a$个锚框，那么我们需要对$hwa$个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。回忆NiN网络一节介绍的使用卷积层的通道来输出类别预测的方法，单发多框检测采用同样的方法来降低模型复杂度。</p>
<p>具体来说，类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入同一空间坐标（$x$、$y$）：输出特征图上（$x$、$y$）坐标的通道里包含了以输入特征图（$x$、$y$）坐标为中心生成的所有锚框的类别预测。因此输出通道数为$a(q+1)$，其中索引为$i(q+1) + j$（$0 \leq j \leq q$）的通道代表了索引为$i$的锚框有关类别索引为$j$的预测。</p>
<p>这个公式说明了如何在输出特征图的通道中安排每个锚框的类别预测。对于第($i$)个锚框，它的类别预测被存储在输出特征图的一系列通道中，这些通道的索引从($i(q+1)$)开始，到($i(q+1) + q$)结束。这里的($j$)是类别索引，范围从0到($q$)，其中0通常代表背景类别</p>
<p>在下面，我们定义了这样一个类别预测层，通过参数<code>num_anchors</code>和<code>num_classes</code>分别指定了$a$和$q$。该图层使用填充为1的$3\times3$的卷积层。此卷积层的输入和输出的宽度和高度保持不变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cls_predictor</span>(<span class="params">num_inputs, num_anchors, num_classes</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(num_inputs, num_anchors * (num_classes + <span class="number">1</span>),</span><br><span class="line">                     kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>边界框预测层的设计与类别预测层的设计类似。 唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是$q+1$个类别。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def bbox_predictor(num_inputs, num_anchors):</span><br><span class="line">    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>连结多尺度的预测</strong></p>
<p>正如我们所提到的，单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。在不同的尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。因此，不同尺度下预测输出的形状可能会有所不同。</p>
<p>在以下示例中，我们为同一个小批量构建两个不同比例（<code>Y1</code>和<code>Y2</code>）的特征图，其中<code>Y2</code>的高度和宽度是<code>Y1</code>的一半。以类别预测为例，假设<code>Y1</code>和<code>Y2</code>的每个单元分别生成了$5$个和$3$个锚框。进一步假设目标类别的数量为$10$，对于特征图<code>Y1</code>和<code>Y2</code>，类别预测输出中的通道数分别为$5\times(10+1)=55$和$3\times(10+1)=33$，其中任一输出的形状是（批量大小，通道数，高度，宽度）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x, block</span>):</span></span><br><span class="line">    <span class="keyword">return</span> block(x)</span><br><span class="line"></span><br><span class="line">Y1 = forward(torch.zeros((<span class="number">2</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">20</span>)), cls_predictor(<span class="number">8</span>, <span class="number">5</span>, <span class="number">10</span>))</span><br><span class="line">Y2 = forward(torch.zeros((<span class="number">2</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>)), cls_predictor(<span class="number">16</span>, <span class="number">3</span>, <span class="number">10</span>))</span><br><span class="line">Y1.shape, Y2.shape</span><br><span class="line"><span class="comment">#(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))</span></span><br></pre></td></tr></table></figure>
<p>正如我们所看到的，除了批量大小这一维度外，其他三个维度都具有不同的尺寸。 为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。</p>
<p>通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。 因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的（批量大小，高×宽×通道数）的格式，以方便之后在维度1上的连结。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_pred</span>(<span class="params">pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.flatten(pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_preds</span>(<span class="params">preds</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.cat([flatten_pred(p) <span class="keyword">for</span> p <span class="keyword">in</span> preds], dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>这样一来，尽管<code>Y1</code>和<code>Y2</code>在通道数、高度和宽度方面具有不同的大小，我们仍然可以在同一个小批量的两个不同尺度上连接这两个预测输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">concat_preds([Y1, Y2]).shape</span><br><span class="line"><span class="comment">#torch.Size([2, 25300])</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>高和宽减半块</strong></p>
<p>为了在多个尺度下检测目标，我们在下面定义了高和宽减半块<code>down_sample_blk</code>，该模块将输入特征图的高度和宽度减半。事实上，该块应用了在<code>subsec_vgg-blocks</code>中的VGG模块设计。</p>
<p>更具体地说，每个高和宽减半块由两个填充为$1$的$3\times3$的卷积层、以及步幅为$2$的$2\times2$最大汇聚层组成。我们知道，填充为$1$的$3\times3$卷积层不改变特征图的形状。但是，其后的$2\times2$的最大汇聚层将输入特征图的高度和宽度减少了一半。</p>
<p>对于此高和宽减半块的输入和输出特征图，因为$1\times 2+(3-1)+(3-1)=6$，所以输出中的每个单元在输入上都有一个$6\times6$的感受野。因此，高和宽减半块会扩大每个单元在其输出特征图中的感受野。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">down_sample_blk</span>(<span class="params">in_channels, out_channels</span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        blk.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                             kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.BatchNorm2d(out_channels))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    blk.append(nn.MaxPool2d(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>基本网络块</strong></p>
<p>基本网络块用于从输入图像中抽取特征。 为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍。 给定输入图像的形状为256×256，此基本网络块输出的特征图形状为32×32（$256/2^3=32$）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">base_net</span>():</span></span><br><span class="line">    blk = []</span><br><span class="line">    num_filters = [<span class="number">3</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(num_filters) - <span class="number">1</span>):</span><br><span class="line">        blk.append(down_sample_blk(num_filters[i], num_filters[i+<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line">forward(torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)), base_net()).shape</span><br><span class="line"><span class="comment">#torch.Size([2, 64, 32, 32])</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>完整模型</strong></p>
<p>完整的单发多框检测模型由五个模块组成。每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是基本网络块，第二个到第四个是高和宽减半块，最后一个模块使用全局最大池将高度和宽度都降到1。从技术上讲，第二到第五个区块都是多尺度特征块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_blk</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        blk = base_net()</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">        blk = down_sample_blk(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">4</span>:</span><br><span class="line">        blk = nn.AdaptiveMaxPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        blk = down_sample_blk(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<p>现在我们为每个块定义前向传播。与图像分类任务不同，此处的输出包括：CNN特征图<code>Y</code>；在当前尺度下根据<code>Y</code>生成的锚框；预测的这些锚框的类别和偏移量（基于<code>Y</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">blk_forward</span>(<span class="params">X, blk, size, ratio, cls_predictor, bbox_predictor</span>):</span></span><br><span class="line">    <span class="comment"># 对输入X应用一个块（如卷积层、汇聚层等组合），这里的blk可能是一个卷积块或者其他类型的网络层块，</span></span><br><span class="line">    <span class="comment"># 它用于提取特征或者改变特征图的尺寸。</span></span><br><span class="line">    Y = blk(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成锚框。这是在特征图Y上基于预定义的尺寸（size）和宽高比（ratio）生成的一系列锚框。</span></span><br><span class="line">    <span class="comment"># 这些锚框用于后续的对象检测任务。multibox_prior函数根据给定的特征图、尺寸和比率生成锚框。</span></span><br><span class="line">    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 应用类别预测器（cls_predictor）于特征图Y，得到类别预测结果。</span></span><br><span class="line">    <span class="comment"># 这个预测器通常是一个卷积层，用于预测每个锚框的类别。</span></span><br><span class="line">    cls_preds = cls_predictor(Y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 应用边界框预测器（bbox_predictor）于特征图Y，得到边界框预测结果。</span></span><br><span class="line">    <span class="comment"># 这个预测器也通常是一个卷积层，用于预测每个锚框的位置调整值。</span></span><br><span class="line">    bbox_preds = bbox_predictor(Y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 返回特征图Y、生成的锚框、类别预测结果和边界框预测结果。</span></span><br><span class="line">    <span class="keyword">return</span> (Y, anchors, cls_preds, bbox_preds)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>回想一下，在图中，一个较接近顶部的多尺度特征块是用于检测较大目标的，因此需要生成更大的锚框。在上面的前向传播中，在每个多尺度特征块上，我们通过调用的<code>multibox_prior</code>函数的<code>sizes</code>参数传递两个比例值的列表。</p>
<p>在下面，0.2和1.05之间的区间被均匀分成五个部分，以确定五个模块的在不同尺度下的较小值：0.2、0.37、0.54、0.71和0.88。之后，他们较大的值由$\sqrt{0.2 \times 0.37} = 0.272$、$\sqrt{0.37 \times 0.54} = 0.447$等给出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sizes = [[<span class="number">0.2</span>, <span class="number">0.272</span>], [<span class="number">0.37</span>, <span class="number">0.447</span>], [<span class="number">0.54</span>, <span class="number">0.619</span>], [<span class="number">0.71</span>, <span class="number">0.79</span>],</span><br><span class="line">         [<span class="number">0.88</span>, <span class="number">0.961</span>]]</span><br><span class="line">ratios = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>]] * <span class="number">5</span></span><br><span class="line">num_anchors = <span class="built_in">len</span>(sizes[<span class="number">0</span>]) + <span class="built_in">len</span>(ratios[<span class="number">0</span>]) - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>现在，我们就可以按如下方式定义完整的模型<code>TinySSD</code>了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TinySSD</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TinySSD, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># num_classes: 模型需要识别的类别数目。</span></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="comment"># idx_to_in_channels: 指定了不同层（或称为&quot;块&quot;）的输入通道数，这是基于模型架构的设计。</span></span><br><span class="line">        idx_to_in_channels = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            <span class="comment"># 动态地为模型添加五个不同的块，每个块负责不同尺度的特征提取。</span></span><br><span class="line">            <span class="comment"># get_blk(i)函数根据索引i返回相应的块。</span></span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;blk_<span class="subst">&#123;i&#125;</span>&#x27;</span>, get_blk(i))</span><br><span class="line">            <span class="comment"># 为每个块添加一个类别预测器，用于预测锚框所属的类别。</span></span><br><span class="line">            <span class="comment"># cls_predictor函数根据输入通道数、锚框数量和类别数创建类别预测器。</span></span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;cls_<span class="subst">&#123;i&#125;</span>&#x27;</span>, cls_predictor(idx_to_in_channels[i], num_anchors, num_classes))</span><br><span class="line">            <span class="comment"># 为每个块添加一个边界框预测器，用于预测锚框的具体位置。</span></span><br><span class="line">            <span class="comment"># bbox_predictor函数根据输入通道数和锚框数量创建边界框预测器。</span></span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;bbox_<span class="subst">&#123;i&#125;</span>&#x27;</span>, bbox_predictor(idx_to_in_channels[i], num_anchors))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># 初始化锚框、类别预测和边界框预测的列表，每个列表包含5个元素，对应于5个不同的块。</span></span><br><span class="line">        anchors, cls_preds, bbox_preds = [<span class="literal">None</span>] * <span class="number">5</span>, [<span class="literal">None</span>] * <span class="number">5</span>, [<span class="literal">None</span>] * <span class="number">5</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            <span class="comment"># 对于每个块，使用blk_forward函数进行前向传播。</span></span><br><span class="line">            <span class="comment"># blk_forward函数接收当前的输入X和该块相关的参数，返回处理后的X、锚框、类别预测和边界框预测。</span></span><br><span class="line">            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(</span><br><span class="line">                X, <span class="built_in">getattr</span>(self, <span class="string">f&#x27;blk_<span class="subst">&#123;i&#125;</span>&#x27;</span>), sizes[i], ratios[i],</span><br><span class="line">                <span class="built_in">getattr</span>(self, <span class="string">f&#x27;cls_<span class="subst">&#123;i&#125;</span>&#x27;</span>), <span class="built_in">getattr</span>(self, <span class="string">f&#x27;bbox_<span class="subst">&#123;i&#125;</span>&#x27;</span>))</span><br><span class="line">        <span class="comment"># 将所有块生成的锚框拼接在一起。</span></span><br><span class="line">        anchors = torch.cat(anchors, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将所有块的类别预测结果拼接在一起，并调整形状以适应后续处理。</span></span><br><span class="line">        cls_preds = concat_preds(cls_preds)</span><br><span class="line">        cls_preds = cls_preds.reshape(cls_preds.shape[<span class="number">0</span>], -<span class="number">1</span>, self.num_classes + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将所有块的边界框预测结果拼接在一起。</span></span><br><span class="line">        bbox_preds = concat_preds(bbox_preds)</span><br><span class="line">        <span class="comment"># 返回最终的锚框、类别预测和边界框预测结果。</span></span><br><span class="line">        <span class="keyword">return</span> anchors, cls_preds, bbox_preds</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>创建一个模型实例，然后使用它对一个256×256像素的小批量图像<code>X</code>执行前向传播。</p>
<p>如本节前面部分所示，第一个模块输出特征图的形状为$32×32$。 回想一下，第二到第四个模块为高和宽减半块，第五个模块为全局汇聚层。 由于以特征图的每个单元为中心有4个锚框生成，因此在所有五个尺度下，每个图像总共生成$(32^2+16^2+8^2+4^2+1)×4=5444$个锚框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = TinySSD(num_classes=<span class="number">1</span>)</span><br><span class="line">X = torch.zeros((<span class="number">32</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">anchors, cls_preds, bbox_preds = net(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output anchors:&#x27;</span>, anchors.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output class preds:&#x27;</span>, cls_preds.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output bbox preds:&#x27;</span>, bbox_preds.shape)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">output anchors: torch.Size([1, 5444, 4])</span></span><br><span class="line"><span class="string">output class preds: torch.Size([32, 5444, 2])</span></span><br><span class="line"><span class="string">output bbox preds: torch.Size([32, 21776])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><strong>读取数据和初始化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_iter, _ = d2l.load_data_bananas(batch_size)</span><br></pre></td></tr></table></figure>
<p>香蕉检测数据集中，目标的类别数为1。 定义好模型后，我们需要初始化其参数并定义优化算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device, net = d2l.try_gpu(), TinySSD(num_classes=<span class="number">1</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>, weight_decay=<span class="number">5e-4</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>定义损失函数和评价函数</strong></p>
<p>目标检测有两种类型的损失。 第一种有关锚框类别的损失：我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算； 第二种有关正类锚框偏移量的损失：预测偏移量是一个回归问题。 但是，对于这个回归问题，我们在这里不使用平方损失，而是使用$L_1$范数损失，即预测值和真实值之差的绝对值。 掩码变量<code>bbox_masks</code>令负类锚框和填充锚框不参与损失的计算。 最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义分类损失函数，使用交叉熵损失。`reduction=&#x27;none&#x27;`意味着损失将不会在此步骤内部进行求和或平均，以便后续可以自定义操作。</span></span><br><span class="line">cls_loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义边界框回归损失函数，使用L1损失。同样地，`reduction=&#x27;none&#x27;`允许后续自定义处理。</span></span><br><span class="line">bbox_loss = nn.L1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_loss</span>(<span class="params">cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks</span>):</span></span><br><span class="line">    <span class="comment"># 获取批量大小和类别数目。</span></span><br><span class="line">    batch_size, num_classes = cls_preds.shape[<span class="number">0</span>], cls_preds.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="comment"># 计算分类损失。</span></span><br><span class="line">    <span class="comment"># 首先，将类别预测结果重塑为二维张量，以适应交叉熵损失函数的输入要求。</span></span><br><span class="line">    <span class="comment"># 然后，重塑类别标签以匹配预测结果的形状。</span></span><br><span class="line">    <span class="comment"># 最后，将损失重塑回原始批量大小，并计算每个样本的平均损失。</span></span><br><span class="line">    cls = cls_loss(cls_preds.reshape(-<span class="number">1</span>, num_classes),</span><br><span class="line">                   cls_labels.reshape(-<span class="number">1</span>)).reshape(batch_size, -<span class="number">1</span>).mean(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 计算边界框回归损失。</span></span><br><span class="line">    <span class="comment"># 首先，应用边界框掩码（bbox_masks）到预测和标签上，只考虑有目标的预测框。</span></span><br><span class="line">    <span class="comment"># 然后，计算每个样本的平均损失。</span></span><br><span class="line">    bbox = bbox_loss(bbox_preds * bbox_masks,</span><br><span class="line">                     bbox_labels * bbox_masks).mean(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 将分类损失和边界框损失相加，得到每个样本的总损失。</span></span><br><span class="line">    <span class="keyword">return</span> cls + bbox</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们可以沿用准确率评价分类结果。 由于偏移量使用了$L_1$范数损失，我们使用<strong><em>平均绝对误差</em></strong>来评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cls_eval</span>(<span class="params">cls_preds, cls_labels</span>):</span></span><br><span class="line">    <span class="comment"># 由于类别预测结果放在最后一维，argmax需要指定最后一维。</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>((cls_preds.argmax(dim=-<span class="number">1</span>).<span class="built_in">type</span>(</span><br><span class="line">        cls_labels.dtype) == cls_labels).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_eval</span>(<span class="params">bbox_preds, bbox_labels, bbox_masks</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>((torch.<span class="built_in">abs</span>((bbox_labels - bbox_preds) * bbox_masks)).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>训练</strong></p>
<p>在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框（<code>anchors</code>），并预测其类别（<code>cls_preds</code>）和偏移量（<code>bbox_preds</code>）。 然后，我们根据标签信息<code>Y</code>为生成的锚框标记类别（<code>cls_labels</code>）和偏移量（<code>bbox_labels</code>）。 最后，我们根据类别和偏移量的预测和标注值计算损失函数。为了代码简洁，这里没有评价测试数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置训练周期数和计时器。</span></span><br><span class="line">num_epochs, timer = <span class="number">20</span>, d2l.Timer()</span><br><span class="line"><span class="comment"># 初始化动画显示器，用于实时显示训练过程中的分类错误和边界框的平均绝对误差。</span></span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], legend=[<span class="string">&#x27;class error&#x27;</span>, <span class="string">&#x27;bbox mae&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型移动到指定的设备上，例如GPU。</span></span><br><span class="line">net = net.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练循环。</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># 初始化累加器，用于统计分类准确性和边界框的平均绝对误差。</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">4</span>)  <span class="comment"># 分别累加分类错误数、分类总数、边界框绝对误差和边界框总数。</span></span><br><span class="line">    net.train()  <span class="comment"># 将模型设置为训练模式。</span></span><br><span class="line">    <span class="keyword">for</span> features, target <span class="keyword">in</span> train_iter:  <span class="comment"># 遍历训练数据集。</span></span><br><span class="line">        timer.start()  <span class="comment"># 开始计时。</span></span><br><span class="line">        trainer.zero_grad()  <span class="comment"># 清除之前的梯度。</span></span><br><span class="line">        X, Y = features.to(device), target.to(device)  <span class="comment"># 将数据移动到指定设备。</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对输入图像生成多尺度的锚框，并预测每个锚框的类别和偏移量。</span></span><br><span class="line">        anchors, cls_preds, bbox_preds = net(X)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 根据真实标注生成每个锚框的类别标签和偏移量标签。</span></span><br><span class="line">        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算分类和边界框预测的损失。</span></span><br><span class="line">        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks)</span><br><span class="line">        l.mean().backward()  <span class="comment"># 反向传播误差。</span></span><br><span class="line">        trainer.step()  <span class="comment"># 更新模型参数。</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新累加器。</span></span><br><span class="line">        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),</span><br><span class="line">                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),</span><br><span class="line">                   bbox_labels.numel())</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 计算并记录分类错误率和边界框的平均绝对误差。</span></span><br><span class="line">    cls_err, bbox_mae = <span class="number">1</span> - metric[<span class="number">0</span>] / metric[<span class="number">1</span>], metric[<span class="number">2</span>] / metric[<span class="number">3</span>]</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, (cls_err, bbox_mae))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最终的分类错误率和边界框平均绝对误差。</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;class err <span class="subst">&#123;cls_err:<span class="number">.2</span>e&#125;</span>, bbox mae <span class="subst">&#123;bbox_mae:<span class="number">.2</span>e&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 输出训练速度，即每秒可以处理的样本数。</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">len</span>(train_iter.dataset) / timer.stop():<span class="number">.1</span>f&#125;</span> examples/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">class err 3.22e-03, bbox mae 3.16e-03</span></span><br><span class="line"><span class="string">3353.9 examples/sec on cuda:0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_ssd_739e1b_210_1.svg" alt="../_images/output_ssd_739e1b_210_1.svg"></p>
<h3 id="预测目标"><a href="#预测目标" class="headerlink" title="预测目标"></a><strong>预测目标</strong></h3><p>在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torchvision.io.read_image(&#x27;../img/banana.jpg&#x27;).unsqueeze(0).float()</span><br><span class="line">img = X.squeeze(0).permute(1, 2, 0).long()</span><br></pre></td></tr></table></figure>
<p>使用下面的<code>multibox_detection</code>函数，我们可以根据锚框及其预测偏移量得到预测边界框。然后，通过非极大值抑制来移除相似的预测边界框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="comment"># 设置模型为评估模式。这通常会关闭dropout和批量归一化层的行为。</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="comment"># 将输入数据X送到指定的设备（如GPU），并通过模型获取锚框、类别预测和边界框预测。</span></span><br><span class="line">    anchors, cls_preds, bbox_preds = net(X.to(device))</span><br><span class="line">    <span class="comment"># 将类别预测的结果通过softmax函数处理，获取预测的概率分布。</span></span><br><span class="line">    <span class="comment"># softmax操作应用于每个位置上的类别得分。</span></span><br><span class="line">    <span class="comment"># permute操作调整维度的顺序，以符合multibox_detection函数的输入要求。</span></span><br><span class="line">    cls_probs = F.softmax(cls_preds, dim=<span class="number">2</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 使用multibox_detection函数处理预测结果。</span></span><br><span class="line">    <span class="comment"># 该函数根据类别概率、边界框预测和锚框生成最终的检测结果。</span></span><br><span class="line">    <span class="comment"># 检测结果中每行的格式为[class_id, score, bbox]，其中bbox是边界框的坐标。</span></span><br><span class="line">    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)</span><br><span class="line">    <span class="comment"># 过滤掉class_id为-1的结果，这些结果代表背景或低置信度的预测。</span></span><br><span class="line">    idx = [i <span class="keyword">for</span> i, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(output[<span class="number">0</span>]) <span class="keyword">if</span> row[<span class="number">0</span>] != -<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 返回过滤后的检测结果。</span></span><br><span class="line">    <span class="keyword">return</span> output[<span class="number">0</span>, idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用predict函数对输入X进行预测，并存储预测结果。</span></span><br><span class="line">output = predict(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后，我们筛选所有置信度不低于0.9的边界框，做为最终输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display</span>(<span class="params">img, output, threshold</span>):</span></span><br><span class="line">    d2l.set_figsize((<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">    fig = d2l.plt.imshow(img)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> output:</span><br><span class="line">        score = <span class="built_in">float</span>(row[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> score &lt; threshold:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        h, w = img.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        bbox = [row[<span class="number">2</span>:<span class="number">6</span>] * torch.tensor((w, h, w, h), device=row.device)]</span><br><span class="line">        d2l.show_bboxes(fig.axes, bbox, <span class="string">&#x27;%.2f&#x27;</span> % score, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">display(img, output.cpu(), threshold=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_ssd_739e1b_246_0.svg" alt="../_images/output_ssd_739e1b_246_0.svg"></p>
<hr>
<h2 id="区域卷积神经网络R-CNN"><a href="#区域卷积神经网络R-CNN" class="headerlink" title="区域卷积神经网络R-CNN"></a>区域卷积神经网络R-CNN</h2><p>除了上一节描述的单发多框检测之外， 区域卷积神经网络（region-based CNN或regions with CNN features，R-CNN）也是将深度模型应用于目标检测的开创性工作之一。 本节将介绍R-CNN及其一系列改进方法：快速的R-CNN（Fast R-CNN）、更快的R-CNN（Faster R-CNN）和掩码R-CNN（Mask R-CNN。 限于篇幅，我们只着重介绍这些模型的设计思路。</p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p><strong><em>R-CNN</em></strong>首先从输入图像中选取若干（例如2000个）<strong><em>提议区域</em></strong>（如锚框也是一种选取方法），并标注它们的类别和边界框（如偏移量）。然后，用卷积神经网络对每个提议区域进行前向传播以抽取其特征。 接下来，我们用每个提议区域的特征来预测类别和边界框。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorar-cnn.svg" alt="../_images/r-cnn.svg"></p>
<p>图中展示了R-CNN模型。具体来说，R-CNN包括以下四个步骤：</p>
<ol>
<li>对输入图像使用<strong><em>选择性搜索</em></strong>来选取多个高质量的提议区域。这些提议区域通常是在多个尺度下选取的，并具有不同的形状和大小。每个提议区域都将被标注类别和真实边界框；</li>
<li>选择一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向传播输出抽取的提议区域特征；</li>
<li>将每个提议区域的特征连同其标注的类别作为一个样本。训练多个支持向量机对目标分类，其中每个支持向量机用来判断样本是否属于某一个类别；</li>
<li>将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。</li>
</ol>
<p>尽管R-CNN模型通过预训练的卷积神经网络有效地抽取了图像特征，但它的速度很慢。 想象一下，我们可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。 这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。</p>
<hr>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast  R-CNN"></a>Fast  R-CNN</h3><p>R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。 由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。 <strong><em>Fast R-CNN</em></strong> 对R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorafast-rcnn.svg" alt="../_images/fast-rcnn.svg"></p>
<p>它的主要计算如下：</p>
<ol>
<li>与R-CNN相比，Fast R-CNN用来提取特征的卷积神经网络的输入是整个图像，而不是各个提议区域。此外，这个网络通常会参与训练。设输入为一张图像，将卷积神经网络的输出的形状记为$1 \times c \times h_1  \times w_1$</li>
<li>假设选择性搜索生成了$n$个提议区域。这些形状各异的提议区域在卷积神经网络的输出上分别标出了形状各异的兴趣区域。然后，这些感兴趣的区域需要进一步抽取出形状相同的特征（比如指定高度$h_2$和宽度$w_2$），以便于连结后输出。为了实现这一目标，Fast R-CNN引入了<strong><em>兴趣区域汇聚层</em></strong>（RoI pooling）：将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为$n \times c \times h_2 \times w_2$；</li>
<li>通过全连接层将输出形状变换为$n \times d$，其中超参数$d$取决于模型设计；</li>
<li>预测$n$个提议区域中每个区域的类别和边界框。更具体地说，在预测类别和边界框时，将全连接层的输出分别转换为形状为$n \times q$（$q$是类别的数量）的输出和形状为$n \times 4$的输出。其中预测类别时使用softmax回归。</li>
</ol>
<p>在Fast R-CNN中提出的兴趣区域汇聚层与CNN中介绍的汇聚层有所不同。在汇聚层中，我们通过设置汇聚窗口、填充和步幅的大小来间接控制输出形状。而兴趣区域汇聚层对每个区域的输出形状是可以直接指定的。</p>
<p>例如，指定每个区域输出的高和宽分别为$h_2$和$w_2$。对于任何形状为$h \times w$的兴趣区域窗口，该窗口将被划分为$h_2 \times w_2$子窗口网格，其中每个子窗口的大小约为$(h/h_2) \times (w/w_2)$。在实践中，任何子窗口的高度和宽度都应向上取整，其中的最大元素作为该子窗口的输出。因此，兴趣区域汇聚层可从形状各异的兴趣区域中均抽取出形状相同的特征。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraroi.svg" alt="../_images/roi.svg"></p>
<p>作为说明性示例，图中提到，在$4 \times 4$的输入中，我们选取了左上角$3\times 3$的兴趣区域。对于该兴趣区域，我们通过$2\times 2$的兴趣区域汇聚层得到一个$2\times 2$的输出。请注意，四个划分后的子窗口中分别含有元素0、1、4、5（5最大）；2、6（6最大）；8、9（9最大）；以及10。</p>
<hr>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>为了较精确地检测目标结果，Fast R-CNN模型通常需要在选择性搜索中生成大量的提议区域。 <strong><em>Faster R-CNN</em></strong> 提出将选择性搜索替换为<strong><em>区域提议网络</em></strong>（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorafaster-rcnn.svg" alt="../_images/faster-rcnn.svg"></p>
<p>描述了Faster R-CNN模型。 与Fast R-CNN相比，Faster R-CNN只将生成提议区域的方法从选择性搜索改为了区域提议网络，模型的其余部分保持不变。具体来说，区域提议网络的计算步骤如下：</p>
<ol>
<li>使用填充为1的3×3的卷积层变换卷积神经网络的输出，并将输出通道数记为$c$。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为$c$的新特征。</li>
<li>以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们。</li>
<li>使用锚框中心单元长度为$c$的特征，分别预测该锚框的二元类别（含目标还是背景）和边界框。</li>
<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。</li>
</ol>
<p>值得一提的是，区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。 换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。 作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。</p>
<hr>
<h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h3><p>如果在训练集中还标注了每个目标在图像上的像素级位置，那么<strong><em>Mask R-CNN</em></strong>能够有效地利用这些详尽的标注信息进一步提升目标检测的精度。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoramask-rcnn.svg" alt="../_images/mask-rcnn.svg"></p>
<p>Mask R-CNN是基于Faster R-CNN修改而来的。 具体来说，Mask R-CNN将兴趣区域汇聚层替换为了 <em>兴趣区域对齐</em>层，使用<strong><em>双线性插值</em></strong>（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。 兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。 它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。 本章的后续章节将更详细地介绍如何使用全卷积网络预测图像中像素级的语义。</p>
<hr>
<h2 id="语义分割和数据集"><a href="#语义分割和数据集" class="headerlink" title="语义分割和数据集"></a>语义分割和数据集</h2><p>在之前讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。 本节将探讨<strong><em>语义分割</em></strong>（semantic segmentation）问题，它重点关注于如何将图像分割成属于不同语义类别的区域。 与目标检测不同，语义分割可以识别并理解图像中每一个像素的内容：其语义区域的标注和预测是像素级的。图中展示了语义分割中图像有关狗、猫和背景的标签。 与目标检测相比，语义分割标注的像素级的边框显然更加精细。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorasegmentation.svg" alt="../_images/segmentation.svg"></p>
<hr>
<h3 id="图像分割和实例分割"><a href="#图像分割和实例分割" class="headerlink" title="图像分割和实例分割"></a>图像分割和实例分割</h3><p>计算机视觉领域还有2个与语义分割相似的重要问题，即<strong><em>图像分割</em></strong>（image segmentation）和<strong><em>实例分割</em></strong>（instance segmentation）。 我们在这里将它们同语义分割简单区分一下。</p>
<ul>
<li><strong><em>图像分割</em></strong> 将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以图中的图像作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。</li>
<li><strong><em>实例分割</em></strong> 也叫<strong><em>同时检测并分割</em></strong>（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。</li>
</ul>
<hr>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure>
<p>数据集的tar文件大约为2GB，所以下载可能需要一段时间。 提取出的数据集位于<code>../data/VOCdevkit/VOC2012</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;voc2012&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;VOCtrainval_11-May-2012.tar&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;4e443f8a2eca6b1dac8a6c57641b67dd40621a49&#x27;</span>)</span><br><span class="line"></span><br><span class="line">voc_dir = d2l.download_extract(<span class="string">&#x27;voc2012&#x27;</span>, <span class="string">&#x27;VOCdevkit/VOC2012&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>进入路径<code>../data/VOCdevkit/VOC2012</code>之后，我们可以看到数据集的不同组件。 <code>ImageSets/Segmentation</code>路径包含用于训练和测试样本的文本文件，而<code>JPEGImages</code>和<code>SegmentationClass</code>路径分别存储着每个示例的输入图像和标签。 此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。 此外，标签中颜色相同的像素属于同一个语义类别。 下面将<code>read_voc_images</code>函数定义为将所有输入的图像和标签读入内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_voc_images</span>(<span class="params">voc_dir, is_train=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取所有VOC图像并标注&quot;&quot;&quot;</span></span><br><span class="line">    txt_fname = os.path.join(voc_dir, <span class="string">&#x27;ImageSets&#x27;</span>, <span class="string">&#x27;Segmentation&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;train.txt&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val.txt&#x27;</span>)</span><br><span class="line">    mode = torchvision.io.image.ImageReadMode.RGB</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(txt_fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = f.read().split()</span><br><span class="line">    features, labels = [], []</span><br><span class="line">    <span class="keyword">for</span> i, fname <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        features.append(torchvision.io.read_image(os.path.join(</span><br><span class="line">            voc_dir, <span class="string">&#x27;JPEGImages&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.jpg&#x27;</span>)))</span><br><span class="line">        labels.append(torchvision.io.read_image(os.path.join(</span><br><span class="line">            voc_dir, <span class="string">&#x27;SegmentationClass&#x27;</span> ,<span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.png&#x27;</span>), mode))</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">train_features, train_labels = read_voc_images(voc_dir, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>下面我们绘制前5个输入图像及其标签。 在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line">imgs = train_features[<span class="number">0</span>:n] + train_labels[<span class="number">0</span>:n]</span><br><span class="line">imgs = [img.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">d2l.show_images(imgs, <span class="number">2</span>, n);</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_semantic-segmentation-and-dataset_23ff18_42_0.png" alt="../_images/output_semantic-segmentation-and-dataset_23ff18_42_0.png"></p>
<p>接下来，我们列举RGB颜色值和类名。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">VOC_COLORMAP = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">192</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">128</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">VOC_CLASSES = [<span class="string">&#x27;background&#x27;</span>, <span class="string">&#x27;aeroplane&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;diningtable&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;motorbike&#x27;</span>, <span class="string">&#x27;person&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;sofa&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;tv/monitor&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>通过上面定义的两个常量，我们可以方便地查找标签中每个像素的类索引。 我们定义了<code>voc_colormap2label</code>函数来构建从上述RGB颜色值到类别索引的映射，而<code>voc_label_indices</code>函数将RGB值映射到在Pascal VOC2012数据集中的类别索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voc_colormap2label</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建从RGB到VOC类别索引的映射&quot;&quot;&quot;</span></span><br><span class="line">    colormap2label = torch.zeros(<span class="number">256</span> ** <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">    <span class="keyword">for</span> i, colormap <span class="keyword">in</span> <span class="built_in">enumerate</span>(VOC_COLORMAP):</span><br><span class="line">        colormap2label[</span><br><span class="line">            (colormap[<span class="number">0</span>] * <span class="number">256</span> + colormap[<span class="number">1</span>]) * <span class="number">256</span> + colormap[<span class="number">2</span>]] = i</span><br><span class="line">    <span class="keyword">return</span> colormap2label</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voc_label_indices</span>(<span class="params">colormap, colormap2label</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将VOC标签中的RGB值映射到它们的类别索引&quot;&quot;&quot;</span></span><br><span class="line">    colormap = colormap.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy().astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    idx = ((colormap[:, :, <span class="number">0</span>] * <span class="number">256</span> + colormap[:, :, <span class="number">1</span>]) * <span class="number">256</span></span><br><span class="line">           + colormap[:, :, <span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> colormap2label[idx]</span><br></pre></td></tr></table></figure>
<p>例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">y = voc_label_indices(train_labels[<span class="number">0</span>], voc_colormap2label())</span><br><span class="line">y[<span class="number">105</span>:<span class="number">115</span>, <span class="number">130</span>:<span class="number">140</span>], VOC_CLASSES[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span></span><br><span class="line"><span class="string">         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]),</span></span><br><span class="line"><span class="string"> &#x27;aeroplane&#x27;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>预处理数据</strong></p>
<p>在之前的实验，我们通过再缩放图像使其符合模型的输入形状。 然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。 这样的映射可能不够精确，尤其在不同语义的分割区域。 为了避免这个问题，我们将图像裁剪为固定尺寸，而不是再缩放。 具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">voc_rand_crop</span>(<span class="params">feature, label, height, width</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;随机裁剪特征和标签图像&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 使用torchvision库中的RandomCrop.get_params方法获取随机裁剪的位置</span></span><br><span class="line">    rect = torchvision.transforms.RandomCrop.get_params(</span><br><span class="line">        feature, (height, width))</span><br><span class="line">    <span class="comment"># 根据获取的裁剪位置(rect)，对特征图像进行裁剪</span></span><br><span class="line">    feature = torchvision.transforms.functional.crop(feature, *rect)</span><br><span class="line">    <span class="comment"># 根据同样的裁剪位置(rect)，对标签图像进行裁剪，以确保特征和标签的一致性</span></span><br><span class="line">    label = torchvision.transforms.functional.crop(label, *rect)</span><br><span class="line">    <span class="keyword">return</span> feature, label</span><br><span class="line"></span><br><span class="line">imgs = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="comment"># 对指定的图像和标签进行n次随机裁剪，裁剪后的图像大小为200x300</span></span><br><span class="line">    <span class="comment"># 这里假设train_features[0]和train_labels[0]是需要裁剪的特征和标签图像</span></span><br><span class="line">    imgs += voc_rand_crop(train_features[<span class="number">0</span>], train_labels[<span class="number">0</span>], <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将裁剪后的图像的通道顺序由CHW(通道、高度、宽度)调整为HWC(高度、宽度、通道)，以符合显示的需求</span></span><br><span class="line">imgs = [img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line"><span class="comment"># 显示裁剪后的图像，每行显示两个图像，总共显示n个图像</span></span><br><span class="line"><span class="comment"># d2l.show_images函数假设是一个用于展示图像列表的自定义函数</span></span><br><span class="line">d2l.show_images(imgs[::<span class="number">2</span>] + imgs[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>, n);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_semantic-segmentation-and-dataset_23ff18_90_0.png" alt="../_images/output_semantic-segmentation-and-dataset_23ff18_90_0.png"></p>
<hr>
<p><strong>自定义语义分割数据集类</strong></p>
<p>我们通过继承高级API提供的<code>Dataset</code>类，自定义了一个语义分割数据集类<code>VOCSegDataset</code>。 通过实现<code>__getitem__</code>函数，我们可以任意访问数据集中索引为<code>idx</code>的输入图像及其每个像素的类别索引。 由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的<code>filter</code>函数移除掉。 此外，我们还定义了<code>normalize_image</code>函数，从而对输入图像的RGB三个通道的值分别做标准化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VOCSegDataset</span>(<span class="params">torch.utils.data.Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载VOC数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, is_train, crop_size, voc_dir</span>):</span></span><br><span class="line">        <span class="comment"># 初始化时对图像进行标准化的变换</span></span><br><span class="line">        self.transform = torchvision.transforms.Normalize(</span><br><span class="line">            mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">        self.crop_size = crop_size  <span class="comment"># 裁剪大小</span></span><br><span class="line">        <span class="comment"># 读取VOC图像数据，返回特征图像和标签图像的列表</span></span><br><span class="line">        features, labels = read_voc_images(voc_dir, is_train=is_train)</span><br><span class="line">        <span class="comment"># 过滤并标准化特征图像</span></span><br><span class="line">        self.features = [self.normalize_image(feature)</span><br><span class="line">                         <span class="keyword">for</span> feature <span class="keyword">in</span> self.<span class="built_in">filter</span>(features)]</span><br><span class="line">        <span class="comment"># 过滤标签图像</span></span><br><span class="line">        self.labels = self.<span class="built_in">filter</span>(labels)</span><br><span class="line">        <span class="comment"># 获取颜色映射到标签的映射字典</span></span><br><span class="line">        self.colormap2label = voc_colormap2label()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normalize_image</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        <span class="comment"># 将图像数据标准化，使其均值为0，标准差为1</span></span><br><span class="line">        <span class="keyword">return</span> self.transform(img.<span class="built_in">float</span>() / <span class="number">255</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filter</span>(<span class="params">self, imgs</span>):</span></span><br><span class="line">        <span class="comment"># 过滤掉尺寸小于裁剪大小的图像</span></span><br><span class="line">        <span class="keyword">return</span> [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs <span class="keyword">if</span> (</span><br><span class="line">            img.shape[<span class="number">1</span>] &gt;= self.crop_size[<span class="number">0</span>] <span class="keyword">and</span></span><br><span class="line">            img.shape[<span class="number">2</span>] &gt;= self.crop_size[<span class="number">1</span>])]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="comment"># 获取单个样本，包括特征图像和对应的标签，根据索引idx进行随机裁剪</span></span><br><span class="line">        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],</span><br><span class="line">                                       *self.crop_size)</span><br><span class="line">        <span class="keyword">return</span> (feature, voc_label_indices(label, self.colormap2label))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 返回数据集中样本的总数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>读取</strong></p>
<p>我们通过自定义的<code>VOCSegDataset</code>类来分别创建训练集和测试集的实例。 假设我们指定随机裁剪的输出图像的形状为320×480， 下面我们可以查看训练集和测试集所保留的样本个数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crop_size = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">voc_train = VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir)</span><br><span class="line">voc_test = VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir)</span><br></pre></td></tr></table></figure>
<p>设批量大小为64，我们定义训练集的迭代器。 打印第一个小批量的形状会发现：与图像分类或目标检测不同，这里的标签是一个三维数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                    drop_last=<span class="literal">True</span>,</span><br><span class="line">                                    num_workers=d2l.get_dataloader_workers())</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([64, 3, 320, 480])</span></span><br><span class="line"><span class="string">torch.Size([64, 320, 480])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>整合</strong></p>
<p>最后，我们定义以下<code>load_data_voc</code>函数来下载并读取Pascal VOC2012语义分割数据集。 它返回训练集和测试集的数据迭代器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_voc</span>(<span class="params">batch_size, crop_size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载VOC语义分割数据集&quot;&quot;&quot;</span></span><br><span class="line">    voc_dir = d2l.download_extract(<span class="string">&#x27;voc2012&#x27;</span>, os.path.join(</span><br><span class="line">        <span class="string">&#x27;VOCdevkit&#x27;</span>, <span class="string">&#x27;VOC2012&#x27;</span>))</span><br><span class="line">    num_workers = d2l.get_dataloader_workers()</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir), batch_size,</span><br><span class="line">        shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir), batch_size,</span><br><span class="line">        drop_last=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h2><p>到目前为止，我们所见到的卷积神经网络层，例如卷积层和汇聚层，通常会<strong>减少</strong>下采样输入图像的空间维度（高和宽）然而如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将会很方便。 例如，输出像素所处的通道维可以保有输入像素在同一位置上的分类结果。</p>
<p>为了实现这一点，尤其是在空间维度被卷积神经网络层缩小后，我们可以使用另一种类型的卷积神经网络层，它可以增加上采样中间层特征图的空间维度。 本节将介绍 <strong><em>转置卷积</em></strong>（transposed convolution）用于逆转下采样导致的空间尺寸减小。</p>
<hr>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>让我们暂时忽略通道，从基本的转置卷积开始，设步幅为1且没有填充。假设我们有一个$n_h \times n_w$的输入张量和一个$k_h \times k_w$的卷积核。以步幅为1滑动卷积核窗口，每行$n_w$次，每列$n_h$次，共产生$n_h n_w$个中间结果。每个中间结果都是一个$(n_h + k_h - 1) \times (n_w + k_w - 1)$的张量，初始化为0。</p>
<p>为了计算每个中间张量，输入张量中的每个元素都要乘以卷积核，从而使所得的$k_h \times k_w$张量替换中间张量的一部分。请注意，每个中间张量被替换部分的位置与输入张量中元素的位置相对应。最后，所有中间结果相加以获得最终结果。</p>
<p>例如，图中解释了如何为$2\times 2$的输入张量计算卷积核为$2\times 2$的转置卷积。我们可以对输入矩阵<code>X</code>和卷积核矩阵<code>K</code>(实现基本的转置卷积运算)<code>trans_conv</code>。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoratrans_conv.svg" alt="../_images/trans_conv.svg"></p>
<p>我们可以对输入矩阵<code>X</code>和卷积核矩阵<code>K</code>实现基本的转置卷积运算<code>trans_conv</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trans_conv</span>(<span class="params">X, K</span>):</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] + h - <span class="number">1</span>, X.shape[<span class="number">1</span>] + w - <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i: i + h, j: j + w] += X[i, j] * K</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<p>与通过卷积核“减少”输入元素的常规卷积相比，转置卷积通过卷积核“广播”输入元素，从而产生大于输入的输出</p>
<hr>
<h3 id="填充，步幅，多通道"><a href="#填充，步幅，多通道" class="headerlink" title="填充，步幅，多通道"></a>填充，步幅，多通道</h3><p>与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。 例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure>
<p>在转置卷积中，步幅被指定为中间结果（输出），而不是输入。 使用图中相同输入和卷积核张量，将步幅从1更改为2会增加中间张量的高和权重，因此输出张量在下图中。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoratrans_conv_stride2.svg" alt="../_images/trans_conv_stride2.svg"></p>
<hr>
<h2 id="全卷积网络"><a href="#全卷积网络" class="headerlink" title="全卷积网络"></a>全卷积网络</h2><p>语义分割是对图像中的每个像素分类。 <strong><em>全卷积网络</em></strong>（fully convolutional network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换。 与我们之前在图像分类或目标检测部分介绍的卷积神经网络不同，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过<strong><em>转置卷积</em></strong>（transposed convolution）实现的。 因此，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测</p>
<hr>
<h3 id="构造模型"><a href="#构造模型" class="headerlink" title="构造模型"></a><strong>构造模型</strong></h3><p>下面我们了解一下全卷积网络模型最基本的设计。 如图所示，全卷积网络先使用卷积神经网络抽取图像特征，然后通过1×1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。 因此，模型输出与输入图像的高和宽相同，且最终输出通道包含了该空间位置像素的类别预测。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorafcn.svg" alt="../_images/fcn.svg"></p>
<p>下面，我们使用在ImageNet数据集上预训练的ResNet-18模型来提取图像特征，并将该网络记为<code>pretrained_net</code>。 ResNet-18模型的最后几层包括全局平均汇聚层和全连接层，然而全卷积网络中不需要它们。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">list(pretrained_net.children())[-3:]</span><br></pre></td></tr></table></figure>
<p>接下来，我们创建一个全卷积网络<code>net</code>。 它复制了ResNet-18中大部分的预训练层，除了最后的全局平均汇聚层和最接近输出的全连接层。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(*list(pretrained_net.children())[:-2])</span><br></pre></td></tr></table></figure>
<p>给定高度为320和宽度为480的输入，<code>net</code>的前向传播将输入的高和宽减小至原来的1/32，即10和15。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">320</span>, <span class="number">480</span>))</span><br><span class="line">net(X).shape</span><br></pre></td></tr></table></figure>
<p>接下来使用$1\times1$卷积层将输出通道数转换为Pascal VOC2012数据集的类数（21类）最后需要将特征图的高度和宽度增加32倍，从而将其变回输入图像的高和宽。</p>
<p>卷积层输出形状的计算方法：由于$(320-64+16\times2+32)/32=10$且$(480-64+16\times2+32)/32=15$，我们构造一个步幅为$32$的转置卷积层，并将卷积核的高和宽设为$64$，填充为$16$。我们可以看到如果步幅为$s$，填充为$s/2$（假设$s/2$是整数）且卷积核的高和宽为$2s$，转置卷积核会将输入的高和宽分别放大$s$倍。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_classes = 21</span><br><span class="line">net.add_module(&#x27;final_conv&#x27;, nn.Conv2d(512, num_classes, kernel_size=1))</span><br><span class="line">net.add_module(&#x27;transpose_conv&#x27;, nn.ConvTranspose2d(num_classes, num_classes,</span><br><span class="line">                                    kernel_size=64, padding=16, stride=32))</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>在图像处理中，我们有时需要将图像放大，即<strong>上采样</strong>（upsampling）。<strong>双线性插值</strong>（bilinear interpolation）是常用的上采样方法之一，它也经常用于初始化转置卷积层。</p>
<p>为了解释双线性插值，假设给定输入图像，我们想要计算上采样输出图像上的每个像素。</p>
<ol>
<li><p>将输出图像的坐标$(x,y)$映射到输入图像的坐标$(x’,y’)$上。例如，根据输入与输出的尺寸之比来映射。请注意，映射后的$x′$和$y′$是实数。</p>
</li>
<li><p>在输入图像上找到离坐标$(x’,y’)$最近的4个像素。</p>
</li>
<li><p>输出图像在坐标$(x,y)$上的像素依据输入图像上这4个像素及其与$(x’,y’)$的相对距离来计算。</p>
</li>
</ol>
<p>双线性插值的上采样可以通过转置卷积层实现，内核由以下<code>bilinear_kernel</code>函数构造。限于篇幅，我们只给出<code>bilinear_kernel</code>函数的实现，不讨论算法的原理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bilinear_kernel</span>(<span class="params">in_channels, out_channels, kernel_size</span>):</span></span><br><span class="line">    factor = (kernel_size + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> kernel_size % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        center = factor - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        center = factor - <span class="number">0.5</span></span><br><span class="line">    og = (torch.arange(kernel_size).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">          torch.arange(kernel_size).reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    filt = (<span class="number">1</span> - torch.<span class="built_in">abs</span>(og[<span class="number">0</span>] - center) / factor) * \</span><br><span class="line">           (<span class="number">1</span> - torch.<span class="built_in">abs</span>(og[<span class="number">1</span>] - center) / factor)</span><br><span class="line">    weight = torch.zeros((in_channels, out_channels,</span><br><span class="line">                          kernel_size, kernel_size))</span><br><span class="line">    weight[<span class="built_in">range</span>(in_channels), <span class="built_in">range</span>(out_channels), :, :] = filt</span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></table></figure>
<p>让我们用双线性插值的上采样实验它由转置卷积层实现。 我们构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用<code>bilinear_kernel</code>函数初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conv_trans = nn.ConvTranspose2d(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="number">4</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>,</span><br><span class="line">                                bias=<span class="literal">False</span>)</span><br><span class="line">conv_trans.weight.data.copy_(bilinear_kernel(<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br></pre></td></tr></table></figure>
<p>读取图像<code>X</code>，将上采样的结果记作<code>Y</code>。为了打印图像，我们需要调整通道维的位置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = torchvision.transforms.ToTensor()(d2l.Image.<span class="built_in">open</span>(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>))</span><br><span class="line">X = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line">Y = conv_trans(X)</span><br><span class="line">out_img = Y[<span class="number">0</span>].permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).detach()</span><br></pre></td></tr></table></figure>
<p>可以看到，转置卷积层将图像的高和宽分别放大了2倍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d2l.set_figsize()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input image shape:&#x27;</span>, img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).shape)</span><br><span class="line">d2l.plt.imshow(img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>));</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output image shape:&#x27;</span>, out_img.shape)</span><br><span class="line">d2l.plt.imshow(out_img);</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_fcn_ce3435_102_1.svg" alt="../_images/output_fcn_ce3435_102_1.svg"></p>
<p>全卷积网络用双线性插值的上采样初始化转置卷积层。对于1×1卷积层，我们使用Xavier初始化参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = bilinear_kernel(num_classes, num_classes, <span class="number">64</span>)</span><br><span class="line">net.transpose_conv.weight.data.copy_(W);</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>用语义分割读取数据集。 指定随机裁剪的输出图像的形状为320×480：高和宽都可以被32整除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, crop_size = <span class="number">32</span>, (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)</span><br></pre></td></tr></table></figure>
<p>现在我们可以训练全卷积网络了。 这里的损失函数和准确率计算与图像分类中的并没有本质上的不同，因为我们使用转置卷积层的通道来预测像素的类别，所以需要在损失计算中指定通道维。 此外，模型基于每个像素的预测类别是否正确来计算准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">inputs, targets</span>):</span></span><br><span class="line">    <span class="keyword">return</span> F.cross_entropy(inputs, targets, reduction=<span class="string">&#x27;none&#x27;</span>).mean(<span class="number">1</span>).mean(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">num_epochs, lr, wd, devices = <span class="number">5</span>, <span class="number">0.001</span>, <span class="number">1e-3</span>, d2l.try_all_gpus()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)</span><br><span class="line">d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_fcn_ce3435_138_1.svg" alt="../_images/output_fcn_ce3435_138_1.svg"></p>
<hr>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>在预测时，我们需要将输入图像在各个通道做标准化，并转成卷积神经网络所需要的四维输入格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">img</span>):</span></span><br><span class="line">    X = test_iter.dataset.normalize_image(img).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    pred = net(X.to(devices[<span class="number">0</span>])).argmax(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> pred.reshape(pred.shape[<span class="number">1</span>], pred.shape[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>为了可视化预测的类别给每个像素，我们将预测类别映射回它们在数据集中的标注颜色。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">label2image</span>(<span class="params">pred</span>):</span></span><br><span class="line">    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[<span class="number">0</span>])</span><br><span class="line">    X = pred.long()</span><br><span class="line">    <span class="keyword">return</span> colormap[X, :]</span><br></pre></td></tr></table></figure>
<p>测试数据集中的图像大小和形状各异。 由于模型使用了步幅为32的转置卷积层，因此当输入图像的高或宽无法被32整除时，转置卷积层输出的高或宽会与输入图像的尺寸有偏差。 为了解决这个问题，我们可以在图像中截取多块高和宽为32的整数倍的矩形区域，并分别对这些区域中的像素做前向传播。 请注意，这些区域的并集需要完整覆盖输入图像。 当一个像素被多个区域所覆盖时，它在不同区域前向传播中转置卷积层输出的平均值可以作为<code>softmax</code>运算的输入，从而预测类别。</p>
<p>为简单起见，我们只读取几张较大的测试图像，并从图像的左上角开始截取形状为$320×480$的区域用于预测。 对于这些测试图像，我们逐一打印它们截取的区域，再打印预测结果，最后打印标注的类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">voc_dir = d2l.download_extract(<span class="string">&#x27;voc2012&#x27;</span>, <span class="string">&#x27;VOCdevkit/VOC2012&#x27;</span>)</span><br><span class="line">test_images, test_labels = d2l.read_voc_images(voc_dir, <span class="literal">False</span>)</span><br><span class="line">n, imgs = <span class="number">4</span>, []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    crop_rect = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)</span><br><span class="line">    pred = label2image(predict(X))</span><br><span class="line">    imgs += [X.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>), pred.cpu(),</span><br><span class="line">             torchvision.transforms.functional.crop(</span><br><span class="line">                 test_labels[i], *crop_rect).permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)]</span><br><span class="line">d2l.show_images(imgs[::<span class="number">3</span>] + imgs[<span class="number">1</span>::<span class="number">3</span>] + imgs[<span class="number">2</span>::<span class="number">3</span>], <span class="number">3</span>, n, scale=<span class="number">2</span>);</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_fcn_ce3435_174_0.svg" alt="../_images/output_fcn_ce3435_174_0.svg"></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/55896.html"><img class="prev-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">10.0 注意力机制</div></div></a></div><div class="next-post pull-right"><a href="/post/49103.html"><img class="next-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ViT</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Candle</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">220</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="toc-number">1.</span> <span class="toc-text">计算机视觉</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF"><span class="toc-number">1.1.</span> <span class="toc-text">图像增广</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BF%BB%E8%BD%AC%E5%92%8C%E8%A3%81%E5%89%AA"><span class="toc-number">1.1.1.</span> <span class="toc-text">翻转和裁剪</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B9%E5%8F%98%E9%A2%9C%E8%89%B2"><span class="toc-number">1.1.2.</span> <span class="toc-text">改变颜色</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83"><span class="toc-number">1.2.</span> <span class="toc-text">微调</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.2.1.</span> <span class="toc-text">步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%83%AD%E7%8B%97%E8%AF%86%E5%88%AB"><span class="toc-number">1.2.2.</span> <span class="toc-text">热狗识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.3.</span> <span class="toc-text">微调模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%92%8C%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-number">1.3.</span> <span class="toc-text">目标检测和边界框</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%94%9A%E6%A1%86"><span class="toc-number">1.4.</span> <span class="toc-text">锚框</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%A4%9A%E4%B8%AA%E9%94%9A%E6%A1%86"><span class="toc-number">1.4.1.</span> <span class="toc-text">生成多个锚框</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%B9%B6%E6%AF%94"><span class="toc-number">1.4.2.</span> <span class="toc-text">交并比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E6%B3%A8%E9%94%9A%E6%A1%86"><span class="toc-number">1.4.3.</span> <span class="toc-text">标注锚框</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6%E9%A2%84%E6%B5%8B%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-number">1.4.4.</span> <span class="toc-text">非极大值抑制预测边界框</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">1.5.</span> <span class="toc-text">多尺度目标检测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%94%9A%E6%A1%86"><span class="toc-number">1.5.1.</span> <span class="toc-text">多尺度锚框</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.6.</span> <span class="toc-text">目标检测数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD"><span class="toc-number">1.6.1.</span> <span class="toc-text">下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96"><span class="toc-number">1.6.2.</span> <span class="toc-text">读取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BC%94%E7%A4%BA"><span class="toc-number">1.6.3.</span> <span class="toc-text">演示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%8F%91%E5%A4%9A%E6%A1%86%E6%A3%80%E6%B5%8BSSD"><span class="toc-number">1.7.</span> <span class="toc-text">单发多框检测SSD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.7.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.7.2.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E7%9B%AE%E6%A0%87"><span class="toc-number">1.7.3.</span> <span class="toc-text">预测目标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8C%BA%E5%9F%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CR-CNN"><span class="toc-number">1.8.</span> <span class="toc-text">区域卷积神经网络R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#R-CNN"><span class="toc-number">1.8.1.</span> <span class="toc-text">R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fast-R-CNN"><span class="toc-number">1.8.2.</span> <span class="toc-text">Fast  R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Faster-R-CNN"><span class="toc-number">1.8.3.</span> <span class="toc-text">Faster R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mask-R-CNN"><span class="toc-number">1.8.4.</span> <span class="toc-text">Mask R-CNN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.9.</span> <span class="toc-text">语义分割和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2"><span class="toc-number">1.9.1.</span> <span class="toc-text">图像分割和实例分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.9.2.</span> <span class="toc-text">数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.10.</span> <span class="toc-text">转置卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.10.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%EF%BC%8C%E6%AD%A5%E5%B9%85%EF%BC%8C%E5%A4%9A%E9%80%9A%E9%81%93"><span class="toc-number">1.10.2.</span> <span class="toc-text">填充，步幅，多通道</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">1.11.</span> <span class="toc-text">全卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E9%80%A0%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.11.1.</span> <span class="toc-text">构造模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.11.2.</span> <span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.11.3.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">1.11.4.</span> <span class="toc-text">预测</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recommend-post"><div class="item-headline"><i class="fas fa-dharmachakra"></i><span>相关推荐</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/46177.html" title="MAE"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="MAE"></a><div class="content"><a class="title" href="/post/46177.html" title="MAE">MAE</a><time datetime="2024-03-20" title="������ 2024-03-20">2024-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/49103.html" title="ViT"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="ViT"></a><div class="content"><a class="title" href="/post/49103.html" title="ViT">ViT</a><time datetime="2024-03-18" title="������ 2024-03-18">2024-03-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/40793.html" title="1.0 Pytorch入门"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="1.0 Pytorch入门"></a><div class="content"><a class="title" href="/post/40793.html" title="1.0 Pytorch入门">1.0 Pytorch入门</a><time datetime="2023-12-24" title="������ 2023-12-24">2023-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/62658.html" title="2.0 线性回归"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="2.0 线性回归"></a><div class="content"><a class="title" href="/post/62658.html" title="2.0 线性回归">2.0 线性回归</a><time datetime="2023-12-25" title="������ 2023-12-25">2023-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/55896.html" title="10.0 注意力机制"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="10.0 注意力机制"></a><div class="content"><a class="title" href="/post/55896.html" title="10.0 注意力机制">10.0 注意力机制</a><time datetime="2024-02-28" title="������ 2024-02-28">2024-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/21849.html" title="5.0 pytorch基础"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="5.0 pytorch基础"></a><div class="content"><a class="title" href="/post/21849.html" title="5.0 pytorch基础">5.0 pytorch基础</a><time datetime="2024-02-21" title="������ 2024-02-21">2024-02-21</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2024 By Candle</div><div class="framework-info"><span>Powered by </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a></div><div class="footer_custom_text">So Long, and Thanks for All the Fish</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-y/mathjax/3.2.0/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.removeEventListener('scroll', window.tocScrollFn)
  window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>