<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>8.0 循环神经网络 | Night's Watch</title><meta name="keywords" content="深度学习"><meta name="author" content="Candle"><meta name="copyright" content="Candle"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="循环神经网络序列模型处理序列数据需要统计工具和新的深度神经网络架构。 为了简单起见，我们以图中所示的股票价格（富时100指数）为例。  其中，用$x_t$表示价格，即在时间步（time step）$t \in \mathbb{Z}^+$时，观察到的价格$x_t$。请注意，$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x">
<meta property="og:type" content="article">
<meta property="og:title" content="8.0 循环神经网络">
<meta property="og:url" content="http://candle1220.github.io/post/53892.html">
<meta property="og:site_name" content="Night&#39;s Watch">
<meta property="og:description" content="循环神经网络序列模型处理序列数据需要统计工具和新的深度神经网络架构。 为了简单起见，我们以图中所示的股票价格（富时100指数）为例。  其中，用$x_t$表示价格，即在时间步（time step）$t \in \mathbb{Z}^+$时，观察到的价格$x_t$。请注意，$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png">
<meta property="article:published_time" content="2024-02-25T09:50:52.827Z">
<meta property="article:modified_time" content="2024-04-02T12:06:16.543Z">
<meta property="article:author" content="Candle">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png"><link rel="shortcut icon" href="/img/candle.png"><link rel="canonical" href="http://candle1220.github.io/post/53892"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#55d5fc","bgDark":"#121212","position":"top-center"},
  source: {
    jQuery: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/jquery/3.6.0/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.js',
      css: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '8.0 循环神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-02 20:06:16'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    document.addEventListener('pjax:complete', detectApple)})(window)</script><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/Swiper/8.0.6/swiper-bundle.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">264</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20240402195814166.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Night's Watch</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">8.0 循环神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-25T09:50:52.827Z" title="发表于 2024-02-25 17:50:52">2024-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-02T12:06:16.543Z" title="更新于 2024-04-02 20:06:16">2024-04-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="8.0 循环神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h2><p>处理序列数据需要统计工具和新的深度神经网络架构。</p>
<p>为了简单起见，我们以图中所示的股票价格（富时100指数）为例。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraftse100.png" alt="../_images/ftse100.png" style="zoom:67%;" /></p>
<p>其中，用$x_t$表示价格，即在<strong>时间步</strong>（time step）$t \in \mathbb{Z}^+$时，观察到的价格$x_t$。请注意，$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x_t$：</p>
<script type="math/tex; mode=display">
x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)</script><hr>
<h3 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h3><p>为了实现这个预测，交易员可以使用<strong>回归模型</strong>，仅有一个主要问题：输入数据的数量，输入$x_{t-1}, \ldots, x_1$本身因$t$而异。也就是说，输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加，因此需要一个近似方法来使这个计算变得容易处理。</p>
<p>本章后面的大部分内容将围绕着如何有效估计$P(x_t \mid x_{t-1}, \ldots, x_1)$展开。简单地说，它归结为以下两种策略。</p>
<p>第一种策略，假设在现实情况下相当长的序列$x_{t-1}, \ldots, x_1$可能是不必要的，因此我们只需要满足某个长度为$\tau$的时间跨度，即使用观测序列$x_{t-1}, \ldots, x_{t-\tau}$。当下获得的最直接的好处就是参数的数量总是不变的，至少在$t &gt; \tau$时如此，这就使我们能够训练一个上面提及的深度网络。这种模型被称为<strong>自回归模型</strong>，因为它们是对自己执行回归。</p>
<p>第二种策略，如图所示，是保留一些对过去观测的总结$h_t$，并且同时更新预测$\hat{x}_t$和总结$h_t$。这就产生了基于$\hat{x}_t = P(x_t \mid h_{t})$估计$x_t$，以及公式$h_t = g(h_{t-1}, x_{t-1})$更新的模型。由于$h_t$从未被观测到，这类模型也被称为<strong>隐变量自回归模型</strong>。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorasequence-model.svg" alt="../_images/sequence-model.svg"></p>
<p>这两种情况都有一个显而易见的问题：如何生成训练数据？</p>
<p>一个经典方法是使用历史观测来预测下一个未来观测。显然，我们并不指望时间会停滞不前。然而，一个常见的假设是虽然特定值$x_t$可能会改变，但是序列本身的动力学不会改变。这样的假设是合理的，因为新的动力学一定受新的数据影响，而我们不可能用目前所掌握的数据来预测新的动力学。统计学家称不变的动力学为<strong>静止的</strong>。因此，整个序列的估计值都将通过以下的方式获得：</p>
<script type="math/tex; mode=display">
P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1).</script><p>注意，如果我们处理的是离散的对象（如单词），而不是连续的数字，则上述的考虑仍然有效。唯一的差别是，对于离散的对象，我们需要使用分类器而不是回归模型来估计$P(x_t \mid  x_{t-1}, \ldots, x_1)$。</p>
<hr>
<h3 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h3><p>回想一下，在自回归模型的近似法中，我们使用$x_{t-1}, \ldots, x_{t-\tau}$而不是$x_{t-1}, \ldots, x_1$来估计$x_t$。只要这种是近似精确的，我们就说序列满足<strong>马尔可夫条件</strong>（Markov condition）。特别是，如果$\tau = 1$，得到一个<strong>一阶马尔可夫模型</strong>，$P(x)$由下式给出：</p>
<script type="math/tex; mode=display">
P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ 当 } P(x_1 \mid x_0) = P(x_1)</script><p>当假设$x_t$仅是离散值时，这样的模型特别棒，因为在这种情况下，使用动态规划可以沿着马尔可夫链精确地计算结果。例如，我们可以高效地计算$P(x_{t+1} \mid x_{t-1})$：</p>
<script type="math/tex; mode=display">
\begin{aligned}

P(x_{t+1} \mid x_{t-1})

&= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\

&= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\

&= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})

\end{aligned}</script><p>利用这一事实，我们只需要考虑过去观察中的一个非常短的历史：$P(x_{t+1} \mid x_t, x_{t-1}) = P(x_{t+1} \mid x_t)$。</p>
<p>隐马尔可夫模型中的动态规划超出了本节的范围，而动态规划这些计算工具已经在控制算法和强化学习算法广泛使用。</p>
<hr>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>首先，我们生成一些数据：使用正弦函数和一些可加性噪声来生成序列数据， 时间步为1,2,…,1000。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">T = <span class="number">1000</span>  <span class="comment"># 总共产生1000个点</span></span><br><span class="line">time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">x = torch.sin(<span class="number">0.01</span> * time) + torch.normal(<span class="number">0</span>, <span class="number">0.2</span>, (T,))</span><br><span class="line">d2l.plot(time, [x], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_sequence_ce248f_6_0.svg" alt="../_images/output_sequence_ce248f_6_0.svg"></p>
<p>接下来，我们将这个序列转换为模型的<strong>特征－标签</strong>（feature-label）对。基于嵌入维度$\tau$，我们将数据映射为<strong>数据对</strong>$y_t = x_t$和$\mathbf{x}_t = [x_{t-\tau}, \ldots, x_{t-1}]$。</p>
<p>这比我们提供的数据样本少了$\tau$个，因为我们没有足够的历史记录来描述前$\tau$个数据样本。一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；另一个方法是用零填充序列。</p>
<p>在这里，我们仅使用前600个“特征－标签”对进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假定x是先前定义的包含时间序列数据的张量，T是时间序列的总长度</span></span><br><span class="line">tau = <span class="number">4</span>  <span class="comment"># tau代表我们用于预测未来一个数据点的过去观测值的数量</span></span><br><span class="line">features = torch.zeros((T - tau, tau))  <span class="comment"># 初始化特征矩阵，每一行包含了tau个连续时间点的数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    <span class="comment"># 为features矩阵的每一列赋值，每一列的数据对应于时间序列的一个滞后</span></span><br><span class="line">    features[:, i] = x[i: T - tau + i]</span><br><span class="line"><span class="comment"># labels是我们要预测的目标，即在给定过去tau个观测值的情况下，下一个时间点的值</span></span><br><span class="line">labels = x[tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># 将标签数据调整为列向量</span></span><br><span class="line"></span><br><span class="line">batch_size, n_train = <span class="number">16</span>, <span class="number">600</span>  <span class="comment"># 定义批量大小和训练样本数量</span></span><br><span class="line"><span class="comment"># 使用d2l.load_array创建数据迭代器，只有前n_train个样本用于训练</span></span><br><span class="line"><span class="comment"># 这里假设d2l.load_array是一个辅助函数，用于将特征和标签组合成小批量数据</span></span><br><span class="line">train_iter = d2l.load_array((features[:n_train], labels[:n_train]),</span><br><span class="line">                            batch_size, is_train=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在这里，我们使用一个相当简单的架构训练模型： 一个拥有两个全连接层的多层感知机，ReLU激活函数和平方损失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化网络权重的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个简单的多层感知机</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span>():</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Linear(<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平方损失。注意：MSELoss计算平方误差时不带系数1/2</span></span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, loss, epochs, lr</span>):</span></span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;loss: <span class="subst">&#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">net = get_net()</span><br><span class="line">train(net, train_iter, loss, <span class="number">5</span>, <span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>由于训练损失很小，因此我们期望模型能有很好的工作效果。 让我们看看这在实践中意味着什么。 首先是检查模型预测下一个时间步的能力， 也就是<strong><em>单步预测</em></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">onestep_preds = net(features)</span><br><span class="line">d2l.plot([time, time[tau:]],</span><br><span class="line">         [x.detach().numpy(), onestep_preds.detach().numpy()], <span class="string">&#x27;time&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>], xlim=[<span class="number">1</span>, <span class="number">1000</span>],</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_sequence_ce248f_66_0.svg" alt="../_images/output_sequence_ce248f_66_0.svg"></p>
<p>正如我们所料，单步预测效果不错。 即使这些预测的时间步超过了600+4（<code>n_train + tau</code>）， 其结果看起来仍然是可信的。 然而有一个小问题：如果数据观察序列的时间步只到604， 我们需要一步一步地向前迈进：</p>
<script type="math/tex; mode=display">
\begin{split}\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\
\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \hat{x}_{605}), \\
\hat{x}_{607} = f(x_{603}, x_{604}, \hat{x}_{605}, \hat{x}_{606}),\\
\hat{x}_{608} = f(x_{604}, \hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}),\\
\hat{x}_{609} = f(\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}, \hat{x}_{608}),\\
\ldots\end{split}</script><p>通常，对于直到$x_t$的观测序列，其在时间步$t+k$处的预测输出$\hat{x}_{t+k}$称为$k$<strong>步预测</strong>由于我们的观察已经到了$x_{604}$，它的$k$步预测是$\hat{x}_{604+k}$。换句话说，我们必须使用我们自己的预测（而不是原始数据）来<strong>进行多步预测</strong>。让我们看看效果如何。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化一个全零的张量用于存储多步预测的结果，长度与时间序列相同</span></span><br><span class="line">multistep_preds = torch.zeros(T)</span><br><span class="line"><span class="comment"># 将训练数据和前tau个观测值直接赋值给multistep_preds的对应位置</span></span><br><span class="line">multistep_preds[: n_train + tau] = x[: n_train + tau]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从n_train + tau开始，逐个生成多步预测的值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_train + tau, T):</span><br><span class="line">    <span class="comment"># 对于每一步，使用前tau个观测值作为输入来预测下一个值</span></span><br><span class="line">    <span class="comment"># 这里的预测是基于模型已经预测出的值（包括训练数据和之前的预测值）</span></span><br><span class="line">    multistep_preds[i] = net(</span><br><span class="line">        multistep_preds[i - tau:i].reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 绘制原始数据、单步预测和多步预测的结果</span></span><br><span class="line">d2l.plot([time, time[tau:], time[n_train + tau:]],</span><br><span class="line">         [x.detach().numpy(), onestep_preds.detach().numpy(),</span><br><span class="line">          multistep_preds[n_train + tau:].detach().numpy()], <span class="string">&#x27;time&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;x&#x27;</span>, legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>, <span class="string">&#x27;multistep preds&#x27;</span>],</span><br><span class="line">         xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_sequence_ce248f_81_0.svg" alt="../_images/output_sequence_ce248f_81_0.svg"></p>
<p>如上面的例子所示，绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。</p>
<p>事实是由于错误的累积：假设在步骤$1$之后，我们积累了一些错误$\epsilon_1 = \bar\epsilon$。于是，步骤$2$的输入被扰动了$\epsilon_1$，结果积累的误差是依照次序的$\epsilon_2 = \bar\epsilon + c \epsilon_1$，其中$c$为某个常数，后面的预测误差依此类推。</p>
<p>因此误差可能会相当快地偏离真实的观测结果。例如，未来$24$小时的天气预报往往相当准确，但超过这一点，精度就会迅速下降。基于$k = 1, 4, 16, 64$，通过对整个序列预测的计算，让我们更仔细地看一下$k$步预测的困难。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">max_steps = <span class="number">64</span>  <span class="comment"># 定义最大预测步长</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化特征矩阵，每行包含过去观测值和预测值，用于后续的预测</span></span><br><span class="line">features = torch.zeros((T - tau - max_steps + <span class="number">1</span>, tau + max_steps))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充特征矩阵的前tau列，这些列包含过去的观测值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: i + T - tau - max_steps + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于特征矩阵的剩余列，使用模型进行预测并填充预测结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau, tau + max_steps):</span><br><span class="line">    <span class="comment"># 使用net对最近的tau个观测值或预测值进行预测，并将结果填充到特征矩阵中</span></span><br><span class="line">    features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义要展示的不同预测步长</span></span><br><span class="line">steps = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">16</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用d2l.plot进行绘图，展示不同预测步长的预测结果</span></span><br><span class="line">d2l.plot([time[tau + i - <span class="number">1</span>: T - max_steps + i] <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">         [features[:, (tau + i - <span class="number">1</span>)].detach().numpy() <span class="keyword">for</span> i <span class="keyword">in</span> steps], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>-step preds&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> steps], xlim=[<span class="number">5</span>, <span class="number">1000</span>],</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_sequence_ce248f_96_0.svg" alt="../_images/output_sequence_ce248f_96_0.svg"></p>
<p>以上例子清楚地说明了当我们试图预测更远的未来时，预测的质量是如何变化的。 虽然“4步预测”看起来仍然不错，但超过这个跨度的任何预测几乎都是无用的。</p>
<hr>
<h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><p>本节中，我们将解析文本的常见预处理步骤。 这些步骤通常包括：</p>
<ol>
<li>将文本作为字符串加载到内存中。</li>
<li>将字符串拆分为词元（如单词和字符）。</li>
<li>建立一个词表，将拆分的词元映射到数字索引。</li>
<li>将文本转换为数字索引序列，方便模型操作。</li>
</ol>
<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>首先，我们从H.G.Well的《时光机器》中加载文本。 这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀， 而现实中的文档集合可能会包含数十亿个单词。 下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。 为简单起见，我们在这里忽略了标点符号和字母大写。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span>():</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;# 文本总行数: <span class="subst">&#123;<span class="built_in">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p><strong>文本预处理</strong>：</p>
<ul>
<li>对于 <code>lines</code> 列表中的每一行文本，函数使用正则表达式 <code>re.sub(&#39;[^A-Za-z]+&#39;, &#39; &#39;, line)</code> 来替换所有非字母字符为一个空格。这样做的目的是清理文本数据，保留文本中的字母字符，并去除数字、标点符号等非字母字符。</li>
<li><code>.strip()</code> 方法用于去除字符串首尾的空白字符。</li>
<li><code>.lower()</code> 方法将所有字母字符转换为小写。这是文本处理中常用的归一化步骤，旨在减少词汇的变体，使模型更容易处理文本数据。</li>
</ul>
<hr>
<h3 id="词元化"><a href="#词元化" class="headerlink" title="词元化"></a>词元化</h3><p>下面的<code>tokenize</code>函数将文本行列表（<code>lines</code>）作为输入， 列表中的每个元素是一个文本序列（如一条文本行）。 每个文本序列又被拆分成一个词元列表，<strong><em>词元</em></strong>（token）是文本的基本单位。 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将文本行拆分为单词或字符词元。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    lines (list of str): 待处理的文本行列表。</span></span><br><span class="line"><span class="string">    token (str): 指定拆分类型，&#x27;word&#x27;表示按单词拆分，&#x27;char&#x27;表示按字符拆分。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    list of list: 如果token=&#x27;word&#x27;，返回每行文本拆分成单词形成的列表的列表；</span></span><br><span class="line"><span class="string">                  如果token=&#x27;char&#x27;，返回每行文本拆分成字符形成的列表的列表。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="comment"># 按单词拆分：对于列表中的每一行文本，使用str.split()方法按空白字符（如空格、换行符等）拆分成单词。</span></span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="comment"># 按字符拆分：对于列表中的每一行文本，将其转换为一个字符列表。</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果token参数不是&#x27;word&#x27;或&#x27;char&#x27;，则打印错误消息。</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知词元类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设`lines`是已经通过read_time_machine()函数预处理过的文本行列表。</span></span><br><span class="line">tokens = tokenize(lines)  <span class="comment"># 使用默认的按单词拆分对文本行进行词元化处理。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="comment"># 打印前11行文本的词元化结果。这里假设tokens列表包含了所有行的词元化结果。</span></span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h3><p>词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，通常也叫做<strong><em>词表</em></strong>（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中。 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为<strong><em>语料</em></strong>（corpus）。 然后根据每个唯一词元的<strong>出现频率</strong>，为其分配一个<strong>数字索引</strong>。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（<code>&lt;pad&gt;</code>）； 序列开始词元（<code>&lt;bos&gt;</code>）； 序列结束词元（<code>&lt;eos&gt;</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span>:</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []  <span class="comment"># 如果未提供tokens，则初始化为空列表</span></span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []  <span class="comment"># 如果未提供保留词元，则初始化为空列表</span></span><br><span class="line">        <span class="comment"># 统计tokens中每个词元的出现次数，并按频率降序排序</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        self._token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 初始化词表，未知词元的索引固定为0</span></span><br><span class="line">        self.idx_to_token = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        <span class="comment"># 创建词元到索引的映射字典</span></span><br><span class="line">        self.token_to_idx = &#123;token: idx <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.idx_to_token)&#125;</span><br><span class="line">        <span class="comment"># 根据出现频率和最小频率阈值添加词元到词表</span></span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> self._token_freqs:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:  <span class="comment"># 如果词元频率小于最小频率阈值，则忽略</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.token_to_idx:  <span class="comment"># 如果词元不在词表中，则添加进去</span></span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = <span class="built_in">len</span>(self.idx_to_token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 返回词表的大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, tokens</span>):</span></span><br><span class="line">        <span class="comment"># 获取单个词元或词元列表的索引</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)  <span class="comment"># 单个词元</span></span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]  <span class="comment"># 词元列表</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span>(<span class="params">self, indices</span>):</span></span><br><span class="line">        <span class="comment"># 将索引或索引列表转换回词元或词元列表</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]  <span class="comment"># 单个索引</span></span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]  <span class="comment"># 索引列表</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unk</span>(<span class="params">self</span>):</span>  <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">token_freqs</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 返回词元及其频率的列表</span></span><br><span class="line">        <span class="keyword">return</span> self._token_freqs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span>(<span class="params">tokens</span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 如果tokens是二维列表，将其展平为一维列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="comment"># 使用collections.Counter统计词元出现的次数</span></span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们首先使用《时间机器》数据集作为语料库来构建词表，然后打印前几个高频词元及其索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#[(&#x27;&lt;unk&gt;&#x27;, 0), (&#x27;the&#x27;, 1), (&#x27;i&#x27;, 2), (&#x27;and&#x27;, 3), (&#x27;of&#x27;, 4), (&#x27;a&#x27;, 5), (&#x27;to&#x27;, 6), (&#x27;was&#x27;, 7), (&#x27;in&#x27;, 8), (&#x27;that&#x27;, 9)]</span></span><br></pre></td></tr></table></figure>
<p>现在，我们可以将每一条文本行转换成一个数字索引列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;文本:&#x27;</span>, tokens[i])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;索引:&#x27;</span>, vocab[tokens[i]])</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">文本: [&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span></span><br><span class="line"><span class="string">索引: [1, 19, 50, 40, 2183, 2184, 400]</span></span><br><span class="line"><span class="string">文本: [&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;]</span></span><br><span class="line"><span class="string">索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h3><p>在使用上述函数时，我们将所有功能打包到<code>load_corpus_time_machine</code>函数中， 该函数返回<code>corpus</code>（词元索引列表）和<code>vocab</code>（时光机器语料库的词表）。 我们在这里所做的改变是：</p>
<ol>
<li>为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化；</li>
<li>时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的<code>corpus</code>仅处理为单个列表，而不是使用多词元列表构成的一个列表。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取《时间机器》数据集，进行字符级的词元化，然后将词元转换为词表中的索引。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    - max_tokens (int): 返回的最大词元数量。-1表示返回所有词元。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    - corpus (list of int): 词元索引列表，每个词元对应词表中的一个索引。</span></span><br><span class="line"><span class="string">    - vocab (Vocab): 由数据集词元构成的词表。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 读取文本数据</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    <span class="comment"># 使用字符级词元化处理文本行，得到词元列表</span></span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    <span class="comment"># 根据词元列表创建词表</span></span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 将所有文本行的词元展平成一个长列表，并转换为词表中的索引</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="comment"># 如果指定了max_tokens，就截取前max_tokens个词元</span></span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="comment"># 返回词元索引列表和词表</span></span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用load_corpus_time_machine函数加载数据并创建词表</span></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="comment"># 输出词元索引列表的长度和词表的大小</span></span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="语言模型和数据集"><a href="#语言模型和数据集" class="headerlink" title="语言模型和数据集"></a>语言模型和数据集</h2><p>我们了解了如何将文本数据映射为词元，以及将这些词元可以视为一系列离散的观测，例如单词或字符。</p>
<p>假设长度为$T$的文本序列中的词元依次为$x_1, x_2, \ldots, x_T$。于是，$x_t$（$1 \leq t \leq T$）可以被认为是文本序列在时间步$t$处的观测或标签。</p>
<p>在给定这样的文本序列时，<strong>语言模型</strong>的目标是估计序列的联合概率</p>
<script type="math/tex; mode=display">
P(x_1, x_2, \ldots, x_T).</script><p>例如，只需要一次抽取一个词元$x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$，一个理想的语言模型就能够基于模型本身生成自然文本。与猴子使用打字机完全不同的是，从这样的模型中提取的文本都将作为自然语言（例如，英语文本）来传递。只需要基于前面的对话片断中的文本，就足以生成一个有意义的对话。显然，我们离设计出这样的系统还很遥远，因为它需要“理解”文本，而不仅仅是生成语法合理的内容。</p>
<p>尽管如此，语言模型依然是非常有用的。例如，短语“to recognize speech”和“to wreck a nice beach”读音上听起来非常相似。这种相似性会导致语音识别中的歧义，但是这很容易通过语言模型来解决，因为第二句的语义很奇怪。同样，在文档摘要生成算法中，“狗咬人”比“人咬狗”出现的频率要高得多。</p>
<hr>
<h3 id="学习语言模型"><a href="#学习语言模型" class="headerlink" title="学习语言模型"></a>学习语言模型</h3><p>显而易见，我们面对的问题是如何对一个文档，甚至是一个词元序列进行建模。</p>
<p>假设在单词级别对文本数据进行词元化</p>
<p>让我们从基本概率规则开始：</p>
<script type="math/tex; mode=display">
P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1})</script><p>例如，包含了四个单词的一个文本序列的概率是：</p>
<script type="math/tex; mode=display">P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is}).</script><p>为了训练语言模型，我们需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概率本质上就是语言模型的参数。</p>
<p>训练数据集中词的概率可以根据给定词的相对词频来计算。例如，可以将估计值$\hat{P}(\text{deep})$计算为任何以单词“deep”开头的句子的概率。一种（稍稍不太精确的）方法是统计单词“deep”在数据集中的出现次数，然后将其除以整个语料库中的单词总数。这种方法效果不错，特别是对于频繁出现的单词。接下来，我们可以尝试估计</p>
<script type="math/tex; mode=display">
\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})},</script><p>其中$n(x)$和$n(x, x’)$分别是单个单词和连续单词对的出现次数。不幸的是，由于连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。</p>
<p>而对于三个或者更多的单词组合，情况会变得更糟。许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。除非我们提供某种解决方案，来将这些单词组合指定为非零计数，否则将无法在语言模型中使用它们。如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。</p>
<hr>
<h3 id="马尔科夫模型与n元语法"><a href="#马尔科夫模型与n元语法" class="headerlink" title="马尔科夫模型与n元语法"></a>马尔科夫模型与n元语法</h3><p>如果$P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$，则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。这种性质推导出了许多可以应用于序列建模的近似公式：</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned}
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\\
P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).
\end{aligned}\end{split}</script><p>通常，涉及一个、两个和三个变量的概率公式分别被称为 <em>一元语法</em>（unigram）、<em>二元语法</em>（bigram）和<em>三元语法</em>（trigram）模型。 下面，我们将学习如何去设计更好的模型。</p>
<hr>
<h3 id="自然语言统计"><a href="#自然语言统计" class="headerlink" title="自然语言统计"></a>自然语言统计</h3><p>根据时光机器数据集构建词表， 并打印前10个最常用的（频率最高的）单词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">tokens = d2l.tokenize(d2l.read_time_machine())</span><br><span class="line"><span class="comment"># 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起</span></span><br><span class="line">corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">vocab = d2l.Vocab(corpus)</span><br><span class="line">vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[(&#x27;the&#x27;, 2261),</span></span><br><span class="line"><span class="string"> (&#x27;i&#x27;, 1267),</span></span><br><span class="line"><span class="string"> (&#x27;and&#x27;, 1245),</span></span><br><span class="line"><span class="string"> (&#x27;of&#x27;, 1155),</span></span><br><span class="line"><span class="string"> (&#x27;a&#x27;, 816),</span></span><br><span class="line"><span class="string"> (&#x27;to&#x27;, 695),</span></span><br><span class="line"><span class="string"> (&#x27;was&#x27;, 552),</span></span><br><span class="line"><span class="string"> (&#x27;in&#x27;, 541),</span></span><br><span class="line"><span class="string"> (&#x27;that&#x27;, 443),</span></span><br><span class="line"><span class="string"> (&#x27;my&#x27;, 440)]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>这些词通常被称为<em>停用词</em>（stop words），因此可以被过滤掉。 尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。 此外，还有个明显的问题是词频衰减的速度相当地快。 例如，最常用单词的词频对比，第10个还不到第1个的1/5。 为了更好地理解，我们可以画出的词频图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line">d2l.plot(freqs, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>,</span><br><span class="line">         xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_language-models-and-dataset_789d14_21_0.svg" alt="../_images/output_language-models-and-dataset_789d14_21_0.svg"></p>
<p>通过此图我们可以发现：词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。 这意味着单词的频率满足<em>齐普夫定律</em>（Zipf’s law）， 即第$i$个最常用单词的频率$n_i$为：</p>
<script type="math/tex; mode=display">
n_i \propto \frac{1}{i^\alpha},</script><p>等价于</p>
<script type="math/tex; mode=display">
\log n_i = -\alpha \log i + c</script><p>其中$\alpha$是刻画分布的指数，$c$是常数。 这告诉我们想要通过计数统计和平滑来建模单词是不可行的， 因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。 </p>
<p>我们直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_language-models-and-dataset_789d14_66_0.svg" alt="../_images/output_language-models-and-dataset_789d14_66_0.svg"></p>
<p>除了一元语法词，单词序列似乎也遵循齐普夫定律， 尽管公式中的指数$\alpha$更小 （指数的大小受序列长度的影响）</p>
<hr>
<h3 id="读取长序列数据"><a href="#读取长序列数据" class="headerlink" title="读取长序列数据"></a>读取长序列数据</h3><p>假设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如$n$个时间步）的一个小批量序列。 现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。</p>
<p>首先，由于文本序列可以是任意长的， 例如整本《时光机器》（<em>The Time Machine</em>）， 于是任意长的序列可以被我们划分为具有相同时间步数的子序列。 当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。 假设网络一次只处理具有n个时间步的子序列，图中画出了从原始文本序列获得子序列的所有不同的方式， 其中n=5，并且每个时间步的词元对应于一个字符。 请注意，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoratimemachine-5gram.svg" alt="../_images/timemachine-5gram.svg"></p>
<p>因此，我们应该从图中选择哪一个呢？ 事实上，他们都一样的好。 然而，如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 因此，我们可以从随机偏移量开始划分序列， 以同时获得<strong><em>覆盖性</em></strong>（coverage）和<strong><em>随机性</em></strong>（randomness）。 下面，我们将描述如何实现<strong><em>随机采样</em></strong>（random sampling）和 <strong><em>顺序分区</em></strong>（sequential partitioning）策略。</p>
<p><strong>随机采样</strong></p>
<p>在随机采样中，<strong>每个样本都是在原始的长序列上任意捕获的子序列</strong>。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此<strong>标签是移位了一个词元的原始序列</strong>。</p>
<p>下面的代码每次可以从数据中随机生成一个小批量。 在这里，参数<code>batch_size</code>指定了每个小批量中子序列样本的数目， 参数<code>num_steps</code>是每个子序列中预定义的时间步数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    <span class="comment"># 减去1，是因为我们需要考虑标签</span></span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># 长度为num_steps的子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 在随机抽样的迭代过程中，</span></span><br><span class="line">    <span class="comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">data</span>(<span class="params">pos</span>):</span></span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_subseqs // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        <span class="comment"># 在这里，initial_indices包含子序列的随机起始索引</span></span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]</span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure>
<p>下面我们生成一个从0到34的序列。 假设批量大小为2，时间步数为5，这意味着可以生成 ⌊(35−1)/5⌋=6个“特征－标签”子序列对。 如果设置小批量大小为2，我们只能得到3个小批量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">my_seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">X:  tensor([[13, 14, 15, 16, 17],</span></span><br><span class="line"><span class="string">        [28, 29, 30, 31, 32]])</span></span><br><span class="line"><span class="string">Y: tensor([[14, 15, 16, 17, 18],</span></span><br><span class="line"><span class="string">        [29, 30, 31, 32, 33]])</span></span><br><span class="line"><span class="string">X:  tensor([[ 3,  4,  5,  6,  7],</span></span><br><span class="line"><span class="string">        [18, 19, 20, 21, 22]])</span></span><br><span class="line"><span class="string">Y: tensor([[ 4,  5,  6,  7,  8],</span></span><br><span class="line"><span class="string">        [19, 20, 21, 22, 23]])</span></span><br><span class="line"><span class="string">X:  tensor([[ 8,  9, 10, 11, 12],</span></span><br><span class="line"><span class="string">        [23, 24, 25, 26, 27]])</span></span><br><span class="line"><span class="string">Y: tensor([[ 9, 10, 11, 12, 13],</span></span><br><span class="line"><span class="string">        [24, 25, 26, 27, 28]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<p><strong>顺序分区</strong></p>
<p>在迭代过程中，除了对原始序列可以随机抽样外， 我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从一个小于num_steps的随机偏移量开始，以增加数据的多样性</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    <span class="comment"># 调整corpus的长度以适应整数倍的batch_size</span></span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    <span class="comment"># 生成输入序列Xs</span></span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    <span class="comment"># 生成目标序列Ys，每个位置向后移动一位</span></span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    <span class="comment"># 重塑Xs和Ys为batch_size行的矩阵形式，方便后续分批处理</span></span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 计算每个epoch中的批次数量</span></span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="comment"># 按照num_steps的步长遍历整个数据集，生成每个小批量的数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        <span class="comment"># 提取当前批次的输入X</span></span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        <span class="comment"># 提取与X对应的目标Y</span></span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="comment"># 以生成器的形式返回当前批次的数据</span></span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>基于相同的设置，通过顺序分区读取每个小批量的子序列的特征<code>X</code>和标签<code>Y</code>。 通过将它们打印出来可以发现： 迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line">   </span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">X:  tensor([[ 0,  1,  2,  3,  4],</span></span><br><span class="line"><span class="string">        [17, 18, 19, 20, 21]])</span></span><br><span class="line"><span class="string">Y: tensor([[ 1,  2,  3,  4,  5],</span></span><br><span class="line"><span class="string">        [18, 19, 20, 21, 22]])</span></span><br><span class="line"><span class="string">X:  tensor([[ 5,  6,  7,  8,  9],</span></span><br><span class="line"><span class="string">        [22, 23, 24, 25, 26]])</span></span><br><span class="line"><span class="string">Y: tensor([[ 6,  7,  8,  9, 10],</span></span><br><span class="line"><span class="string">        [23, 24, 25, 26, 27]])</span></span><br><span class="line"><span class="string">X:  tensor([[10, 11, 12, 13, 14],</span></span><br><span class="line"><span class="string">        [27, 28, 29, 30, 31]])</span></span><br><span class="line"><span class="string">Y: tensor([[11, 12, 13, 14, 15],</span></span><br><span class="line"><span class="string">        [28, 29, 30, 31, 32]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>现在，我们将上面的两个采样函数包装到一个类中， 以便稍后可以将其用作数据迭代器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeqDataLoader</span>:</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span></span><br><span class="line">        <span class="comment"># 根据use_random_iter参数选择数据迭代器的类型</span></span><br><span class="line">        <span class="comment"># 如果use_random_iter为True, 使用随机抽样方法</span></span><br><span class="line">        <span class="comment"># 否则，使用顺序分区方法</span></span><br><span class="line">        <span class="keyword">if</span> use_random_iter:</span><br><span class="line">            self.data_iter_fn = d2l.seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加载语料库和词汇表，max_tokens限制了加载的最大词元数量</span></span><br><span class="line">        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 设置批量大小和序列长度</span></span><br><span class="line">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 当迭代这个类的实例时，使用选择的数据迭代函数</span></span><br><span class="line">        <span class="comment"># 生成并返回小批量数据</span></span><br><span class="line">        <span class="keyword">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后，我们定义了一个函数<code>load_data_time_machine</code>， 它同时返回数据迭代器和词表， 因此可以与其他带有<code>load_data</code>前缀的函数 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_time_machine</span>(<span class="params">batch_size, num_steps,  <span class="comment">#@save</span></span></span></span><br><span class="line"><span class="params"><span class="function">                           use_random_iter=<span class="literal">False</span>, max_tokens=<span class="number">10000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    data_iter = SeqDataLoader(</span><br><span class="line">        batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="循环神经网络-1"><a href="#循环神经网络-1" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>之前我们介绍了$n$元语法模型，其中单词$x_t$在时间步$t$的条件概率仅取决于前面$n-1$个单词。对于时间步$t-(n-1)$之前的单词，如果我们想将其可能产生的影响合并到$x_t$上，需要增加$n$，然而模型参数的数量也会随之呈指数增长，因为词表$\mathcal{V}$需要存储$|\mathcal{V}|^n$个数字，因此与其将$P(x_t \mid x_{t-1}, \ldots, x_{t-n+1})$模型化，</p>
<p>不如使用隐变量模型：</p>
<script type="math/tex; mode=display">
P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1})</script><p>其中$h_{t-1}$是<strong>隐状态</strong>（hidden state），也称为<strong>隐藏变量</strong>（hidden variable），它存储了到时间步$t-1$的序列信息。通常，我们可以基于当前输入$x_{t}$和先前隐状态$h_{t-1}$来计算时间步$t$处的任何时间的隐状态：</p>
<script type="math/tex; mode=display">
h_t = f(x_{t}, h_{t-1})</script><p>对于公式中的函数$f$，隐变量模型不是近似值。毕竟$h_t$是可以仅仅存储到目前为止观察到的所有数据，然而这样的操作可能会使计算和存储的代价都变得昂贵。</p>
<p>回想一下，我们在多层感知机中讨论过的具有隐藏单元的隐藏层。值得注意的是，隐藏层和隐状态指的是两个截然不同的概念。如上所述，隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的<strong>输入</strong>，并且这些状态只能通过先前时间步的数据来计算。</p>
<hr>
<h3 id="无隐状态的神经网络"><a href="#无隐状态的神经网络" class="headerlink" title="无隐状态的神经网络"></a>无隐状态的神经网络</h3><p>让我们来看一看只有单隐藏层的多层感知机。设隐藏层的激活函数为$\phi$，给定一个小批量样本$\mathbf{X} \in \mathbb{R}^{n \times d}$，其中批量大小为$n$，输入维度为$d$，则隐藏层的输出$\mathbf{H} \in \mathbb{R}^{n \times h}$通过下式计算：</p>
<script type="math/tex; mode=display">
\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h)</script><p>在公式中，我们拥有的隐藏层权重参数为$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$，偏置参数为$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及隐藏单元的数目为$h$。因此求和时可以应用广播机制接下来，将隐藏变量$\mathbf{H}$用作输出层的输入。输出层由下式给出：</p>
<script type="math/tex; mode=display">
\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q</script><p>其中，$\mathbf{O} \in \mathbb{R}^{n \times q}$是输出变量，</p>
<p>$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$是权重参数，</p>
<p>$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的偏置参数。</p>
<p>如果是分类问题，我们可以用$\text{softmax}(\mathbf{O})$来计算输出类别的概率分布。这完全类似于之前在之前股票预测问题中解决的回归问题，因此我们省略了细节。无须多言，只要可以随机选择“特征-标签”对，并且通过自动微分和随机梯度下降能够学习网络参数就可以了。</p>
<hr>
<h3 id="有隐状态的神经网络"><a href="#有隐状态的神经网络" class="headerlink" title="有隐状态的神经网络"></a>有隐状态的神经网络</h3><p>有了隐状态后，情况就完全不同了。假设我们在时间步$t$有小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$。换言之，对于$n$个序列样本的小批量，$\mathbf{X}_t$的每一行对应于来自该序列的时间步$t$处的一个样本。接下来，用$\mathbf{H}_t  \in \mathbb{R}^{n \times h}$表示时间步$t$的隐藏变量。</p>
<p>与多层感知机不同的是，我们在这里保存了前一个时间步的隐藏变量$\mathbf{H}_{t-1}$，并引入了一个新的权重参数$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$，来描述如何在当前时间步中使用前一个时间步的隐藏变量。具体地说，当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：</p>
<script type="math/tex; mode=display">
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)</script><p>与原公式相比，有隐状态多添加了一项$\mathbf{H}_{t-1} \mathbf{W}_{hh}$，从而实例化了隐状态，从相邻时间步的隐藏变量$\mathbf{H}_t$和$\mathbf{H}_{t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为<strong>隐状态</strong></p>
<p>由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义<strong>相同</strong>，因此公式中的计算是<strong>循环的</strong>。于是基于循环计算的隐状态神经网络被命名为<strong>循环神经网络</strong>（recurrent neural network）。在循环神经网络中执行计算的层称为<strong>循环层</strong></p>
<hr>
<p>有许多不同的方法可以构建循环神经网络，公式中定义的隐状态的循环神经网络是非常常见的一种。对于时间步$t$，输出层的输出类似于多层感知机中的计算：</p>
<script type="math/tex; mode=display">
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q</script><p>循环神经网络的参数包括隐藏层的权重$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$和偏置$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$，以及输出层的权重$\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$和偏置$\mathbf{b}_q \in \mathbb{R}^{1 \times q}$。值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。</p>
<p>循环神经网络（RNN）的参数包括以下几部分，这些参数共同定义了RNN的行为：</p>
<ol>
<li><p><strong>隐藏层的权重</strong> ($W_{xh} \in \mathbb{R}^{d \times h}$)：这是输入层到隐藏层的权重矩阵。其中(d)代表输入特征的维度，(h)代表隐藏层单元的数量。该矩阵负责将输入转换到隐藏状态空间。</p>
</li>
<li><p><strong>隐藏层自回归的权重</strong> ($W_{hh} \in \mathbb{R}^{h \times h}$)：这是从前一时间步的隐藏层到当前时间步的隐藏层的权重矩阵。它使网络能够学习时间序列数据中的序列依赖性。</p>
</li>
<li><p><strong>隐藏层的偏置</strong> ($b_h \in \mathbb{R}^{1 \times h}$)：这是隐藏层的偏置向量，用于添加到隐藏状态的线性转换中。</p>
</li>
<li><p><strong>输出层的权重</strong> ($W_{hq} \in \mathbb{R}^{h \times q}$)：这是从隐藏层到输出层的权重矩阵。(q)代表输出特征的维度。该矩阵负责将隐藏状态转换为最终的输出。</p>
</li>
<li><p><strong>输出层的偏置</strong> ($b_q \in \mathbb{R}^{1 \times q}$)：这是输出层的偏置向量，用于添加到输出层的线性转换中。</p>
</li>
</ol>
<p>RNN的特点是在所有时间步中重复使用这些参数。这种参数共享机制意味着，即使序列数据的长度变化，RNN模型的参数数量保持不变，从而使RNN能够处理不同长度的序列数据。这种设计不仅减少了模型的参数量，还帮助模型捕捉长期依赖，是RNN处理时间序列数据的关键特性。</p>
<hr>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorarnn.svg" alt="../_images/rnn.svg"></p>
<p>展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步$t$，隐状态的计算可以被视为：</p>
<ul>
<li>拼接当前时间步$t$的输入$\mathbf{X}_t$和前一时间步$t-1$的隐状态$\mathbf{H}_{t-1}$</li>
<li>将拼接的结果送入带有激活函数$\phi$的全连接层。全连接层的输出是当前时间步$t$的隐状态$\mathbf{H}_t$。</li>
</ul>
<p>在本例中，模型参数是$\mathbf{W}_{xh}$和$\mathbf{W}_{hh}$的拼接，以及$\mathbf{b}_h$的偏置，所有这些参数都来自公式，当前时间步$t$的隐状态$\mathbf{H}_t$将参与计算下一时间步$t+1$的隐状态$\mathbf{H}_{t+1}$。而且$\mathbf{H}_t$还将送入全连接输出层，用于计算当前时间步$t$的输出$\mathbf{O}_t$。</p>
<hr>
<h3 id="基于循环神经网络的字符级语言模型"><a href="#基于循环神经网络的字符级语言模型" class="headerlink" title="基于循环神经网络的字符级语言模型"></a>基于循环神经网络的字符级语言模型</h3><p>回想一下之前的语言模型， 我们的目标是根据过去的和当前的词元预测下一个词元， 因此我们将原始序列移位一个词元作为标签。接下来，我们看一下如何使用循环神经网络来构建语言模型。 设小批量大小为1，批量中的文本序列为“machine”。 为了简化后续部分的训练，我们考虑使用 <strong><em>字符级语言模型</em></strong>， 将文本词元化为字符而不是单词。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typorarnn-train.svg" alt="../_images/rnn-train.svg"></p>
<p>图中演示了 如何通过基于字符级语言建模的循环神经网络， 使用当前的和先前的字符预测下一个字符。</p>
<p>在训练过程中，我们对每个时间步的输出层的输出进行$softmax$操作， 然后利用<strong>交叉熵损失</strong>计算模型输出和标签之间的误差。 由于隐藏层中隐状态的循环计算，图中的第3个时间步的输出$O_3$ 由文本序列“m”“a”和“c”确定。 由于训练数据中这个文本序列的下一个字符是“h”， 因此第3个时间步的损失将取决于下一个字符的概率分布， 而下一个字符是基于特征序列“m”“a”“c”和这个时间步的标签“h”生成的。</p>
<p>在实践中，我们使用的批量大小为$n&gt;1$，每个词元都由一个$d$维向量表示。因此，在时间步$t$输入$\mathbf X_t$将是一个$n\times d$矩阵，这与我们在上一小节中的讨论相同。</p>
<hr>
<h3 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h3><p>最后，让我们讨论如何度量语言模型的质量， 这将在后续部分中用于评估基于循环神经网络的模型。 一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。</p>
<p>我们可以通过计算序列的似然概率来度量模型的质量。 然而这是一个难以理解、难以比较的数字。 毕竟，较短的序列比较长的序列更有可能出现， 因此评估模型产生托尔斯泰的巨著《战争与和平》的可能性 不可避免地会比产生圣埃克苏佩里的中篇小说《小王子》可能性要小得多。 而缺少的可能性值相当于平均数。</p>
<p>如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。 一个更好的语言模型应该能让我们更准确地预测下一个词元。 因此，它应该允许我们在压缩序列时花费更少的比特。 所以我们可以通过一个序列中所有的$n$个词元的<strong>交叉熵损失</strong>的平均值来衡量：</p>
<script type="math/tex; mode=display">
\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)</script><p>其中$P$由语言模型给出，$x_t$是在时间步$t$从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了可比性。由于历史原因，自然语言处理的科学家更喜欢使用一个叫做<strong>困惑度</strong>（perplexity）的量。简而言之，是交叉熵损失的平均值的指数</p>
<script type="math/tex; mode=display">
\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)</script><p>困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。 我们看看一些案例。</p>
<ul>
<li>在最好的情况下，模型总是完美地估计标签词元的概率为1。 在这种情况下，模型的困惑度为1。</li>
<li>在最坏的情况下，模型总是预测标签词元的概率为0。 在这种情况下，困惑度是正无穷大。</li>
<li>在基线上，该模型的预测是词表的所有可用词元上的均匀分布。 在这种情况下，困惑度等于词表中唯一词元的数量。 事实上，如果我们在没有任何压缩的情况下存储序列， 这将是我们能做的最好的编码方式。 因此，这种方式提供了一个重要的上限， 而任何实际模型都必须超越这个上限。</li>
</ul>
<hr>
<h2 id="从零实现"><a href="#从零实现" class="headerlink" title="从零实现"></a>从零实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>
<h3 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h3><p>回想一下，在<code>train_iter</code>中，每个词元都表示为一个数字索引， 将这些索引直接输入神经网络可能会使学习变得困难。 我们通常将每个词元表示为更具表现力的特征向量。 最简单的表示称为<strong><em>独热编码</em></strong></p>
<p>我们每次采样的小批量数据形状是二维张量： （批量大小，时间步数）。 <code>one_hot</code>函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（<code>len(vocab)</code>）。 我们经常转换输入的维度，以便获得形状为 （时间步数，批量大小，词表大小）的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。</p>
<hr>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>接下来，我们初始化循环神经网络模型的模型参数。 隐藏单元数<code>num_hiddens</code>是一个可调的超参数。 当训练语言模型时，输入和输出来自相同的词表。 因此，它们具有相同的维度，即词表的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="comment"># vocab_size是词汇表的大小，即输入和输出的维度大小</span></span><br><span class="line">    <span class="comment"># num_hiddens是隐藏层的维度大小</span></span><br><span class="line">    <span class="comment"># device是模型运行的设备（CPU或GPU）</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">shape</span>):</span></span><br><span class="line">        <span class="comment"># 生成正态分布的随机数，用于初始化权重，乘以0.01使得权重较小</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))  <span class="comment"># 输入到隐藏层的权重</span></span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens)) <span class="comment"># 隐藏层到隐藏层的权重</span></span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device) <span class="comment"># 隐藏层的偏置，初始化为0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs)) <span class="comment"># 隐藏层到输出层的权重</span></span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device) <span class="comment"># 输出层的偏置，初始化为0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有参数合并到一个列表中，并设置它们为需要梯度的，以便在训练过程中对它们进行更新</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>) <span class="comment"># 开启梯度</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>为了定义循环神经网络模型， 我们首先需要一个<code>init_rnn_state</code>函数在初始化时返回隐状态。 这个函数的返回是一个张量，张量全用0填充， 形状为（批量大小，隐藏单元数）。 在后面的章节中我们将会遇到隐状态包含多个变量的情况， 而使用元组可以更容易地处理些。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>
<p>下面的<code>rnn</code>函数定义了如何在一个时间步内计算隐状态和输出。 循环神经网络模型通过<code>inputs</code>最外层的维度实现循环， 以便逐时间步更新小批量数据的隐状态<code>H</code>。 这里使用$tanh$作为激活函数，当元素在实数上满足均匀分布时，$tanh$函数的平均值为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn</span>(<span class="params">inputs, state, params</span>):</span></span><br><span class="line">    <span class="comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X的形状：(批量大小，词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure>
<p>定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数， 并存储从零开始实现的循环神经网络模型的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModelScratch</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device,</span></span></span><br><span class="line"><span class="params"><span class="function">                 get_params, init_state, forward_fn</span>):</span></span><br><span class="line">        <span class="comment"># vocab_size: 词汇表的大小，即输入和输出的维度。</span></span><br><span class="line">        <span class="comment"># num_hiddens: 隐藏层的维度大小。</span></span><br><span class="line">        <span class="comment"># device: 模型运行的设备（CPU或GPU）。</span></span><br><span class="line">        <span class="comment"># get_params: 函数，用于初始化模型参数。</span></span><br><span class="line">        <span class="comment"># init_state: 函数，用于初始化隐藏状态。</span></span><br><span class="line">        <span class="comment"># forward_fn: 函数，定义了模型的前向传播。</span></span><br><span class="line">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        self.params = get_params(vocab_size, num_hiddens, device)  <span class="comment"># 初始化模型参数</span></span><br><span class="line">        self.init_state, self.forward_fn = init_state, forward_fn  <span class="comment"># 初始化状态和前向函数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># 将输入X通过one_hot编码，并转换数据类型为float32。</span></span><br><span class="line">        <span class="comment"># X.T使得批量大小维度和时间步维度交换，以满足one_hot函数的输入需求。</span></span><br><span class="line">        X = F.one_hot(X.T, self.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="comment"># 调用forward_fn函数执行模型的前向传播。</span></span><br><span class="line">        <span class="comment"># 返回计算得到的输出和新的隐藏状态。</span></span><br><span class="line">        <span class="keyword">return</span> self.forward_fn(X, state, self.params)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span>(<span class="params">self, batch_size, device</span>):</span></span><br><span class="line">        <span class="comment"># 调用init_state函数生成初始的隐藏状态。</span></span><br><span class="line">        <span class="comment"># batch_size: 指定批量的大小。</span></span><br><span class="line">        <span class="comment"># 返回初始化的隐藏状态。</span></span><br><span class="line">        <span class="keyword">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>让我们检查输出是否具有正确的形状。 例如，隐状态的维数是否保持不变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">512</span></span><br><span class="line">net = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params,</span><br><span class="line">                      init_rnn_state, rnn)</span><br><span class="line">state = net.begin_state(X.shape[<span class="number">0</span>], d2l.try_gpu())</span><br><span class="line">Y, new_state = net(X.to(d2l.try_gpu()), state)</span><br><span class="line">Y.shape, <span class="built_in">len</span>(new_state), new_state[<span class="number">0</span>].shape</span><br><span class="line"><span class="comment">#(torch.Size([10, 28]), 1, torch.Size([2, 512]))</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们可以看到输出形状是（时间步数×批量大小，词表大小）， 而隐状态形状保持不变，即（批量大小，隐藏单元数）。</p>
<hr>
<h3 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h3><p>让我们首先定义预测函数来生成<code>prefix</code>之后的新字符， 其中的<code>prefix</code>是一个用户提供的包含多个字符的字符串。 在循环遍历<code>prefix</code>中的开始字符时， 我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。 这被称为<em>预热</em>（warm-up）期， 因为在此期间模型会自我更新（例如，更新隐状态）， 但不会进行预测。 预热期结束后，隐状态的值通常比刚开始的初始值更适合预测， 从而预测字符并输出它们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_ch8</span>(<span class="params">prefix, num_preds, net, vocab, device</span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化模型的隐藏状态，batch_size设为1，因为一次只生成一个字符。</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device)</span><br><span class="line">    <span class="comment"># outputs列表初始化为前缀的第一个字符的索引。</span></span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]]</span><br><span class="line">    <span class="comment"># 定义一个lambda函数，用于获取模型的当前输入。</span></span><br><span class="line">    <span class="comment"># 输入是outputs列表中的最后一个元素（即最新生成的字符索引）。</span></span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预热期：使用前缀中的字符（除了第一个）来更新模型的隐藏状态。</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:</span><br><span class="line">        <span class="comment"># 更新隐藏状态，但不收集这些字符的输出，因为这一阶段只是为了根据已知的前缀来设置隐藏状态。</span></span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        <span class="comment"># 将前缀中当前字符的索引添加到输出列表中。</span></span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测阶段：根据当前的隐藏状态和最后一个字符来生成新的字符。</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_preds):</span><br><span class="line">        <span class="comment"># 生成一个字符及更新隐藏状态。</span></span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        <span class="comment"># 选择概率最高的字符索引作为输出，并添加到outputs列表中。</span></span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将生成的字符索引转换回字符，并拼接成字符串返回。</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在我们可以测试<code>predict_ch8</code>函数。 我们将前缀指定为<code>time traveller</code>， 并基于这个前缀生成10个后续字符。 鉴于我们还没有训练网络，它会生成荒谬的预测结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_ch8(<span class="string">&#x27;time traveller &#x27;</span>, <span class="number">10</span>, net, vocab, d2l.try_gpu())</span><br><span class="line"><span class="comment">#&#x27;time traveller aaaaaaaaaa&#x27;</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="梯度剪裁"><a href="#梯度剪裁" class="headerlink" title="梯度剪裁"></a>梯度剪裁</h3><p>这个<code>grad_clipping</code>函数的目的是在训练神经网络时对梯度进行裁剪，以防止梯度爆炸问题，这是在训练循环神经网络（RNNs）时经常需要采取的一种措施。函数的工作原理如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_clipping</span>(<span class="params">net, theta</span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 如果net是一个nn.Module的实例，即PyTorch的模型，</span></span><br><span class="line">    <span class="comment"># 则从模型参数中选出需要梯度的参数。</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 否则，假设net是一个自定义的模型，其参数存储在net.params中。</span></span><br><span class="line">        params = net.params</span><br><span class="line">    <span class="comment"># 计算所有参数的梯度的范数。</span></span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>((p.grad ** <span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="comment"># 如果范数超过了给定的阈值theta，</span></span><br><span class="line">    <span class="comment"># 则对所有参数的梯度进行等比缩放，使得范数降至theta。</span></span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>梯度裁剪通过限制梯度的范数，避免了在训练过程中由于梯度过大导致的参数更新过猛，从而引起的模型不稳定或梯度爆炸问题。</p>
<hr>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>在训练模型之前，让我们定义一个函数在一个迭代周期内训练模型。 它与softmax训练模型的方式有三个不同之处。</p>
<ol>
<li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li>
<li>我们在更新模型参数之前裁剪梯度。 这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li>
<li>我们用<strong>困惑度</strong>来评价模型。 这样的度量确保了不同长度的序列具有可比性。</li>
</ol>
<p><strong>顺序分区</strong>：我们只在每个迭代周期的开始位置初始化隐状态。 由于下一个小批量数据中的第$i$个子序列样本 与当前第$i$个子序列样本相邻， 因此当前小批量数据最后一个样本的隐状态， 将用于初始化下一个小批量数据第一个样本的隐状态。 这样，存储在隐状态中的序列的历史信息 可以在一个迭代周期内流经相邻的子序列。 然而，在任何一点隐状态的计算， 都依赖于同一迭代周期中前面所有的小批量数据， 这使得梯度计算变得复杂。 为了降低计算量，在处理任何一个小批量数据之前， 我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。</p>
<p><strong>随机抽样</strong>：因为每个样本都是在一个随机位置抽样的， 因此需要为每个迭代周期重新初始化隐状态。 与softmax中的 <code>train_epoch_ch3</code>函数相同， <code>updater</code>是更新模型参数的常用函数。 它既可以是从头开始实现的<code>d2l.sgd</code>函数， 也可以是深度学习框架中内置的优化函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, d2l.Timer()  <span class="comment"># 初始化状态和计时器</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 初始化累加器，用于累计训练损失和处理的词元数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:  <span class="comment"># 遍历数据集</span></span><br><span class="line">        <span class="comment"># 根据use_random_iter决定是否在每次迭代时重新初始化状态</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device) </span><br><span class="line">            <span class="comment"># 初始化或重置状态</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果状态不需要重置，且是可分离的，对其进行分离操作</span></span><br><span class="line">            <span class="comment"># 这样做是为了截断反向传播时的梯度流</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)  <span class="comment"># 将标签Y转换为适合损失函数的形式</span></span><br><span class="line">        X, y = X.to(device), y.to(device)  <span class="comment"># 将数据和标签移至指定的设备</span></span><br><span class="line"></span><br><span class="line">        y_hat, state = net(X, state)  <span class="comment"># 前向传播</span></span><br><span class="line">        l = loss(y_hat, y.long()).mean()  <span class="comment"># 计算损失并取均值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据updater的类型进行梯度清零、反向传播、梯度裁剪和参数更新</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">            l.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)  <span class="comment"># 梯度裁剪</span></span><br><span class="line">            updater.step()  <span class="comment"># 参数更新</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)  <span class="comment"># 梯度裁剪</span></span><br><span class="line">            updater(batch_size=<span class="number">1</span>)  <span class="comment"># 手动更新参数</span></span><br><span class="line"></span><br><span class="line">        metric.add(l * y.numel(), y.numel())  <span class="comment"># 更新累加器的统计</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算本迭代周期的困惑度和吞吐量（每秒处理的词元数量）</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在训练循环神经网络（RNN）时，<code>detach_()</code>方法的调用是为了截断反向传播时的梯度流。这是因为在RNN中，通过时间反向传播（Backpropagation Through Time, BPTT）是用来计算梯度的。BPTT会追溯每一步的计算，这可能会导致两个主要问题：</p>
<ol>
<li><strong>梯度消失或梯度爆炸</strong>：当梯度通过很长的序列传播时，它们可能会变得非常小（消失）或非常大（爆炸），这使得网络难以学习。</li>
<li><strong>计算成本高</strong>：随着序列长度的增加，存储过去所有状态的梯度所需的计算资源（如内存）会显著增加。</li>
</ol>
<p>为了缓解这些问题，<code>detach_()</code>方法被用于将状态变量（<code>state</code>）从前一时间步的计算历史中分离出来。这样做的结果是，在当前时间步计算梯度时，梯度不会被反向传播到这个状态变量之前的任何时间步。换句话说，这限制了梯度反向传播的深度，从而减少了梯度消失或爆炸的风险，并减少了每次迭代的计算负担。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device, use_random_iter=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 使用交叉熵损失函数</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 初始化动画制作工具，用于可视化训练进度</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>,</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 根据网络类型初始化优化器</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)  <span class="comment"># 对于PyTorch模型</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size) </span><br><span class="line">        <span class="comment"># 对于从零开始的实现</span></span><br><span class="line">    <span class="comment"># 定义预测函数，用于在训练过程中生成文本</span></span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 训练一个迭代周期，并返回困惑度和处理速度</span></span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        <span class="comment"># 每10个周期打印一次生成的文本并更新动画</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))  <span class="comment"># 生成文本</span></span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])  <span class="comment"># 更新动画数据</span></span><br><span class="line">    <span class="comment"># 打印最终的困惑度和速度，以及最后的文本生成示例</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;困惑度 <span class="subst">&#123;ppl:<span class="number">.1</span>f&#125;</span>, <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span> 词元/秒 <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;traveller&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在，我们训练循环神经网络模型。 因为我们在数据集中只使用了10000个词元， 所以模型需要更多的迭代周期来更好地收敛。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">困惑度 1.0, 67212.6 词元/秒 cuda:0</span></span><br><span class="line"><span class="string">time traveller for so it will be convenient to speak of himwas e</span></span><br><span class="line"><span class="string">travelleryou can show black is white by argument said filby</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_rnn-scratch_546c4d_202_1.svg" alt="../_images/output_rnn-scratch_546c4d_202_1.svg"></p>
<p>最后，让我们检查一下使用随机抽样方法的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params,</span><br><span class="line">                      init_rnn_state, rnn)</span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),</span><br><span class="line">          use_random_iter=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">困惑度 1.5, 65222.3 词元/秒 cuda:0</span></span><br><span class="line"><span class="string">time traveller held in his hand was a glitteringmetallic framewo</span></span><br><span class="line"><span class="string">traveller but now you begin to seethe object of my investig</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_rnn-scratch_546c4d_217_1.svg" alt="../_images/output_rnn-scratch_546c4d_217_1.svg"></p>
<hr>
<h3 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置批量大小和时间步长</span></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line"><span class="comment"># 加载时间机器数据集</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置隐藏单元的数量</span></span><br><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line"><span class="comment"># 初始化一个RNN层</span></span><br><span class="line">rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hiddens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐状态</span></span><br><span class="line">state = torch.zeros((<span class="number">1</span>, batch_size, num_hiddens))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个随机的输入数据</span></span><br><span class="line">X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line"><span class="comment"># 通过RNN层处理输入，获得输出和新的隐状态</span></span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义RNN模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 根据RNN是否双向来设置num_directions，并定义线性层</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, state</span>):</span></span><br><span class="line">        <span class="comment"># 将输入转换为one-hot编码，并传递给RNN层</span></span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size).to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 将RNN层的输出通过全连接层，改变形状为(时间步数*批量大小, 词表大小)</span></span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="comment"># 根据RNN类型返回相应的初始隐状态</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># 对于非LSTM的RNN，返回一个全0的张量</span></span><br><span class="line">            <span class="keyword">return</span> torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                 batch_size, self.num_hiddens), device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 对于LSTM，返回两个全0的张量组成的元组</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                 batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((self.num_directions * self.rnn.num_layers,</span><br><span class="line">                                 batch_size, self.num_hiddens), device=device))</span><br><span class="line">            </span><br><span class="line"><span class="comment"># 设置设备，将模型移到相应的设备上</span></span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练的轮数和学习率</span></span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line"><span class="comment"># 调用d2l库的train_ch8函数进行模型训练</span></span><br><span class="line">d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/54714.html"><img class="prev-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">7.0 现代卷积神经网络</div></div></a></div><div class="next-post pull-right"><a href="/post/6823.html"><img class="next-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">9.0 现代循环神经网络</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Candle</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">264</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">序列模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">自回归模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.2.</span> <span class="toc-text">马尔科夫模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.1.3.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">1.1.4.</span> <span class="toc-text">预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">文本预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.1.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%85%83%E5%8C%96"><span class="toc-number">1.2.2.</span> <span class="toc-text">词元化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8"><span class="toc-number">1.2.3.</span> <span class="toc-text">词表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88"><span class="toc-number">1.2.4.</span> <span class="toc-text">整合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.3.</span> <span class="toc-text">语言模型和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text">学习语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8En%E5%85%83%E8%AF%AD%E6%B3%95"><span class="toc-number">1.3.2.</span> <span class="toc-text">马尔科夫模型与n元语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%BB%9F%E8%AE%A1"><span class="toc-number">1.3.3.</span> <span class="toc-text">自然语言统计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.4.</span> <span class="toc-text">读取长序列数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1"><span class="toc-number">1.4.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E9%9A%90%E7%8A%B6%E6%80%81%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.1.</span> <span class="toc-text">无隐状态的神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%89%E9%9A%90%E7%8A%B6%E6%80%81%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.2.</span> <span class="toc-text">有隐状态的神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%97%E7%AC%A6%E7%BA%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.3.</span> <span class="toc-text">基于循环神经网络的字符级语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%B0%E6%83%91%E5%BA%A6"><span class="toc-number">1.4.4.</span> <span class="toc-text">困惑度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.5.</span> <span class="toc-text">从零实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81"><span class="toc-number">1.5.1.</span> <span class="toc-text">独热编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">1.5.2.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.5.3.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B-1"><span class="toc-number">1.5.4.</span> <span class="toc-text">预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E5%89%AA%E8%A3%81"><span class="toc-number">1.5.5.</span> <span class="toc-text">梯度剪裁</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">1.5.6.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.5.7.</span> <span class="toc-text">简洁实现</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recommend-post"><div class="item-headline"><i class="fas fa-dharmachakra"></i><span>相关推荐</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/46177.html" title="MAE"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="MAE"></a><div class="content"><a class="title" href="/post/46177.html" title="MAE">MAE</a><time datetime="2024-03-20" title="������ 2024-03-20">2024-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/49103.html" title="ViT"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="ViT"></a><div class="content"><a class="title" href="/post/49103.html" title="ViT">ViT</a><time datetime="2024-03-18" title="������ 2024-03-18">2024-03-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/62658.html" title="2.0 线性回归"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="2.0 线性回归"></a><div class="content"><a class="title" href="/post/62658.html" title="2.0 线性回归">2.0 线性回归</a><time datetime="2023-12-25" title="������ 2023-12-25">2023-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/40793.html" title="1.0 Pytorch入门"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="1.0 Pytorch入门"></a><div class="content"><a class="title" href="/post/40793.html" title="1.0 Pytorch入门">1.0 Pytorch入门</a><time datetime="2023-12-24" title="������ 2023-12-24">2023-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/58837.html" title="4.0 多层感知机"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="4.0 多层感知机"></a><div class="content"><a class="title" href="/post/58837.html" title="4.0 多层感知机">4.0 多层感知机</a><time datetime="2024-02-21" title="������ 2024-02-21">2024-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/56989.html" title="11.0 计算机视觉"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="11.0 计算机视觉"></a><div class="content"><a class="title" href="/post/56989.html" title="11.0 计算机视觉">11.0 计算机视觉</a><time datetime="2024-03-01" title="������ 2024-03-01">2024-03-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2024 By Candle</div><div class="framework-info"><span>Powered by </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a></div><div class="footer_custom_text">So Long, and Thanks for All the Fish</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-y/mathjax/3.2.0/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.removeEventListener('scroll', window.tocScrollFn)
  window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>