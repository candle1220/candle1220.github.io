<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>10.0 注意力机制 | Night's Watch</title><meta name="keywords" content="深度学习"><meta name="author" content="Candle"><meta name="copyright" content="Candle"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="注意力机制注意力提示定义首先，考虑一个相对简单的状况， 即只使用非自主性提示。 要想将选择偏向于感官输入， 则可以简单地使用参数化的全连接层， 甚至是非参数化的最大汇聚层或平均汇聚层。 因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为查询（query）。 给定任何查询，注意力机制通过注意力汇聚（attention pooling） 将选择">
<meta property="og:type" content="article">
<meta property="og:title" content="10.0 注意力机制">
<meta property="og:url" content="http://candle1220.github.io/post/55896.html">
<meta property="og:site_name" content="Night&#39;s Watch">
<meta property="og:description" content="注意力机制注意力提示定义首先，考虑一个相对简单的状况， 即只使用非自主性提示。 要想将选择偏向于感官输入， 则可以简单地使用参数化的全连接层， 甚至是非参数化的最大汇聚层或平均汇聚层。 因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为查询（query）。 给定任何查询，注意力机制通过注意力汇聚（attention pooling） 将选择">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png">
<meta property="article:published_time" content="2024-02-25T09:50:38.989Z">
<meta property="article:modified_time" content="2024-02-29T13:52:42.107Z">
<meta property="article:author" content="Candle">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png"><link rel="shortcut icon" href="/img/candle.png"><link rel="canonical" href="http://candle1220.github.io/post/55896"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#55d5fc","bgDark":"#121212","position":"top-center"},
  source: {
    jQuery: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/jquery/3.6.0/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.js',
      css: 'https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/fancybox/3.5.7/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '10.0 注意力机制',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-29 21:52:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    document.addEventListener('pjax:complete', detectApple)})(window)</script><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/Swiper/8.0.6/swiper-bundle.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">195</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/background.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Night's Watch</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-crow"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">10.0 注意力机制</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-25T09:50:38.989Z" title="发表于 2024-02-25 17:50:38">2024-02-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-29T13:52:42.107Z" title="更新于 2024-02-29 21:52:42">2024-02-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="10.0 注意力机制"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><h2 id="注意力提示"><a href="#注意力提示" class="headerlink" title="注意力提示"></a>注意力提示</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>首先，考虑一个相对简单的状况， 即只使用非自主性提示。 要想将选择偏向于感官输入， 则可以简单地使用参数化的全连接层， 甚至是非参数化的最大汇聚层或平均汇聚层。</p>
<p>因此，“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为<strong><em>查询</em></strong>（query）。 给定任何查询，注意力机制通过<strong><em>注意力汇聚</em></strong>（attention pooling） 将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为<strong><em>值</em></strong>（value）。 更通俗的解释，每个值都与一个<strong><em>键</em></strong>（key）配对， 这可以想象为感官输入的非自主提示。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraqkv.svg" alt="../_images/qkv.svg"></p>
<p> 如图所示，可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。</p>
<p>注意力机制通过注意力汇聚将<strong><em>查询</em></strong>（自主性提示）和<strong><em>键</em></strong>（非自主性提示）结合在一起，实现对<strong><em>值</em></strong>（感官输入）的选择倾向</p>
<p>假设我们有一个简单的英文句子：“The cat sat on the mat.”（猫坐在垫子上。），我们希望将其翻译为中文。在翻译过程中，我们当前的任务是生成“猫”这个词的中文翻译。在这个过程中，我们将使用注意力机制来决定源句子中的哪些词对于当前的翻译任务最为重要。</p>
<ul>
<li><strong>查询（Query）</strong>：假设在我们的模型中，当前的查询是由翻译模型的状态表示的，这个状态试图找到“cat”这个词的最佳中文对应。在这个例子中，查询是对“猫”这个概念的内部表示。</li>
<li><strong>键（Key）</strong>：每个英文单词都会有一个与之对应的键。这些键代表了模型对每个单词的内部表示，用于帮助模型理解每个词与当前查询的相关性。例如，”cat”, “sat”, “on”, “the”, “mat”每个词都有一个键。</li>
<li><strong>值（Value）</strong>：与键相对应，每个单词也都有一个值，这些值是实际用于计算输出的数据。在我们的翻译任务中，这些值可能包含了每个英文单词的含义、用法和上下文信息，这些信息将用于生成翻译。</li>
</ul>
<hr>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>平均汇聚层可以被视为输入的加权平均值， 其中各输入的权重是一样的。 实际上，注意力汇聚得到的是加权平均的总和值， 其中权重是在给定的查询和不同的键之间计算得出的。</p>
<p>为了可视化注意力权重，需要定义一个<code>show_heatmaps</code>函数。 其输入<code>matrices</code>的形状是 （要显示的行数，要显示的列数，查询的数目，键的数目）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># #@save 注释用于标记这个函数可能会被保存或导入到其他脚本中使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_heatmaps</span>(<span class="params">matrices, xlabel, ylabel, titles=<span class="literal">None</span>, figsize=(<span class="params"><span class="number">2.5</span>, <span class="number">2.5</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                  cmap=<span class="string">&#x27;Reds&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示矩阵热图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.use_svg_display()  <span class="comment"># 使用SVG格式显示图像，以获得更清晰的图像质量</span></span><br><span class="line">    num_rows, num_cols = matrices.shape[<span class="number">0</span>], matrices.shape[<span class="number">1</span>]  <span class="comment"># 获取矩阵数组的行数和列数</span></span><br><span class="line">    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,</span><br><span class="line">                                 sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>, squeeze=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 创建一个图形和一组子图，每个矩阵都会在子图中显示</span></span><br><span class="line">    <span class="keyword">for</span> i, (row_axes, row_matrices) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, matrices)):</span><br><span class="line">        <span class="keyword">for</span> j, (ax, matrix) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(row_axes, row_matrices)):</span><br><span class="line">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class="line">            <span class="comment"># 使用imshow函数绘制矩阵的热图，detach().numpy()将矩阵从PyTorch张量转换为NumPy数组</span></span><br><span class="line">            <span class="keyword">if</span> i == num_rows - <span class="number">1</span>:</span><br><span class="line">                ax.set_xlabel(xlabel)  <span class="comment"># 在最下方的子图设置x轴标签</span></span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                ax.set_ylabel(ylabel)  <span class="comment"># 在最左侧的子图设置y轴标签</span></span><br><span class="line">            <span class="keyword">if</span> titles:</span><br><span class="line">                ax.set_title(titles[j])  <span class="comment"># 如果提供了标题，为每个子图设置标题</span></span><br><span class="line">    fig.colorbar(pcm, ax=axes, shrink=<span class="number">0.6</span>)  <span class="comment"># 为子图集添加一个颜色条，显示颜色映射的数值范围</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面使用一个简单的例子进行演示。 在本例子中，仅当查询和键相同时，注意力权重为1，否则为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.eye(<span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">show_heatmaps(attention_weights, xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_attention-cues_054b1a_36_0.svg" alt="../_images/output_attention-cues_054b1a_36_0.svg"></p>
<hr>
<h2 id="Nadaraya-Watson-核回归"><a href="#Nadaraya-Watson-核回归" class="headerlink" title="Nadaraya-Watson 核回归"></a>Nadaraya-Watson 核回归</h2><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>简单起见，考虑下面这个回归问题：给定的成对的“输入－输出”数据集$\{(x_1, y_1), \ldots, (x_n, y_n)\}$，如何学习$f$来预测任意新输入$x$的输出$\hat{y} = f(x)$？</p>
<p>根据下面的非线性函数生成一个人工数据集，其中加入的噪声项为$\epsilon$：</p>
<script type="math/tex; mode=display">
y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon</script><p>其中$\epsilon$服从均值为$0$和标准差为$0.5$的正态分布。在这里生成了$50$个训练样本和$50$个测试样本。为了更好地可视化之后的注意力模式，需要将训练样本进行<strong>排序</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n_train = <span class="number">50</span>  <span class="comment"># 训练样本数</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)   <span class="comment"># 排序后的训练样本</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x**<span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(<span class="number">0.0</span>, <span class="number">0.5</span>, (n_train,))  <span class="comment"># 训练样本的输出</span></span><br><span class="line">x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)  <span class="comment"># 测试样本</span></span><br><span class="line">y_truth = f(x_test)  <span class="comment"># 测试样本的真实输出</span></span><br><span class="line">n_test = <span class="built_in">len</span>(x_test)  <span class="comment"># 测试样本数</span></span><br></pre></td></tr></table></figure>
<p>下面的函数将绘制所有的训练样本（样本由圆圈表示）， 不带噪声项的真实数据生成函数$f$（标记为“Truth”）， 以及学习得到的预测函数（标记为“Pred”）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span></span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, legend=[<span class="string">&#x27;Truth&#x27;</span>, <span class="string">&#x27;Pred&#x27;</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>);</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="平均汇聚"><a href="#平均汇聚" class="headerlink" title="平均汇聚"></a>平均汇聚</h3><p>先使用最简单的估计器来解决回归问题。基于平均汇聚来计算所有训练样本输出值的平均值：</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{n}\sum_{i=1}^n y_i</script><p>如图所示，这个估计器确实不够聪明。真实函数$f$（“Truth”）和预测函数（“Pred”）相差很大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#torch.repeat_interleave 是一个PyTorch函数，用于沿指定维度重复张量中的元素。</span></span><br><span class="line">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_nadaraya-waston_736177_39_0.svg" alt="../_images/output_nadaraya-waston_736177_39_0.svg"></p>
<hr>
<h3 id="非参数注意力汇聚"><a href="#非参数注意力汇聚" class="headerlink" title="非参数注意力汇聚"></a>非参数注意力汇聚</h3><p>显然，平均汇聚忽略了输入$x_i$。于是Nadaraya和Watson提出了一个更好的想法，根据输入的位置对输出$y_i$进行加权：</p>
<script type="math/tex; mode=display">
f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i</script><p>其中$K$是<strong>核</strong>（kernel）。公式所描述的估计器被称为<strong>Nadaraya-Watson核回归</strong></p>
<p>这里不会深入讨论核函数的细节，但受此启发，我们可以从注意力机制框架的角度重写成为一个更加通用的<strong>注意力汇聚</strong>（attention pooling）公式：</p>
<script type="math/tex; mode=display">
f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,</script><p>其中$x$是查询，$(x_i, y_i)$是键值对。注意力汇聚是$y_i$的加权平均。将查询$x$和键$x_i$之间的关系建模为<strong>注意力权重</strong>$\alpha(x, x_i)$，这个权重将被分配给每一个对应值$y_i$。对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1。</p>
<p>为了更好地理解注意力汇聚，下面考虑一个<strong>高斯核</strong>（Gaussian kernel），其定义为：</p>
<script type="math/tex; mode=display">
K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).</script><p>将高斯核代入可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}</script><p>在公式中，如果一个键$x_i$越是接近给定的查询$x$，那么分配给这个键对应值$y_i$的注意力权重就会越大，也就“获得了更多的注意力”。</p>
<p>值得注意的是，Nadaraya-Watson核回归是一个非参数模型。因此，公式是<strong>非参数的注意力汇聚</strong>模型。接下来，我们将基于这个非参数的注意力汇聚模型来绘制预测结果。绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_repeat的形状:(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着相同的测试输入（例如：同样的查询）</span></span><br><span class="line">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line"><span class="comment"># x_train包含着键。attention_weights的形状：(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重</span></span><br><span class="line">attention_weights = nn.functional.softmax(-(X_repeat - x_train)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class="line">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_nadaraya-waston_736177_54_0.svg" alt="../_images/output_nadaraya-waston_736177_54_0.svg"></p>
<p>现在来观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近， 注意力汇聚的注意力权重就越高。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_nadaraya-waston_736177_69_0.svg" alt="../_images/output_nadaraya-waston_736177_69_0.svg"></p>
<hr>
<h3 id="带参数注意力汇聚"><a href="#带参数注意力汇聚" class="headerlink" title="带参数注意力汇聚"></a>带参数注意力汇聚</h3><p>非参数的Nadaraya-Watson核回归具有<strong><em>一致性</em></strong>（consistency）的优点： 如果有足够的数据，此模型会收敛到最优结果。 尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。</p>
<p>在下面的查询$x$和键$x_i$之间的距离乘以可学习参数$w$：</p>
<script type="math/tex; mode=display">
\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}</script><p>本节的余下部分将通过训练这个模型来学习注意力汇聚的参数。</p>
<p><strong>批量矩阵乘法</strong></p>
<p>在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><code>torch.bmm</code></p>
<ul>
<li><strong>功能</strong>：<code>torch.bmm</code> 是批量矩阵乘法（Batch Matrix Multiplication）的缩写。这个函数用于计算两个张量中包含的多组矩阵的乘积。具体来说，如果你有两个三维张量，每个张量中包含了多个二维矩阵，<code>torch.bmm</code> 可以一次性计算这些矩阵的乘积。</li>
<li><strong>输入</strong>：两个形状为 <code>(b, n, m)</code> 和 <code>(b, m, p)</code> 的张量，其中 <code>b</code> 是批次大小，表示有多少组矩阵需要相乘，<code>n</code>, <code>m</code>, <code>p</code> 分别是这些矩阵的维度。</li>
<li><strong>输出</strong>：一个形状为 <code>(b, n, p)</code> 的张量，包含了每一组输入矩阵乘积的结果。</li>
</ul>
<p><code>torch.unsqueeze</code></p>
<ul>
<li><strong>功能</strong>：<code>torch.unsqueeze</code> 用于在指定位置增加一个维度（即增加一个轴）。这个操作不会改变张量的数据，但会改变张量的形状</li>
<li><strong>输入</strong>：一个张量和一个指定的维度（位置）。</li>
<li><p><strong>输出</strong>：形状改变后的张量，其在指定位置上增加了一个大小为1的维度。</p>
</li>
<li><p><code>weights.unsqueeze(1)</code> 将 <code>weights</code> 张量从形状 <code>(2, 10)</code> 改变为 <code>(2, 1, 10)</code>。这里，<code>1</code> 表示在原有的行和列之间增加了一个新的维度。</p>
</li>
<li><code>values.unsqueeze(-1)</code> 将 <code>values</code> 张量从形状 <code>(2, 10)</code> 改变为 <code>(2, 10, 1)</code>。这里，<code>-1</code> 表示在张量的最后增加了一个新的维度。</li>
</ul>
<hr>
<p>接下来，将训练数据集变换为键和值用于训练注意力模型。 在带参数的注意力汇聚模型中， 任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算， 从而得到其对应的预测输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入</span></span><br><span class="line">X_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出</span></span><br><span class="line">Y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># keys的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">keys = X_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># values的形状:(&#x27;n_train&#x27;，&#x27;n_train&#x27;-1)</span></span><br><span class="line">values = Y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>训练带参数的注意力汇聚模型时，使用平方损失函数和随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    l = loss(net(x_train, keys, values), y_train)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_nadaraya-waston_736177_144_0.svg" alt="../_images/output_nadaraya-waston_736177_144_0.svg"></p>
<p>如下所示，训练完带参数的注意力汇聚模型后可以发现： 在尝试拟合带噪声的训练数据时， 预测结果绘制的线不如之前非参数模型的平滑。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(net.attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_nadaraya-waston_736177_174_0.svg" alt="../_images/output_nadaraya-waston_736177_174_0.svg"></p>
<hr>
<h2 id="注意力评分函数"><a href="#注意力评分函数" class="headerlink" title="注意力评分函数"></a>注意力评分函数</h2><p>高斯核指数部分可以视为<strong><em>注意力评分函数</em></strong>（attention scoring function）， 简称<strong><em>评分函数</em></strong>（scoring function）， 然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。</p>
<p>从宏观来看，上述算法可以用来实现注意力机制框架。图中说明了如何将注意力汇聚的输出计算成为值的加权和，其中$a$表示注意力评分函数。由于注意力权重是概率分布，因此加权和其本质上是加权平均值。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraattention-output.svg" alt="../_images/attention-output.svg"></p>
<p>用数学语言描述，假设有一个查询$\mathbf{q} \in \mathbb{R}^q$和$m$个“键－值”对$(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$，其中$\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$。</p>
<p>注意力汇聚函数$f$就被表示成值的加权和：</p>
<script type="math/tex; mode=display">
f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v</script><p>其中查询$\mathbf{q}$和键$\mathbf{k}_i$的注意力权重（标量）是通过注意力评分函数$a$将两个向量映射成标量，再经过softmax运算得到的：</p>
<script type="math/tex; mode=display">
\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}</script><p>正如上图所示，选择不同的注意力评分函数$a$会导致不同的注意力汇聚操作。本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。</p>
<hr>
<h3 id="掩蔽softmax"><a href="#掩蔽softmax" class="headerlink" title="掩蔽softmax"></a>掩蔽softmax</h3><p>正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。 例如，为了在机器翻译中高效处理小批量数据集， 某些文本序列被填充了没有意义的特殊词元。 </p>
<p>为了仅将有意义的词元作为值来获取注意力汇聚， 可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。 下面的<code>masked_softmax</code>函数 实现了这样的<strong><em>掩蔽softmax操作</em></strong>， 其中任何超出有效长度的位置都被掩蔽并置为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)  <span class="comment"># 如果没有提供有效长度，直接在最后一个轴上应用softmax</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape  <span class="comment"># 保存X的原始形状</span></span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果valid_lens是1D张量，则对每个样本的所有序列重复相同的长度</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果valid_lens是2D张量，则将其展平</span></span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 对序列进行掩蔽操作，有效长度之外的位置被置为一个非常大的负数(-1e6)</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="comment"># 将掩蔽后的X重新塑形为原始形状，并在最后一个轴上应用softmax</span></span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>为了演示此函数是如何工作的， 考虑由两个$2×4$矩阵表示的样本， 这两个样本的有效长度分别为2和3。 经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[0.5980, 0.4020, 0.0000, 0.0000],</span></span><br><span class="line"><span class="string">         [0.5548, 0.4452, 0.0000, 0.0000]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[0.3716, 0.3926, 0.2358, 0.0000],</span></span><br><span class="line"><span class="string">         [0.3455, 0.3337, 0.3208, 0.0000]]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="加性注意力"><a href="#加性注意力" class="headerlink" title="加性注意力"></a>加性注意力</h3><p>一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。</p>
<p>给定查询$\mathbf{q} \in \mathbb{R}^q$和键$\mathbf{k} \in \mathbb{R}^k$，<strong>加性注意力</strong>（additive attention）的评分函数为</p>
<script type="math/tex; mode=display">
a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}</script><p>其中可学习的参数是$\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$和$\mathbf w_v\in\mathbb R^{h}$。</p>
<p>将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$。通过使用$\tanh$作为激活函数，并且禁用偏置项。</p>
<p>下面来实现加性注意力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdditiveAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 使用线性层将键和查询转换到相同的隐藏维度</span></span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 线性层用于计算加性注意力的分数</span></span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># Dropout层用于正则化</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        <span class="comment"># 首先，通过W_k和W_q将键和查询映射到隐藏空间</span></span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 扩展维度，使得queries和keys能够进行广播相加</span></span><br><span class="line">        <span class="comment"># queries形状变为(batch_size, 查询的个数, 1, num_hiddens)</span></span><br><span class="line">        <span class="comment"># keys形状变为(batch_size, 1, 键-值对的个数, num_hiddens)</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 应用tanh激活函数</span></span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># 计算加性注意力分数</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 应用掩蔽softmax函数，考虑到有效长度</span></span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># 应用注意力权重到值上，进行加权求和</span></span><br><span class="line">        <span class="comment"># values形状：(batch_size, 键-值对的个数, 值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>通过广播，<code>1</code>这个维度会被自动扩展以匹配另一个张量的相应维度（查询的<code>1</code>扩展以匹配键的“键-值对的个数”，键的<code>1</code>扩展以匹配查询的“查询的个数”），从而两者可以在每个维度上相加。</p>
<p>这个广播相加的结果是一个形状为<code>(batch_size, 查询的个数, 键-值对的个数, num_hiddens)</code>的张量。这个张量的每个元素代表了一个查询与一个键在映射到共同隐藏空间后的加性组合。这个加性组合随后通过激活函数和进一步的处理来计算注意力分数。</p>
<p>用一个小例子来演示上面的<code>AdditiveAttention</code>类， 其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小）， 实际输出为$(2,1,20)$、$(2,10,2)$和$(2,10,4)$。 注意力汇聚输出的形状为（批量大小，查询的步数，值的维度）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化输入数据</span></span><br><span class="line">queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化加性注意力模型，设置为评估模式</span></span><br><span class="line">attention = AdditiveAttention(key_size=<span class="number">2</span>, query_size=<span class="number">20</span>, num_hiddens=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过加性注意力模型计算输出</span></span><br><span class="line">output = attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=&lt;BmmBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<ol>
<li><strong>输入维度</strong>：<ul>
<li><strong>查询（Queries）</strong>：初始维度为<code>(batch_size, 查询的个数, query_size)</code>。在示例中，这个维度是<code>(2, 1, 20)</code>。</li>
<li><strong>键（Keys）和值（Values）</strong>：键的初始维度为<code>(batch_size, 键-值对的个数, key_size)</code>，值的维度为<code>(batch_size, 键-值对的个数, value_size)</code>。在示例中，键的维度是<code>(2, 10, 2)</code>，值的维度是<code>(2, 10, 4)</code>。</li>
</ul>
</li>
<li><strong>线性变换后的维度</strong>：<ul>
<li>经过<code>self.W_q(queries)</code>和<code>self.W_k(keys)</code>的线性变换后，查询和键都被映射到了<code>num_hiddens</code>维度的空间。这里<code>num_hiddens=8</code>。因此，变换后查询和键的维度分别变为<code>(batch_size, 查询的个数, num_hiddens)</code>和<code>(batch_size, 键-值对的个数, num_hiddens)</code>。在示例中，这意味着它们都变为<code>(2, 1, 8)</code>和<code>(2, 10, 8)</code>。</li>
</ul>
</li>
<li><strong><code>unsqueeze</code>操作后的维度</strong>：<ul>
<li>执行<code>queries.unsqueeze(2)</code>后，查询的维度变为<code>(batch_size, 查询的个数, 1, num_hiddens)</code>，在示例中为<code>(2, 1, 1, 8)</code>。</li>
<li>执行<code>keys.unsqueeze(1)</code>后，键的维度变为<code>(batch_size, 1, 键-值对的个数, num_hiddens)</code>，在示例中为<code>(2, 1, 10, 8)</code>。</li>
</ul>
</li>
<li><strong>广播相加后的维度</strong>：<ul>
<li>在执行加法操作后，由于广播机制，最终的<code>features</code>维度为<code>(batch_size, 查询的个数, 键-值对的个数, num_hiddens)</code>。在示例中，这个维度是<code>(2, 1, 10, 8)</code>。</li>
</ul>
</li>
<li><strong>通过<code>self.w_v</code>后维度变化</strong>：<ul>
<li>经过<code>self.w_v(features)</code>计算得到的分数在最后一个维度为1，因此维度是<code>(batch_size, 查询的个数, 键-值对的个数, 1)</code>。在示例中，维度变为<code>(2, 1, 10, 1)</code>。</li>
</ul>
</li>
<li><strong><code>squeeze</code>操作后的维度</strong>：<ul>
<li>执行<code>scores.squeeze(-1)</code>后，去除了最后一个维度，所以<code>scores</code>的维度变为<code>(batch_size, 查询的个数, 键-值对的个数)</code>。在示例中，这变为<code>(2, 1, 10)</code>。</li>
</ul>
</li>
<li><strong>输出维度</strong>：<ul>
<li>最终的输出是通过对<code>values</code>进行加权求和得到的，其维度为<code>(batch_size, 查询的个数, value_size)</code>。在示例中，输出维度是<code>(2, 1, 4)</code>，这表示每个查询对应的加权值维度。</li>
</ul>
</li>
</ol>
<p>尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的， 所以注意力权重是均匀的，由指定的有效长度决定。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),</span><br><span class="line">                  xlabel=&#x27;Keys&#x27;, ylabel=&#x27;Queries&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoratyporaoutput_attention-scoring-functions_2a8fdc_96_0.svg" alt="../_images/output_attention-scoring-functions_2a8fdc_96_0.svg"></p>
<hr>
<h3 id="缩放点积注意力"><a href="#缩放点积注意力" class="headerlink" title="缩放点积注意力"></a>缩放点积注意力</h3><p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度$d$。</p>
<p>假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为$0$，方差为$d$。为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是$1$，我们再将点积除以$\sqrt{d}$，则<strong>缩放点积注意力</strong>（scaled dot-product attention）评分函数为：</p>
<script type="math/tex; mode=display">
a(\mathbf q, \mathbf k) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}</script><p>在实践中，我们通常从小批量的角度来考虑提高效率，例如基于$n$个查询和$m$个键－值对计算注意力，其中查询和键的长度为$d$，值的长度为$v$。</p>
<p>查询$\mathbf Q\in\mathbb R^{n\times d}$、</p>
<p>键$\mathbf K\in\mathbb R^{m\times d}$和</p>
<p>值$\mathbf V\in\mathbb R^{m\times v}$的缩放点积注意力是：</p>
<script type="math/tex; mode=display">
\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.</script><p>下面的缩放点积注意力的实现使用了暂退法进行模型正则化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># queries的形状：(batch_size，查询的个数，d)</span></span><br><span class="line">    <span class="comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span></span><br><span class="line">    <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">    <span class="comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span></span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 设置transpose_b=True为了交换keys的最后两个维度</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>
<p>为了演示上述的<code>DotProductAttention</code>类， 我们使用与先前加性注意力例子中相同的键、值和有效长度。 对于点积操作，我们令查询的特征维度与键的特征维度大小相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[10.0000, 11.0000, 12.0000, 13.0000]]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>与加性注意力演示相同，由于键包含的是相同的元素， 而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_attention-scoring-functions_2a8fdc_141_0.svg" alt="../_images/output_attention-scoring-functions_2a8fdc_141_0.svg"></p>
<p>当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的“点－积”注意力评分函数的计算效率更高。</p>
<hr>
<h2 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h2><p>在实践中，当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 <strong><em>子空间表示</em></strong> 可能是有益的。</p>
<p>为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的ℎ组不同的 <strong><em>线性投影</em></strong>来变换查询、键和值。 然后，这ℎ组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这ℎ个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为<strong><em>多头注意力</em></strong>， 对于ℎ个注意力汇聚输出，每一个注意力汇聚都被称作一个<em>头</em>（head）。 图中展示了使用全连接层来实现可学习的线性变换的多头注意力</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoramulti-head-attention.svg" alt="../_images/multi-head-attention.svg"></p>
<hr>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>在实现多头注意力之前，让我们用数学语言将这个模型形式化地描述出来。</p>
<p>给定查询$\mathbf{q} \in \mathbb{R}^{d_q}$、键$\mathbf{k} \in \mathbb{R}^{d_k}$和值$\mathbf{v} \in \mathbb{R}^{d_v}$，每个注意力头$\mathbf{h}_i$（$i = 1, \ldots, h$）的计算方法为：</p>
<script type="math/tex; mode=display">
\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v}</script><p>其中，可学习的参数包括$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$、$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$，以及代表注意力汇聚的函数$f$。$f$可以是上一节中的<strong>加性注意力</strong>和<strong>缩放点积注意力</strong></p>
<p>多头注意力的输出需要经过另一个线性转换，它对应着$h$个头连结后的结果，因此其可学习参数是$\mathbf W_o\in\mathbb R^{p_o\times h p_v}$：</p>
<script type="math/tex; mode=display">
\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}</script><p>基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。</p>
<hr>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>在实现过程中通常选择<strong>缩放点积注意力</strong>作为每一个注意力头。为了避免计算代价和参数代价的大幅增长，</p>
<p>我们设定$p_q = p_k = p_v = p_o / h$。 $p_q,p_k,p_v$分别代表单个注意力头的查询、键和值的维度，而 $p_o$ 是所有头合并后的输出维度。通过这种方式，每个头处理的维度更小，但总的来看，模型能够并行处理，并学习到不同子空间的表示。ℎ 是头的数量，$p_o$ 通过模型的参数（如<code>num_hiddens</code>）指定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads  <span class="comment"># 定义头的数量</span></span><br><span class="line">        <span class="comment"># 初始化点积注意力机制，其中包含dropout</span></span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        <span class="comment"># 为查询、键、值分别初始化线性变换层</span></span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        <span class="comment"># 最后的线性变换层，用于合并多头注意力的输出</span></span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        <span class="comment"># 对输入的查询、键、值进行线性变换</span></span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果提供了有效长度，则对其进行处理以匹配扩展后的批次大小</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用点积注意力机制，计算多头注意力的输出</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并多头注意力的输出</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="comment"># 通过最后的线性变换层</span></span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>为了能够使多个头并行计算， 上面的<code>MultiHeadAttention</code>类将使用下面定义的两个转置函数。 具体来说，<code>transpose_output</code>函数反转了<code>transpose_qkv</code>函数的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>下面使用键和值相同的小例子来测试我们编写的<code>MultiHeadAttention</code>类。 多头注意力输出的形状是（<code>batch_size</code>，<code>num_queries</code>，<code>num_hiddens</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                               num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">MultiHeadAttention(</span></span><br><span class="line"><span class="string">  (attention): DotProductAttention(</span></span><br><span class="line"><span class="string">    (dropout): Dropout(p=0.5, inplace=False)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (W_q): Linear(in_features=100, out_features=100, bias=False)</span></span><br><span class="line"><span class="string">  (W_k): Linear(in_features=100, out_features=100, bias=False)</span></span><br><span class="line"><span class="string">  (W_v): Linear(in_features=100, out_features=100, bias=False)</span></span><br><span class="line"><span class="string">  (W_o): Linear(in_features=100, out_features=100, bias=False)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">num_kvpairs, valid_lens =  <span class="number">6</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">Y = torch.ones((batch_size, num_kvpairs, num_hiddens))</span><br><span class="line">attention(X, Y, Y, valid_lens).shape</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([2, 4, 100])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h2><p>在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。 想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中， 以便<strong>同一组词元同时充当查询、键和值</strong>。 具体来说，<strong>每个查询都会关注所有的键－值对并生成一个注意力输出</strong>。 由于查询、键和值来自同一组输入，因此被称为 <strong><em>自注意力</em></strong></p>
<p>给定一个由词元组成的输入序列$\mathbf{x}_1, \ldots, \mathbf{x}_n$，其中任意$\mathbf{x}_i \in \mathbb{R}^d$（$1 \leq i \leq n$）。该序列的自注意力输出为一个长度相同的序列$\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中：</p>
<script type="math/tex; mode=display">
\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d</script><p>根据中之前定义的注意力汇聚函数$f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,$下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为（批量大小，时间步的数目或词元序列的长度，$d$）。输出与输入的张量形状相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                                   num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">MultiHeadAttention(</span></span><br><span class="line"><span class="string">  (attention): DotProductAttention(</span></span><br><span class="line"><span class="string">    (dropout): Dropout(p=0.5, inplace=False)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (W_q): Linear(in_features=100, out_features=100, bias=False)</span></span><br><span class="line"><span class="string">  (W_k): Linear(in_features=100, out_features=100, bias=False)</span></span><br><span class="line"><span class="string">  (W_v): Linear(in_features=100, out_features=100, bias=False)</span></span><br><span class="line"><span class="string">  (W_o): Linear(in_features=100, out_features=100, bias=False)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>num_hiddens</code>: 这个参数指定了查询（Query）、键（Key）、值（Value）以及最终输出向量的维度。在这个例子中，所有这些维度都被设置为100。</li>
<li><code>num_heads</code>: 多头注意力中头的数量。在这里，设置为5，意味着注意力机制会被分成5个头进行并行计算，每个头处理的是输入数据的不同子空间。</li>
<li>第三个参数<code>0.5</code>是<code>dropout</code>的比率，用于防止模型过拟合，通过随机丢弃一部分注意力权重来增加模型的泛化能力。</li>
</ul>
<p>综上所述，这段代码创建了一个具有100维隐藏层和5个注意力头的<code>MultiHeadAttention</code>模型实例，并通过设置<code>dropout</code>比率为0.5来帮助防止过拟合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries, valid_lens = <span class="number">2</span>, <span class="number">4</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([2, 4, 100])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><p>接下来比较下面几个架构，目标都是将由$n$个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由$d$维向量表示。具体来说，将比较的是卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。</p>
<p>请注意，顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离<strong>依赖关系</strong> </p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoracnn-rnn-self-attention.svg" alt="../_images/cnn-rnn-self-attention.svg"></p>
<p>考虑一个卷积核大小为$k$的卷积层。</p>
<p>目前只需要知道的是，由于序列长度是$n$，输入和输出的通道数量都是$d$，所以卷积层的计算复杂度为$\mathcal{O}(knd^2)$。如图所示，卷积神经网络是分层的，因此为有$\mathcal{O}(1)$个顺序操作，最大路径长度为$\mathcal{O}(n/k)$。例如，$\mathbf{x}_1$和$\mathbf{x}_5$处于图中卷积核大小为3的双层卷积神经网络的感受野内。</p>
<p>当更新循环神经网络的隐状态时，$d \times d$权重矩阵和$d$维隐状态的乘法计算复杂度为$\mathcal{O}(d^2)$。由于序列长度为$n$，因此循环神经网络层的计算复杂度为$\mathcal{O}(nd^2)$。有$\mathcal{O}(n)$个顺序操作无法并行化，最大路径长度也是$\mathcal{O}(n)$。</p>
<p>在自注意力中，查询、键和值都是$n \times d$矩阵。考虑到缩放的”点－积“注意力，其中$n \times d$矩阵乘以$d \times n$矩阵。之后输出的$n \times n$矩阵乘以$n \times d$矩阵。因此，自注意力具有$\mathcal{O}(n^2d)$计算复杂性。每个词元都通过自注意力直接连接到任何其他词元。因此，有$\mathcal{O}(1)$个顺序操作可以并行计算，最大路径长度也是$\mathcal{O}(1)$。</p>
<p>总而言之，卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的<strong>最大路径长度最短</strong>。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</p>
<hr>
<h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 <strong><em>位置编码</em></strong>（positional encoding）来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。 接下来描述的是基于正弦函数和余弦函数的固定位置编码</p>
<p>假设输入表示$\mathbf{X} \in \mathbb{R}^{n \times d}$包含一个序列中$n$个词元的$d$维嵌入表示。位置编码使用相同形状的位置嵌入矩阵$\mathbf{P} \in \mathbb{R}^{n \times d}$输出$\mathbf{X} + \mathbf{P}$，矩阵第$i$行、第$2j$列和$2j+1$列上的元素为：</p>
<script type="math/tex; mode=display">
\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right)\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right)\end{aligned}</script><p>乍一看，这种基于三角函数的设计看起来很奇怪。在解释这个设计之前，让我们先在下面的<code>PositionalEncoding</code>类中实现它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        <span class="comment"># 初始化一个Dropout层，用于在位置编码后对输出进行dropout操作以防止过拟合</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 初始化一个位置编码矩阵P，它有max_len行和num_hiddens列</span></span><br><span class="line">        <span class="comment"># 这里max_len是序列的最大长度，num_hiddens是每个词元的隐藏向量维度</span></span><br><span class="line">        self.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算位置编码的公式部分</span></span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(-<span class="number">1</span>, <span class="number">1</span>) / torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(<span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) / num_hiddens)</span><br><span class="line">        <span class="comment"># 对偶数位置使用正弦函数进行编码</span></span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        <span class="comment"># 对奇数位置使用余弦函数进行编码</span></span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># 将位置编码添加到输入X上</span></span><br><span class="line">        <span class="comment"># 这里X是一个三维张量，形状为(batch_size, sequence_length, num_hiddens)</span></span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="comment"># 对结果应用dropout并返回</span></span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在位置嵌入矩阵$P$中， 行代表词元在序列中的位置，列代表位置编码的不同维度。 从下面的例子中可以看到位置嵌入矩阵的第6列和第7列的频率高于第8列和第9列。 第6列和第7列之间的偏移量（第8列和第9列相同）是由于正弦函数和余弦函数的交替。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim)))</span><br><span class="line">P = pos_encoding.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">&#x27;Row (position)&#x27;</span>,</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">&quot;Col %d&quot;</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_self-attention-and-positional-encoding_d76d5a_52_0.svg" alt="../_images/output_self-attention-and-positional-encoding_d76d5a_52_0.svg"></p>
<hr>
<p><strong>绝对位置信息</strong></p>
<p>为了明白沿着编码维度单调降低的频率与绝对位置信息的关系， 让我们打印出0,1,…,7的二进制表示形式。 正如所看到的，每个数字、每两个数字和每四个数字上的比特值 在第一个最低位、第二个最低位和第三个最低位上<strong>分别交替</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>的二进制是：<span class="subst">&#123;i:&gt;03b&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">0的二进制是：000</span></span><br><span class="line"><span class="string">1的二进制是：001</span></span><br><span class="line"><span class="string">2的二进制是：010</span></span><br><span class="line"><span class="string">3的二进制是：011</span></span><br><span class="line"><span class="string">4的二进制是：100</span></span><br><span class="line"><span class="string">5的二进制是：101</span></span><br><span class="line"><span class="string">6的二进制是：110</span></span><br><span class="line"><span class="string">7的二进制是：111</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">P = P[<span class="number">0</span>, :, :].unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">d2l.show_heatmaps(P, xlabel=<span class="string">&#x27;Column (encoding dimension)&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Row (position)&#x27;</span>, figsize=(<span class="number">3.5</span>, <span class="number">4</span>), cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_self-attention-and-positional-encoding_d76d5a_82_0.svg" alt="../_images/output_self-attention-and-positional-encoding_d76d5a_82_0.svg"></p>
<hr>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>自注意力同时具有并行计算和最短的最大路径长度这两个优势。因此，使用自注意力来设计深度架构是很有吸引力的。对比之前仍然依赖循环神经网络实现输入表示的自注意力模型，Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层。尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。</p>
<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><p>Transformer作为编码器－解码器架构的一个实例，其整体架构图在图中展示。正如所见到的，Transformer是由编码器和解码器组成的。Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的<strong><em>嵌入</em></strong>（embedding）表示将加上<strong><em>位置编码</em></strong>（positional encoding），再分别输入到<strong>编码器</strong>和<strong>解码器</strong>中。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoratransformer.svg" alt="../_images/transformer.svg"></p>
<p>从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为$\mathrm{sublayer}$）。第一个子层是 <strong><em>多头自注意力</em></strong> 汇聚；第二个子层是<strong><em>基于位置的前馈网络</em></strong>。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。</p>
<p>受ResNet中残差网络的启发，每个子层都采用了<strong><em>残差连接</em></strong>。在Transformer中，对于序列中任何位置的任何输入$\mathbf{x} \in \mathbb{R}^d$，都要求满足$\mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$，以便残差连接满足$\mathbf{x} + \mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$。在残差连接的加法计算之后，紧接着应用<strong>层规范化</strong>，因此，输入序列对应的每个位置，Transformer编码器都将输出一个$d$维表示向量。</p>
<p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为<strong>编码器－解码器注意力</strong>层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种<strong>掩蔽</strong>注意力保留了<strong>自回归</strong>属性，确保预测仅依赖于已生成的输出词元。</p>
<p>接下来将实现Transformer模型的剩余部分。</p>
<hr>
<h3 id="基于位置的前馈神经"><a href="#基于位置的前馈神经" class="headerlink" title="基于位置的前馈神经"></a>基于位置的前馈神经</h3><p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是<strong><em>基于位置的</em></strong>的原因。在下面的实现中，输入<code>X</code>的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，<code>ffn_num_outputs</code>）的输出张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 第一个全连接层，将输入维度从ffn_num_input变换到ffn_num_hiddens</span></span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        <span class="comment"># ReLU激活函数</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        <span class="comment"># 第二个全连接层，将隐藏层维度从ffn_num_hiddens变换到ffn_num_outputs</span></span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># X是输入的张量，形状为(batch_size, sequence_length, ffn_num_input)</span></span><br><span class="line">        <span class="comment"># 先通过第一个全连接层和ReLU激活函数</span></span><br><span class="line">        <span class="comment"># 然后通过第二个全连接层</span></span><br><span class="line">        <span class="comment"># 输出张量的形状为(batch_size, sequence_length, ffn_num_outputs)</span></span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面的例子显示，改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<p>最后会输出(2,3,8)的张量，前馈神经网络只会把输入的最后一个维度改变成指定的输出维度</p>
<hr>
<h3 id="残差连接和层规范化"><a href="#残差连接和层规范化" class="headerlink" title="残差连接和层规范化"></a>残差连接和层规范化</h3><p>现在让我们关注图中的<strong><em>加法和规范化</em></strong>（add&amp;norm）组件。正如在本节开头所述，这是由残差连接和紧随其后的层规范化组成的。两者都是构建有效的深度架构的关键。</p>
<p>批量规范化<code>batchnorm</code>中解释了在一个小批量的样本内基于批量规范化对数据进行重新中心化和重新缩放的调整。层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。</p>
<p>现在可以使用残差连接和层规范化来实现<code>AddNorm</code>类。暂退法也被作为正则化方法使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 初始化一个Dropout层，用于在加法操作之前对Y进行随机丢弃，以减少过拟合</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 初始化一个LayerNorm层，用于在加法操作之后对结果进行规范化</span></span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        <span class="comment"># 首先对Y应用dropout，然后与X进行加法操作（残差连接）</span></span><br><span class="line">        <span class="comment"># 最后，对加法操作的结果进行层规范化</span></span><br><span class="line">        <span class="comment"># X是来自前一个层的输入，Y是当前层的输出</span></span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>比较层规范化和批量规范化</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特性/应用</th>
<th>层规范化 (LN)</th>
<th>批量规范化 (BN)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>规范化维度</strong></td>
<td>对单个样本内的所有特征进行规范化</td>
<td>对批次内的同一特征进行规范化</td>
</tr>
<tr>
<td><strong>计算统计量</strong></td>
<td>每个样本独立计算均值和方差</td>
<td>跨整个批次的样本计算均值和方差</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>循环神经网络（RNN）、Transformer</td>
<td>卷积神经网络（CNN）</td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td>适用于变长输入，不依赖于批次大小</td>
<td>可以加速训练过程，有助于稳定训练</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>可能不如BN在某些卷积网络中有效</td>
<td>对小批量大小敏感，可能影响模型在小批量数据上的表现</td>
</tr>
<tr>
<td><strong>自回归任务</strong></td>
<td>适合，因为不泄露未来信息</td>
<td>需要特别设计以避免未来信息泄露</td>
</tr>
<tr>
<td><strong>并行计算</strong></td>
<td>容易实现，因为计算独立于其他样本</td>
<td>需要整个批次的数据进行计算</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20240229204921088.png" alt="image-20240229204921088"></p>
<p>有了组成Transformer编码器的基础组件，现在可以先实现编码器中的一个层。下面的<code>EncoderBlock</code>类包含两个子层：<strong>多头自注意力和基于位置的前馈网络</strong>，这两个子层都使用了<strong>残差连接和紧随的层规范化</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line"> <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">      参数:</span></span><br><span class="line"><span class="string">    - key_size: 键向量的维度大小。</span></span><br><span class="line"><span class="string">    - query_size: 查询向量的维度大小。</span></span><br><span class="line"><span class="string">    - value_size: 值向量的维度大小。</span></span><br><span class="line"><span class="string">    - num_hiddens: 自注意力层和前馈网络输出的隐藏单元数量，也是模型中间层的维度。</span></span><br><span class="line"><span class="string">    - norm_shape: 层规范化应用的维度。这通常是输入特征的维度。</span></span><br><span class="line"><span class="string">    - ffn_num_input: 前馈网络的输入维度。</span></span><br><span class="line"><span class="string">    - ffn_num_hiddens: 前馈网络中间层的维度。</span></span><br><span class="line"><span class="string">    - num_heads: 多头注意力机制中头的数量。</span></span><br><span class="line"><span class="string">    - dropout: Dropout层的丢弃比例。</span></span><br><span class="line"><span class="string">    - use_bias: 是否在多头自注意力机制和前馈网络中使用偏置项。默认为False。</span></span><br><span class="line"><span class="string"> &quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 初始化多头自注意力机制，其中包括指定的头数和大小参数</span></span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        <span class="comment"># 第一个残差连接和层规范化模块</span></span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="comment"># 初始化基于位置的前馈网络</span></span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        <span class="comment"># 第二个残差连接和层规范化模块</span></span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens</span>):</span></span><br><span class="line">        <span class="comment"># 先通过多头自注意力机制，然后应用残差连接和层规范化</span></span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="comment"># 接着通过基于位置的前馈网络，再次应用残差连接和层规范化</span></span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>正如从代码中所看到的，Transformer编码器中的任何层都不会改变其输入的形状</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br><span class="line"><span class="comment"># 输出依然是：torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure>
<p>下面实现的Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例。由于这里使用的是值范围在−1和1之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">d2l.Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化Transformer编码器。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        - vocab_size: 词汇表大小，用于嵌入层。</span></span><br><span class="line"><span class="string">        - key_size: 键向量的维度。</span></span><br><span class="line"><span class="string">        - query_size: 查询向量的维度。</span></span><br><span class="line"><span class="string">        - value_size: 值向量的维度。</span></span><br><span class="line"><span class="string">        - num_hiddens: 自注意力层和前馈网络的输出维度，即模型的隐藏单元数。</span></span><br><span class="line"><span class="string">        - norm_shape: 层规范化应用的维度，通常是模型的隐藏层维度。</span></span><br><span class="line"><span class="string">        - ffn_num_input: 前馈网络的输入维度。</span></span><br><span class="line"><span class="string">        - ffn_num_hiddens: 前馈网络的隐藏层维度。</span></span><br><span class="line"><span class="string">        - num_heads: 多头自注意力机制中的头数。</span></span><br><span class="line"><span class="string">        - num_layers: 编码器中的层数。</span></span><br><span class="line"><span class="string">        - dropout: Dropout比例。</span></span><br><span class="line"><span class="string">        - use_bias: 是否在自注意力和前馈网络中使用偏置项，默认为False。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        其他参数:</span></span><br><span class="line"><span class="string">        - **kwargs: 捕获未明确列出的关键字参数。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens  <span class="comment"># 存储隐藏单元数</span></span><br><span class="line">        <span class="comment"># 初始化嵌入层，将词汇表索引转换为固定大小的密集向量</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        <span class="comment"># 初始化位置编码，为每个位置的词元添加位置信息</span></span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        <span class="comment"># 创建编码器层的堆叠</span></span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            <span class="comment"># 逐层添加编码器块</span></span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        - X: 输入序列的张量。</span></span><br><span class="line"><span class="string">        - valid_lens: 输入序列中每个元素的有效长度。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">        - 经过Transformer编码器处理的序列张量。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 对嵌入向量进行缩放并添加位置编码</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            <span class="comment"># 逐个编码器块处理</span></span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            <span class="comment"># 存储每个编码器块的注意力权重</span></span><br><span class="line">            self.attention_weights[i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面我们指定了超参数来创建一个两层的Transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，<code>num_hiddens</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>Transformer解码器也是由多个相同的层组成。在<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。</p>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20240229212551267.png" alt="image-20240229212551267"></p>
<p>正如在本节前面所述，在掩蔽多头解码器自注意力层（第一个子层）中，查询、键和值都来自上一个解码器层的输出。关于<strong><em>序列到序列模型</em></strong>，在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数<code>dec_valid_lens</code>，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, i, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化解码器块。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        - key_size, query_size, value_size: 键、查询、值向量的维度。</span></span><br><span class="line"><span class="string">        - num_hiddens: 多头注意力机制和前馈网络的隐藏层维度。</span></span><br><span class="line"><span class="string">        - norm_shape: 层规范化的维度。</span></span><br><span class="line"><span class="string">        - ffn_num_input, ffn_num_hiddens: 前馈网络的输入和隐藏层维度。</span></span><br><span class="line"><span class="string">        - num_heads: 多头自注意力的头数。</span></span><br><span class="line"><span class="string">        - dropout: Dropout层的丢弃比例。</span></span><br><span class="line"><span class="string">        - i: 解码器块的索引，用于在解码过程中跟踪状态。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        <span class="comment"># 第一个多头自注意力层，用于处理目标序列自身的依赖关系。</span></span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        <span class="comment"># 第一个残差连接和层规范化。</span></span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="comment"># 第二个多头自注意力层，用于处理编码器的输出和当前目标序列的依赖关系。</span></span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        <span class="comment"># 第二个残差连接和层规范化。</span></span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="comment"># 基于位置的前馈网络。</span></span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        <span class="comment"># 第三个残差连接和层规范化。</span></span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        - X: 输入序列的张量。</span></span><br><span class="line"><span class="string">        - state: 包含编码器输出和其他信息的状态，用于解码器的计算。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">        - 解码器块的输出和更新后的状态。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]  <span class="comment"># 编码器的输出和有效长度。</span></span><br><span class="line">        <span class="comment"># 处理解码器的自注意力状态更新，区分训练和预测模式。</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自注意力机制。</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器-解码器注意力机制。</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="comment"># 前馈网络。</span></span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是<code>num_hiddens</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<p>现在我们构建了由<code>num_layers</code>个<code>DecoderBlock</code>实例组成的完整的Transformer解码器。最后，通过一个全连接层计算所有<code>vocab_size</code>个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">d2l.AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化Transformer解码器。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        - vocab_size: 词汇表的大小，用于嵌入层和最终的输出层。</span></span><br><span class="line"><span class="string">        - key_size, query_size, value_size: 在多头自注意力机制中，键、查询和值的维度。</span></span><br><span class="line"><span class="string">        - num_hiddens: 内部嵌入的维度，也是多头自注意力和前馈网络的输出维度。</span></span><br><span class="line"><span class="string">        - norm_shape: 层规范化应用的维度，通常是模型的隐藏层维度。</span></span><br><span class="line"><span class="string">        - ffn_num_input, ffn_num_hiddens: 前馈网络的输入和隐藏层的维度。</span></span><br><span class="line"><span class="string">        - num_heads: 多头自注意力中的头数。</span></span><br><span class="line"><span class="string">        - num_layers: 解码器中的层数。</span></span><br><span class="line"><span class="string">        - dropout: Dropout层的丢弃率。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        <span class="comment"># 词嵌入层，将输入的词汇索引转换为密集的向量表示。</span></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        <span class="comment"># 位置编码层，为每个元素的嵌入添加位置信息。</span></span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        <span class="comment"># 解码器块的堆叠，每个块包括两个多头自注意力层和一个前馈网络。</span></span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        <span class="comment"># 最终的线性层，将解码器的输出转换为词汇表大小的向量，用于预测下一个词元。</span></span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化解码器的状态，包括编码器的输出和有效长度。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        - X: 输入序列的张量。</span></span><br><span class="line"><span class="string">        - state: 包含编码器输出和其他信息的状态。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">        - 解码器的输出和更新后的状态。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 存储当前块的解码器自注意力权重和编码器-解码器注意力权重。</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][i] = blk.attention1.attention.attention_weights</span><br><span class="line">            self._attention_weights[<span class="number">1</span>][i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="comment"># 将解码器的输出通过一个线性层转换为预测下一个词元的概率分布。</span></span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        返回解码器内部的注意力权重。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>依照Transformer架构来实例化编码器－解码器模型。在这里，指定Transformer的编码器和解码器都是2层，都使用4头注意力</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置模型参数</span></span><br><span class="line">num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span>  <span class="comment"># 模型隐藏单元数，层数，以及dropout比率</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span>  <span class="comment"># 批量大小和序列长度（步数）</span></span><br><span class="line">lr, num_epochs = <span class="number">0.005</span>, <span class="number">200</span>  <span class="comment"># 学习率和训练周期</span></span><br><span class="line">device = d2l.try_gpu()  <span class="comment"># 训练使用的设备，尽量使用GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置前馈网络和多头注意力的参数</span></span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = <span class="number">32</span>, <span class="number">64</span>, <span class="number">4</span>  <span class="comment"># 前馈网络输入和隐藏层维度，注意力头数</span></span><br><span class="line">key_size, query_size, value_size = <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span>  <span class="comment"># 设置键、查询、值的维度（对于Transformer通常相等）</span></span><br><span class="line">norm_shape = [<span class="number">32</span>]  <span class="comment"># 层规范化应用的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载机器翻译数据集</span></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化编码器</span></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab),  <span class="comment"># 源语言词汇表大小</span></span><br><span class="line">    key_size, query_size, value_size,</span><br><span class="line">    num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">    num_heads, num_layers, dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化解码器</span></span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab),  <span class="comment"># 目标语言词汇表大小</span></span><br><span class="line">    key_size, query_size, value_size,</span><br><span class="line">    num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">    num_heads, num_layers, dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合编码器和解码器为一个完整的模型</span></span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br><span class="line"><span class="comment">#loss 0.030, 5202.9 tokens/sec on cuda:0</span></span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_transformer_5722f1_201_1.svg" alt="../_images/output_transformer_5722f1_201_1.svg"></p>
<p>训练结束后，使用Transformer模型将一些英语句子翻译成法语，并且计算它们的BLEU分数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">go . =&gt; va !,  bleu 1.000</span></span><br><span class="line"><span class="string">i lost . =&gt; j&#x27;ai perdu .,  bleu 1.000</span></span><br><span class="line"><span class="string">he&#x27;s calm . =&gt; il est calme .,  bleu 1.000</span></span><br><span class="line"><span class="string">i&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>当进行最后一个英语到法语的句子翻译工作时，让我们可视化Transformer的注意力权重。编码器自注意力权重的形状为（编码器层数，注意力头数，<code>num_steps</code>或查询的数目，<code>num_steps</code>或“键－值”对的数目）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">enc_attention_weights = torch.cat(net.encoder.attention_weights, <span class="number">0</span>).reshape((num_layers, num_heads,</span><br><span class="line">    -<span class="number">1</span>, num_steps))</span><br><span class="line">enc_attention_weights.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 10, 10])</span></span><br></pre></td></tr></table></figure>
<p>在编码器的自注意力中，查询和键都来自相同的输入序列。因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    enc_attention_weights.cpu(), xlabel=&#x27;Key positions&#x27;,</span><br><span class="line">    ylabel=&#x27;Query positions&#x27;, titles=[&#x27;Head %d&#x27; % i for i in range(1, 5)],</span><br><span class="line">    figsize=(7, 3.5))</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_transformer_5722f1_246_0.svg" alt="../_images/output_transformer_5722f1_246_0.svg"></p>
<p>为了可视化解码器的自注意力权重和“编码器－解码器”的注意力权重，我们需要完成更多的数据操作工作。例如用零填充被掩蔽住的注意力权重。值得注意的是，解码器的自注意力权重和“编码器－解码器”的注意力权重都有相同的查询：即以<strong><em>序列开始词元</em></strong>（beginning-of-sequence,BOS）打头，再与后续输出的词元共同组成序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dec_attention_weights_2d = [head[<span class="number">0</span>].tolist()</span><br><span class="line">                            <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq</span><br><span class="line">                            <span class="keyword">for</span> attn <span class="keyword">in</span> step <span class="keyword">for</span> blk <span class="keyword">in</span> attn <span class="keyword">for</span> head <span class="keyword">in</span> blk]</span><br><span class="line">dec_attention_weights_filled = torch.tensor(</span><br><span class="line">    pd.DataFrame(dec_attention_weights_2d).fillna(<span class="number">0.0</span>).values)</span><br><span class="line">dec_attention_weights = dec_attention_weights_filled.reshape((-<span class="number">1</span>, <span class="number">2</span>, num_layers, num_heads, num_steps))</span><br><span class="line">dec_self_attention_weights, dec_inter_attention_weights = \</span><br><span class="line">    dec_attention_weights.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">dec_self_attention_weights.shape, dec_inter_attention_weights.shape</span><br></pre></td></tr></table></figure>
<p>由于解码器自注意力的自回归属性，查询不会对当前位置之后的“键－值”对进行注意力计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plusonetoincludethebeginning-of-sequencetoken</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_self_attention_weights[:, :, :, :<span class="built_in">len</span>(translation.split()) + <span class="number">1</span>],</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">    titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_transformer_5722f1_276_0.svg" alt="../_images/output_transformer_5722f1_276_0.svg"></p>
<p>与编码器的自注意力的情况类似，通过指定输入序列的有效长度，输出序列的查询不会与输入序列中填充位置的词元进行注意力计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_inter_attention_weights, xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraoutput_transformer_5722f1_291_0.svg" alt="../_images/output_transformer_5722f1_291_0.svg"></p>
<p>尽管Transformer架构是为了<strong><em>序列到序列</em></strong>的学习而提出的，但正如本书后面将提及的那样，Transformer编码器或Transformer解码器通常被单独用于不同的深度学习任务中。</p>
<ul>
<li>Transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。</li>
<li>在Transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。</li>
<li>Transformer中的残差连接和层规范化是训练非常深度模型的重要工具。</li>
<li>Transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。</li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/54714.html"><img class="prev-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">7.0 现代卷积神经网络</div></div></a></div><div class="next-post pull-right"><a href="/post/53892.html"><img class="next-cover" src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">8.0 循环神经网络</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com/typora/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Candle</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">195</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.</span> <span class="toc-text">注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8F%90%E7%A4%BA"><span class="toc-number">1.1.</span> <span class="toc-text">注意力提示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.1.2.</span> <span class="toc-text">可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Nadaraya-Watson-%E6%A0%B8%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">Nadaraya-Watson 核回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.1.</span> <span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E6%B1%87%E8%81%9A"><span class="toc-number">1.2.2.</span> <span class="toc-text">平均汇聚</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E5%8F%82%E6%95%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A"><span class="toc-number">1.2.3.</span> <span class="toc-text">非参数注意力汇聚</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E5%8F%82%E6%95%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A"><span class="toc-number">1.2.4.</span> <span class="toc-text">带参数注意力汇聚</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AF%84%E5%88%86%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.</span> <span class="toc-text">注意力评分函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A9%E8%94%BDsoftmax"><span class="toc-number">1.3.1.</span> <span class="toc-text">掩蔽softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.3.2.</span> <span class="toc-text">加性注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.3.3.</span> <span class="toc-text">缩放点积注意力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.4.</span> <span class="toc-text">多头注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.5.</span> <span class="toc-text">自注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AF%94%E8%BE%83"><span class="toc-number">1.5.1.</span> <span class="toc-text">比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.5.2.</span> <span class="toc-text">位置编码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer"><span class="toc-number">1.6.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">1.6.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F"><span class="toc-number">1.6.2.</span> <span class="toc-text">基于位置的前馈神经</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%B1%82%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">1.6.3.</span> <span class="toc-text">残差连接和层规范化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.6.4.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">1.6.5.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.6.6.</span> <span class="toc-text">训练</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recommend-post"><div class="item-headline"><i class="fas fa-dharmachakra"></i><span>相关推荐</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/40793.html" title="1.0 Pytorch入门"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="1.0 Pytorch入门"></a><div class="content"><a class="title" href="/post/40793.html" title="1.0 Pytorch入门">1.0 Pytorch入门</a><time datetime="2023-12-24" title="������ 2023-12-24">2023-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/56727.html" title="3.0 softmax回归"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="3.0 softmax回归"></a><div class="content"><a class="title" href="/post/56727.html" title="3.0 softmax回归">3.0 softmax回归</a><time datetime="2024-02-20" title="������ 2024-02-20">2024-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/62658.html" title="2.0 线性回归"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="2.0 线性回归"></a><div class="content"><a class="title" href="/post/62658.html" title="2.0 线性回归">2.0 线性回归</a><time datetime="2023-12-25" title="������ 2023-12-25">2023-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/21849.html" title="5.0 pytorch基础"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="5.0 pytorch基础"></a><div class="content"><a class="title" href="/post/21849.html" title="5.0 pytorch基础">5.0 pytorch基础</a><time datetime="2024-02-21" title="������ 2024-02-21">2024-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/58837.html" title="4.0 多层感知机"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="4.0 多层感知机"></a><div class="content"><a class="title" href="/post/58837.html" title="4.0 多层感知机">4.0 多层感知机</a><time datetime="2024-02-21" title="������ 2024-02-21">2024-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/4590.html" title="6.0 卷积神经网络"><img src="https://candle-1308820096.cos.ap-chengdu.myqcloud.com//typoraimage-20231210050612555.png" alt="6.0 卷积神经网络"></a><div class="content"><a class="title" href="/post/4590.html" title="6.0 卷积神经网络">6.0 卷积神经网络</a><time datetime="2024-02-22" title="������ 2024-02-22">2024-02-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2024 By Candle</div><div class="framework-info"><span>Powered by </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a></div><div class="footer_custom_text">So Long, and Thanks for All the Fish</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-y/mathjax/3.2.0/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="https://npm.elemecdn.com/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.removeEventListener('scroll', window.tocScrollFn)
  window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>